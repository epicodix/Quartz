---
title: ë™ì  í™•ì¥(Dynamic Scaling) ì•„í‚¤í…ì²˜
tags:
  - architecture
  - dynamic-scaling
  - auto-scaling
  - cloud-computing
  - performance
  - devops
  - infrastructure
aliases:
  - Dynamic-Scaling
  - Auto-Scaling-Architecture
  - ë™ì í™•ì¥
date: 2025-11-26
category: êµìœ¡/êµ¬ë¦„ë”¥ë‹¤ì´ë¸Œ/ì•„í‚¤í…ì²˜
status: ì™„ì„±
priority: ë†’ìŒ
---

# âš¡ ë™ì  í™•ì¥(Dynamic Scaling) ì•„í‚¤í…ì²˜

## ğŸ“‘ ëª©ì°¨
- [[#1. í•µì‹¬ ê°œë…|1. í•µì‹¬ ê°œë…]]
- [[#2. í™•ì¥ì˜ ë‘ ê°€ì§€ ë°©í–¥|2. í™•ì¥ì˜ ë‘ ê°€ì§€ ë°©í–¥]]
- [[#3. í™•ì¥ ë°©ì‹ì˜ ì¢…ë¥˜|3. í™•ì¥ ë°©ì‹ì˜ ì¢…ë¥˜]]
- [[#4. ì£¼ìš” êµ¬ì„± ìš”ì†Œ|4. ì£¼ìš” êµ¬ì„± ìš”ì†Œ]]
- [[#5. í™•ì¥ ì •ì±… ì„¤ê³„|5. í™•ì¥ ì •ì±… ì„¤ê³„]]
- [[#6. ì‹¤ì œ ì ìš© ì‚¬ë¡€|6. ì‹¤ì œ ì ìš© ì‚¬ë¡€]]
- [[#7. ì¥ì ê³¼ ê³¼ì œ|7. ì¥ì ê³¼ ê³¼ì œ]]
- [[#ğŸ¯ ì‹¤ì „ ì˜ˆì‹œ|ì‹¤ì „ ì˜ˆì‹œ]]

---

## 1. í•µì‹¬ ê°œë…

> [!note] ë™ì  í™•ì¥ì´ë€?
> ë™ì  í™•ì¥ ì•„í‚¤í…ì²˜ëŠ” ì‹œìŠ¤í…œì˜ ë¶€í•˜ë‚˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ê±°ë‚˜ ê°ì†Œì‹œí‚¤ëŠ” ì„¤ê³„ ë°©ì‹ì…ë‹ˆë‹¤. íŠ¸ë˜í”½ ë³€í™”ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€ì‘í•˜ì—¬ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ë¹„ìš©ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ë™ì  í™•ì¥ì˜ í•µì‹¬ ê°€ì¹˜

#### ğŸ¯ **ì ì‘ì„± (Adaptability)**
ì‹œìŠ¤í…œì´ ë³€í™”í•˜ëŠ” ì›Œí¬ë¡œë“œì— ìë™ìœ¼ë¡œ ì ì‘í•©ë‹ˆë‹¤.

```yaml
# ì˜ˆì‹œ: ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ íŒ¨í„´
traffic_patterns:
  morning_peak:    # ì˜¤ì „ 9-10ì‹œ
    expected_load: "200% of baseline"
    scale_target: "20 instances"
    
  lunch_time:      # ì ì‹¬ 12-13ì‹œ  
    expected_load: "150% of baseline"
    scale_target: "15 instances"
    
  evening_peak:    # ì˜¤í›„ 6-8ì‹œ
    expected_load: "250% of baseline" 
    scale_target: "25 instances"
    
  night_low:       # ìƒˆë²½ 2-5ì‹œ
    expected_load: "30% of baseline"
    scale_target: "3 instances"
```

#### âš¡ **ë°˜ì‘ì„± (Responsiveness)**
ë¶€í•˜ ë³€í™”ë¥¼ ê°ì§€í•˜ê³  ë¹ ë¥´ê²Œ ëŒ€ì‘í•©ë‹ˆë‹¤.

```python
# ë™ì  í™•ì¥ ê²°ì • ë¡œì§ ì˜ˆì‹œ
class AutoScalingEngine:
    def __init__(self):
        self.scaling_policies = {
            'scale_up_threshold': 70,    # CPU 70% ì´ˆê³¼ ì‹œ í™•ì¥
            'scale_down_threshold': 30,  # CPU 30% ë¯¸ë§Œ ì‹œ ì¶•ì†Œ
            'cooldown_period': 300,      # 5ë¶„ ì¿¨ë‹¤ìš´
            'max_instances': 50,
            'min_instances': 2
        }
    
    def evaluate_scaling_decision(self, metrics):
        current_cpu = metrics['avg_cpu_utilization']
        current_instances = metrics['current_instance_count']
        
        if current_cpu > self.scaling_policies['scale_up_threshold']:
            if current_instances < self.scaling_policies['max_instances']:
                return self.calculate_scale_up(current_cpu, current_instances)
                
        elif current_cpu < self.scaling_policies['scale_down_threshold']:
            if current_instances > self.scaling_policies['min_instances']:
                return self.calculate_scale_down(current_cpu, current_instances)
                
        return {'action': 'no_change', 'reason': 'within_threshold'}
    
    def calculate_scale_up(self, cpu_usage, current_instances):
        # CPU ì‚¬ìš©ë¥  ê¸°ë°˜ í™•ì¥ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ê³„ì‚°
        target_instances = min(
            int(current_instances * (cpu_usage / 60)),  # ëª©í‘œ 60% ì‚¬ìš©ë¥ 
            self.scaling_policies['max_instances']
        )
        
        return {
            'action': 'scale_up',
            'current_instances': current_instances,
            'target_instances': target_instances,
            'reason': f'High CPU usage: {cpu_usage}%'
        }
```

#### ğŸ’° **ë¹„ìš© íš¨ìœ¨ì„± (Cost Efficiency)**
í•„ìš”í•œ ë§Œí¼ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.

---

## 2. í™•ì¥ì˜ ë‘ ê°€ì§€ ë°©í–¥

### ğŸ“ˆ ìˆ˜í‰ í™•ì¥ (Scale Out/In)

> [!tip] ìˆ˜í‰ í™•ì¥ì˜ íŠ¹ì§•
> ì„œë²„ë‚˜ ì¸ìŠ¤í„´ìŠ¤ì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í´ë¼ìš°ë“œ í™˜ê²½ì— ê°€ì¥ ì í•©í•œ í™•ì¥ ë°©ì‹ì…ë‹ˆë‹¤.

#### âœ… ì¥ì 
- **ë¬´í•œ í™•ì¥ ê°€ëŠ¥**: ì´ë¡ ì ìœ¼ë¡œ ë¬´ì œí•œ í™•ì¥
- **ê³ ê°€ìš©ì„±**: ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¥ì•  ë¶„ì‚°
- **ì ì§„ì  í™•ì¥**: í•„ìš”í•œ ë§Œí¼ë§Œ ë‹¨ê³„ì  ì¶”ê°€
- **í´ë¼ìš°ë“œ ì¹œí™”ì **: í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì™€ ì™„ë²½ í˜¸í™˜

#### âš ï¸ ë‹¨ì ê³¼ ê³ ë ¤ì‚¬í•­
- **ë¡œë“œ ë°¸ëŸ°ì„œ í•„ìš”**: íŠ¸ë˜í”½ ë¶„ì‚° ë©”ì»¤ë‹ˆì¦˜ í•„ìˆ˜
- **ìƒíƒœ ê´€ë¦¬ ë³µì¡**: ì„¸ì…˜, ë°ì´í„° ì¼ê´€ì„± ì²˜ë¦¬
- **ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ**: ì¸ìŠ¤í„´ìŠ¤ ê°„ í†µì‹  ë¹„ìš©

#### ğŸ“Š ìˆ˜í‰ í™•ì¥ êµ¬í˜„ ì˜ˆì‹œ

```yaml
# AWS Auto Scaling Group ì„¤ì •
AutoScalingGroup:
  GroupName: "web-servers-asg"
  LaunchTemplate:
    LaunchTemplateId: "lt-12345678"
    Version: "$Latest"
  MinSize: 2
  MaxSize: 20
  DesiredCapacity: 5
  TargetGroupARNs:
    - "arn:aws:elasticloadbalancing:..."
  
  # í™•ì¥ ì •ì±…
  ScalingPolicies:
    ScaleUpPolicy:
      PolicyType: "TargetTrackingScaling"
      TargetTrackingConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: "ASGAverageCPUUtilization"
        TargetValue: 70.0
        ScaleOutCooldown: 300  # 5ë¶„
        
    ScaleDownPolicy:
      PolicyType: "TargetTrackingScaling"  
      TargetTrackingConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: "ASGAverageCPUUtilization"
        TargetValue: 70.0
        ScaleInCooldown: 600   # 10ë¶„ (ë³´ìˆ˜ì )
```

#### ğŸ³ Kubernetes HPA ì˜ˆì‹œ

```yaml
# Kubernetes Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100        # 100% ì¦ê°€ (2ë°°)
        periodSeconds: 60
      - type: Pods  
        value: 5          # ë˜ëŠ” 5ê°œì”© ì¶”ê°€
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10         # 10%ì”© ê°ì†Œ
        periodSeconds: 60
```

### ğŸ“ ìˆ˜ì§ í™•ì¥ (Scale Up/Down)

> [!info] ìˆ˜ì§ í™•ì¥ì˜ íŠ¹ì§•
> ê°œë³„ ì„œë²„ì˜ ì‚¬ì–‘ì„ ë†’ì´ê±°ë‚˜ ë‚®ì¶”ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. CPU ì½”ì–´ ìˆ˜, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ ë“±ì„ ì¦ì„¤í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

#### âœ… ì¥ì 
- **êµ¬í˜„ ë‹¨ìˆœì„±**: ê¸°ì¡´ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì • ìµœì†Œí™”
- **ê´€ë¦¬ ìš©ì´ì„±**: ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ê°€ ì ì–´ ê´€ë¦¬ ê°„í¸
- **ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ì—†ìŒ**: ë‹¨ì¼ ë¨¸ì‹  ë‚´ ì²˜ë¦¬

#### âš ï¸ ë‹¨ì ê³¼ ì œì•½ì‚¬í•­
- **í•˜ë“œì›¨ì–´ í•œê³„**: ë¬¼ë¦¬ì  í™•ì¥ í•œê³„ ì¡´ì¬
- **ë‹¤ìš´íƒ€ì„ ë°œìƒ**: ìŠ¤í™ ë³€ê²½ ì‹œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨
- **ë‹¨ì¼ ì¥ì• ì **: í•˜ë‚˜ì˜ ì„œë²„ ì¥ì• ê°€ ì „ì²´ ì˜í–¥
- **ë¹„ìš© ë¹„íš¨ìœ¨**: ê³ ì„±ëŠ¥ í•˜ë“œì›¨ì–´ì˜ ë†’ì€ ë‹¨ê°€

#### ğŸ“Š ìˆ˜ì§ í™•ì¥ êµ¬í˜„ ì˜ˆì‹œ

```yaml
# Kubernetes VPA (Vertical Pod Autoscaler)
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: webapp-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  updatePolicy:
    updateMode: "Auto"     # Auto, Off, Initial
  resourcePolicy:
    containerPolicies:
    - containerName: webapp
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 4
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
```

#### ğŸ–¥ï¸ AWSì—ì„œì˜ ìˆ˜ì§ í™•ì¥

```python
# AWSì—ì„œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë³€ê²½
import boto3

class VerticalScaler:
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        
    def scale_instance_type(self, instance_id, new_instance_type):
        try:
            # 1. ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
            self.ec2.stop_instances(InstanceIds=[instance_id])
            
            # 2. ì¤‘ì§€ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
            waiter = self.ec2.get_waiter('instance_stopped')
            waiter.wait(InstanceIds=[instance_id])
            
            # 3. ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë³€ê²½
            self.ec2.modify_instance_attribute(
                InstanceId=instance_id,
                InstanceType={'Value': new_instance_type}
            )
            
            # 4. ì¸ìŠ¤í„´ìŠ¤ ì¬ì‹œì‘
            self.ec2.start_instances(InstanceIds=[instance_id])
            
            return True
            
        except Exception as e:
            print(f"Vertical scaling failed: {e}")
            return False
    
    def get_recommended_instance_type(self, cpu_utilization, memory_utilization):
        """ì‚¬ìš©ë¥  ê¸°ë°˜ ê¶Œì¥ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        if cpu_utilization > 80 or memory_utilization > 80:
            return self.get_larger_instance_type()
        elif cpu_utilization < 20 and memory_utilization < 20:
            return self.get_smaller_instance_type()
        
        return None  # ë³€ê²½ ë¶ˆí•„ìš”
```

---

## 3. í™•ì¥ ë°©ì‹ì˜ ì¢…ë¥˜

### ğŸ”„ ë°˜ì‘í˜• í™•ì¥ (Reactive Scaling)

> [!warning] ë°˜ì‘í˜• í™•ì¥ì˜ íŠ¹ì§•
> í˜„ì¬ ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì„ê³„ê°’ì— ë„ë‹¬í•˜ë©´ í™•ì¥í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì´ì§€ë§Œ **ì§€ì—° ì‹œê°„**ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

#### ğŸ“Š ë°˜ì‘í˜• í™•ì¥ íë¦„

```mermaid
graph TD
    A[ë©”íŠ¸ë¦­ ìˆ˜ì§‘] --> B[ì„ê³„ê°’ í™•ì¸]
    B --> C{ì„ê³„ê°’ ì´ˆê³¼?}
    C -->|Yes| D[í™•ì¥ ê²°ì •]
    C -->|No| E[í˜„ìƒ ìœ ì§€]
    D --> F[ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±]
    F --> G[ë¡œë“œë°¸ëŸ°ì„œ ë“±ë¡]
    G --> H[í—¬ìŠ¤ì²´í¬ í†µê³¼]
    H --> I[íŠ¸ë˜í”½ ìˆ˜ì‹  ì‹œì‘]
```

#### ğŸ’» ë°˜ì‘í˜• í™•ì¥ êµ¬í˜„

```python
class ReactiveScaler:
    def __init__(self):
        self.thresholds = {
            'cpu_high': 80,
            'cpu_low': 20,
            'memory_high': 85,
            'response_time_high': 2.0,  # 2ì´ˆ
            'error_rate_high': 5.0      # 5%
        }
        self.cooldown_period = 300  # 5ë¶„
        self.last_scaling_time = 0
        
    def should_scale_up(self, metrics):
        conditions = [
            metrics['avg_cpu'] > self.thresholds['cpu_high'],
            metrics['avg_memory'] > self.thresholds['memory_high'],
            metrics['avg_response_time'] > self.thresholds['response_time_high'],
            metrics['error_rate'] > self.thresholds['error_rate_high']
        ]
        
        # ë‹¤ì¤‘ ì¡°ê±´ í™•ì¸
        return any(conditions) and self.is_cooldown_passed()
    
    def should_scale_down(self, metrics):
        conditions = [
            metrics['avg_cpu'] < self.thresholds['cpu_low'],
            metrics['avg_memory'] < self.thresholds['cpu_low'],
            metrics['avg_response_time'] < 0.5,
            metrics['error_rate'] < 1.0
        ]
        
        # ëª¨ë“  ì¡°ê±´ì´ ë§Œì¡±ë˜ì–´ì•¼ ì¶•ì†Œ
        return all(conditions) and self.is_cooldown_passed()
    
    def is_cooldown_passed(self):
        import time
        return time.time() - self.last_scaling_time > self.cooldown_period
```

### ğŸ”® ì˜ˆì¸¡í˜• í™•ì¥ (Predictive Scaling)

> [!tip] ì˜ˆì¸¡í˜• í™•ì¥ì˜ ì¥ì 
> ê³¼ê±° ë°ì´í„°ì™€ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ë¯¸ë¦¬ í™•ì¥í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. **ì„ ì œì  ëŒ€ì‘**ì´ ê°€ëŠ¥í•˜ì—¬ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.

#### ğŸ“ˆ ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import datetime

class PredictiveScaler:
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100)
        self.is_trained = False
        
    def prepare_features(self, historical_data):
        """ì‹œê³„ì—´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
        df = historical_data.copy()
        
        # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        df['month'] = df.index.month
        df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)
        
        # ê³„ì ˆì„± íŠ¹ì„± (ì‚¬ì¸, ì½”ì‚¬ì¸ ë³€í™˜)
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        
        # ì´ë™ í‰ê·  (íŠ¸ë Œë“œ)
        df['ma_24h'] = df['cpu_usage'].rolling(24).mean()
        df['ma_7d'] = df['cpu_usage'].rolling(24*7).mean()
        
        # ì§€ì—° íŠ¹ì„± (Lag features)
        df['cpu_lag_1h'] = df['cpu_usage'].shift(1)
        df['cpu_lag_24h'] = df['cpu_usage'].shift(24)
        
        return df.dropna()
    
    def train_model(self, historical_data):
        """ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨"""
        df = self.prepare_features(historical_data)
        
        features = ['hour', 'day_of_week', 'month', 'is_weekend',
                   'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
                   'ma_24h', 'ma_7d', 'cpu_lag_1h', 'cpu_lag_24h']
        
        X = df[features]
        y = df['cpu_usage']
        
        self.model.fit(X, y)
        self.feature_names = features
        self.is_trained = True
        
        return self.model.score(X, y)  # RÂ² ì ìˆ˜ ë°˜í™˜
    
    def predict_next_24h(self):
        """ë‹¤ìŒ 24ì‹œê°„ CPU ì‚¬ìš©ë¥  ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("Model not trained yet")
        
        predictions = []
        current_time = datetime.datetime.now()
        
        for hour in range(24):
            future_time = current_time + datetime.timedelta(hours=hour)
            features = self.extract_time_features(future_time)
            
            predicted_cpu = self.model.predict([features])[0]
            predictions.append({
                'time': future_time,
                'predicted_cpu': predicted_cpu,
                'recommended_instances': self.cpu_to_instances(predicted_cpu)
            })
        
        return predictions
    
    def cpu_to_instances(self, cpu_usage):
        """CPU ì‚¬ìš©ë¥ ì„ í•„ìš” ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ë¡œ ë³€í™˜"""
        base_instances = 2
        target_cpu = 60  # ëª©í‘œ CPU ì‚¬ìš©ë¥  60%
        
        if cpu_usage <= target_cpu:
            return base_instances
        
        # ì˜ˆìƒ ë¶€í•˜ì— ë”°ë¥¸ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ê³„ì‚°
        needed_instances = max(base_instances, int((cpu_usage / target_cpu) * base_instances))
        return min(needed_instances, 50)  # ìµœëŒ€ 50ê°œë¡œ ì œí•œ
```

#### ğŸ“… AWS Predictive Scaling ì„¤ì •

```yaml
# AWS Predictive Scaling ì •ì±…
PredictiveScalingPolicy:
  PolicyName: "predictive-cpu-scaling"
  PolicyType: "PredictiveScaling"
  PredictiveScalingConfiguration:
    MetricSpecifications:
    - TargetValue: 70.0
      PredefinedMetricSpecification:
        PredefinedMetricType: "ASGAverageCPUUtilization"
    
    Mode: "ForecastAndScale"     # ForecastOnly, ForecastAndScale
    SchedulingBufferTime: 600    # 10ë¶„ ì „ ë¯¸ë¦¬ í™•ì¥
    MaxCapacityBreachBehavior: "IncreaseMaxCapacity"
    MaxCapacityBuffer: 20        # ìµœëŒ€ ìš©ëŸ‰ì˜ 20% ì¶”ê°€ ë²„í¼
```

### â° ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í™•ì¥ (Scheduled Scaling)

> [!note] ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í™•ì¥
> ì •í•´ì§„ ì‹œê°„ì— í™•ì¥í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ì— ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.

#### ğŸ“… ìŠ¤ì¼€ì¤„ë§ ì˜ˆì‹œ

```python
import schedule
import time
from datetime import datetime, timedelta

class ScheduledScaler:
    def __init__(self, auto_scaling_client):
        self.asg_client = auto_scaling_client
        self.scaling_schedules = []
        
    def add_schedule(self, day_of_week, time, desired_capacity, description=""):
        """ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
        self.scaling_schedules.append({
            'day': day_of_week,  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
            'time': time,        # "09:00" í˜•ì‹
            'capacity': desired_capacity,
            'description': description
        })
    
    def setup_workday_schedule(self):
        """í‰ì¼ ì—…ë¬´ ì‹œê°„ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # í‰ì¼ ì˜¤ì „ 8ì‹œ - í™•ì¥
        for day in range(5):  # ì›”~ê¸ˆ
            schedule.every().monday.at("08:00").do(
                self.scale_to_capacity, 15, "ì›Œí¬ë°ì´ ì‹œì‘"
            )
            
        # í‰ì¼ ì ì‹¬ ì‹œê°„ - ì¶”ê°€ í™•ì¥  
        for day in range(5):
            schedule.every().monday.at("11:30").do(
                self.scale_to_capacity, 25, "ì ì‹¬ì‹œê°„ ëŒ€ë¹„"
            )
            
        # í‰ì¼ ì €ë… - ì¶•ì†Œ
        for day in range(5):
            schedule.every().monday.at("19:00").do(
                self.scale_to_capacity, 8, "ì—…ë¬´ì‹œê°„ ì¢…ë£Œ"
            )
            
        # í‰ì¼ ì•¼ê°„ - ìµœì†Œ ìœ ì§€
        for day in range(5):
            schedule.every().monday.at("23:00").do(
                self.scale_to_capacity, 3, "ì•¼ê°„ ìµœì†Œ ìœ ì§€"
            )
    
    def setup_weekend_schedule(self):
        """ì£¼ë§ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # ì£¼ë§ì€ ë‚®ì€ ìš©ëŸ‰ ìœ ì§€
        schedule.every().saturday.at("09:00").do(
            self.scale_to_capacity, 5, "ì£¼ë§ ê¸°ë³¸ ìš©ëŸ‰"
        )
        schedule.every().sunday.at("09:00").do(
            self.scale_to_capacity, 5, "ì£¼ë§ ê¸°ë³¸ ìš©ëŸ‰"
        )
    
    def setup_special_events(self):
        """íŠ¹ë³„ ì´ë²¤íŠ¸ ìŠ¤ì¼€ì¤„"""
        # ë¸”ë™í”„ë¼ì´ë°ì´ ëŒ€ë¹„
        schedule.every().year.november.at("00:00").tag('black-friday').do(
            self.scale_to_capacity, 100, "ë¸”ë™í”„ë¼ì´ë°ì´ ëŒ€ë¹„"
        )
        
        # ì‹ ì œí’ˆ ì¶œì‹œì¼ ëŒ€ë¹„
        schedule.every().year.september.at("10:00").tag('product-launch').do(
            self.scale_to_capacity, 75, "ì‹ ì œí’ˆ ì¶œì‹œ"
        )
    
    def scale_to_capacity(self, desired_capacity, reason=""):
        """ì§€ì •ëœ ìš©ëŸ‰ìœ¼ë¡œ í™•ì¥/ì¶•ì†Œ"""
        try:
            response = self.asg_client.set_desired_capacity(
                AutoScalingGroupName='web-servers-asg',
                DesiredCapacity=desired_capacity,
                HonorCooldown=False
            )
            print(f"Scaled to {desired_capacity} instances. Reason: {reason}")
            return response
        except Exception as e:
            print(f"Scaling failed: {e}")
    
    def run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
```

#### ğŸ³ Kubernetes CronJob ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§

```yaml
# ì—…ë¬´ ì‹œê°„ í™•ì¥ CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-up-workday
spec:
  schedule: "0 8 * * 1-5"  # í‰ì¼ ì˜¤ì „ 8ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: autoscaler
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl patch hpa webapp-hpa -p '{"spec":{"minReplicas":15}}'
              echo "Scaled up for workday"
          restartPolicy: OnFailure

---
# ì•¼ê°„ ì¶•ì†Œ CronJob  
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-down-night
spec:
  schedule: "0 22 * * 1-5"  # í‰ì¼ ì˜¤í›„ 10ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: autoscaler
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl patch hpa webapp-hpa -p '{"spec":{"minReplicas":3}}'
              echo "Scaled down for night"
          restartPolicy: OnFailure
```

---

## 4. ì£¼ìš” êµ¬ì„± ìš”ì†Œ

### ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

> [!info] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì˜ ì¤‘ìš”ì„±
> ë™ì  í™•ì¥ì˜ ê¸°ë°˜ì€ ì •í™•í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì…ë‹ˆë‹¤. CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬, ì‘ë‹µ ì‹œê°„, í ê¸¸ì´ ë“±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

#### ğŸ“ˆ í•µì‹¬ ë©”íŠ¸ë¦­ ë¶„ë¥˜

| ë©”íŠ¸ë¦­ ìœ í˜• | êµ¬ì²´ì  ì§€í‘œ | ìˆ˜ì§‘ ì£¼ê¸° | ì¤‘ìš”ë„ |
|-------------|-------------|-----------|---------|
| **ì¸í”„ë¼ ë©”íŠ¸ë¦­** | CPU, Memory, Disk I/O, Network | 1ë¶„ | ë†’ìŒ |
| **ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­** | ì‘ë‹µì‹œê°„, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨ | 30ì´ˆ | ë§¤ìš° ë†’ìŒ |
| **ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­** | ë™ì‹œ ì‚¬ìš©ì, ì£¼ë¬¸ëŸ‰, ë§¤ì¶œ | 5ë¶„ | ì¤‘ê°„ |
| **ì™¸ë¶€ ë©”íŠ¸ë¦­** | í ê¸¸ì´, ì™¸ë¶€ API ì‘ë‹µì‹œê°„ | 1ë¶„ | ë†’ìŒ |

#### ğŸ”§ Prometheus ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

```yaml
# Prometheus ì„¤ì •
global:
  scrape_interval: 30s
  evaluation_interval: 30s

rule_files:
  - "autoscaling.rules.yml"

scrape_configs:
  # Kubernetes ë©”íŠ¸ë¦­
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
    - role: node
    relabel_configs:
    - source_labels: [__address__]
      regex: '(.*):10250'
      target_label: __address__
      replacement: '${1}:9100'
  
  # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­
  - job_name: 'webapp'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)

  # ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­
  - job_name: 'custom-metrics'
    static_configs:
    - targets: ['custom-metrics-server:8080']
```

#### ğŸ“Š Custom Metrics êµ¬í˜„

```python
from prometheus_client import start_http_server, Counter, Histogram, Gauge
import time
import threading

class ApplicationMetrics:
    def __init__(self):
        # ì¹´ìš´í„° ë©”íŠ¸ë¦­
        self.request_count = Counter(
            'http_requests_total', 
            'Total HTTP requests', 
            ['method', 'endpoint', 'status_code']
        )
        
        # íˆìŠ¤í† ê·¸ë¨ ë©”íŠ¸ë¦­ (ì‘ë‹µ ì‹œê°„)
        self.request_duration = Histogram(
            'http_request_duration_seconds',
            'HTTP request duration',
            ['method', 'endpoint']
        )
        
        # ê²Œì´ì§€ ë©”íŠ¸ë¦­ (í˜„ì¬ í™œì„± ì—°ê²°)
        self.active_connections = Gauge(
            'active_connections_total',
            'Current active connections'
        )
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
        self.active_users = Gauge(
            'active_users_current',
            'Current active users'
        )
        
        self.queue_length = Gauge(
            'processing_queue_length',
            'Current processing queue length'
        )
    
    def record_request(self, method, endpoint, status_code, duration):
        """HTTP ìš”ì²­ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.request_count.labels(
            method=method, 
            endpoint=endpoint, 
            status_code=status_code
        ).inc()
        
        self.request_duration.labels(
            method=method, 
            endpoint=endpoint
        ).observe(duration)
    
    def update_active_connections(self, count):
        self.active_connections.set(count)
    
    def update_active_users(self, count):
        self.active_users.set(count)
    
    def update_queue_length(self, length):
        self.queue_length.set(length)

# ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
metrics = ApplicationMetrics()
start_http_server(8000)
```

### ğŸ›ï¸ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì—”ì§„

#### âš™ï¸ AWS Auto Scaling ì„¤ì •

```yaml
# CloudFormationìœ¼ë¡œ Auto Scaling êµ¬ì„±
Resources:
  WebServerLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: WebServerTemplate
      LaunchTemplateData:
        ImageId: ami-0c55b159cbfafe1d0
        InstanceType: t3.medium
        SecurityGroupIds: [!Ref WebServerSecurityGroup]
        UserData:
          Fn::Base64: !Sub |
            #!/bin/bash
            yum update -y
            yum install -y docker
            systemctl start docker
            systemctl enable docker
            # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
            
  WebServerAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier: 
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2
      LaunchTemplate:
        LaunchTemplateId: !Ref WebServerLaunchTemplate
        Version: !GetAtt WebServerLaunchTemplate.LatestVersionNumber
      MinSize: 2
      MaxSize: 20
      DesiredCapacity: 5
      TargetGroupARNs: [!Ref WebServerTargetGroup]
      HealthCheckType: ELB
      HealthCheckGracePeriod: 300
      
      Tags:
      - Key: Name
        Value: WebServer
        PropagateAtLaunch: true

  # CPU ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ì •ì±…
  ScaleUpPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      AdjustmentType: ChangeInCapacity
      AutoScalingGroupName: !Ref WebServerAutoScalingGroup
      Cooldown: 300
      ScalingAdjustment: 2  # 2ê°œì”© ì¶”ê°€
      
  ScaleDownPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      AdjustmentType: ChangeInCapacity
      AutoScalingGroupName: !Ref WebServerAutoScalingGroup
      Cooldown: 600
      ScalingAdjustment: -1  # 1ê°œì”© ì œê±°
      
  # CloudWatch ì•ŒëŒ
  HighCPUAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: HighCPUUtilization
      AlarmDescription: Triggers when CPU exceeds 70%
      MetricName: CPUUtilization
      Namespace: AWS/EC2
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 70
      ComparisonOperator: GreaterThanThreshold
      AlarmActions: [!Ref ScaleUpPolicy]
      Dimensions:
      - Name: AutoScalingGroupName
        Value: !Ref WebServerAutoScalingGroup
        
  LowCPUAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: LowCPUUtilization
      AlarmDescription: Triggers when CPU below 30%
      MetricName: CPUUtilization
      Namespace: AWS/EC2
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 30
      ComparisonOperator: LessThanThreshold
      AlarmActions: [!Ref ScaleDownPolicy]
      Dimensions:
      - Name: AutoScalingGroupName
        Value: !Ref WebServerAutoScalingGroup
```

### âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì„œ

#### ğŸŒ Application Load Balancer ì„¤ì •

```yaml
# ALB ì„¤ì • with Auto Scaling
Resources:
  ApplicationLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: WebApp-ALB
      Scheme: internet-facing
      Type: application
      SecurityGroups: [!Ref ALBSecurityGroup]
      Subnets: 
        - !Ref PublicSubnet1
        - !Ref PublicSubnet2
      
  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: WebApp-TG
      Protocol: HTTP
      Port: 80
      VpcId: !Ref VPC
      TargetType: instance
      
      # í—¬ìŠ¤ ì²´í¬ ì„¤ì •
      HealthCheckProtocol: HTTP
      HealthCheckPath: /health
      HealthCheckIntervalSeconds: 30
      HealthCheckTimeoutSeconds: 5
      HealthyThresholdCount: 2
      UnhealthyThresholdCount: 3
      
      # ê³ ê¸‰ ì„¤ì •
      TargetGroupAttributes:
      - Key: deregistration_delay.timeout_seconds
        Value: '30'  # ë¹ ë¥¸ ë“œë ˆì´ë‹
      - Key: stickiness.enabled
        Value: 'false'  # ì„¸ì…˜ ê³ ì • ë¹„í™œì„±í™”
        
  Listener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      DefaultActions:
      - Type: forward
        TargetGroupArn: !Ref TargetGroup
      LoadBalancerArn: !Ref ApplicationLoadBalancer
      Port: 80
      Protocol: HTTP
```

### ğŸª ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í”Œë«í¼

#### ğŸ³ Kubernetes í´ëŸ¬ìŠ¤í„° ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬

```yaml
# Cluster Autoscaler ë°°í¬
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/eks-cluster-name
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-util-threshold=0.5

---
# í´ëŸ¬ìŠ¤í„° ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ê¶Œí•œ
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/ClusterAutoscalerRole

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["events", "endpoints"]
  verbs: ["create", "patch"]
- apiGroups: [""]
  resources: ["pods/eviction"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["pods/status"]
  verbs: ["update"]
- apiGroups: [""]
  resources: ["endpoints"]
  resourceNames: ["cluster-autoscaler"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["watch", "list", "get", "update"]
- apiGroups: [""]
  resources: ["pods", "services", "replicationcontrollers", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["extensions"]
  resources: ["replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["watch", "list"]
- apiGroups: ["apps"]
  resources: ["statefulsets", "replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses", "csinodes"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "patch"]
```

---

## 5. í™•ì¥ ì •ì±… ì„¤ê³„

### ğŸ“ ë©”íŠ¸ë¦­ ì„ íƒ

> [!tip] ë©”íŠ¸ë¦­ ì„ íƒ ê¸°ì¤€
> í™•ì¥ ê²°ì •ì˜ ì •í™•ì„±ì€ ì˜¬ë°”ë¥¸ ë©”íŠ¸ë¦­ ì„ íƒì— ë‹¬ë ¤ìˆìŠµë‹ˆë‹¤. ë‹¨ì¼ ë©”íŠ¸ë¦­ë³´ë‹¤ëŠ” ë³µí•© ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

#### ğŸ¯ ë©”íŠ¸ë¦­ ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤

| ë©”íŠ¸ë¦­ | ë°˜ì‘ì„± | ì˜ˆì¸¡ì„± | ì•ˆì •ì„± | ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ê´€ì„± | ê¶Œì¥ë„ |
|--------|--------|--------|--------|----------------|--------|
| **CPU ì‚¬ìš©ë¥ ** | â­â­â­ | â­â­ | â­â­â­ | â­â­ | â­â­â­ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ** | â­â­ | â­â­ | â­â­â­ | â­â­ | â­â­â­ |
| **ìš”ì²­ ìˆ˜ (RPS)** | â­â­â­ | â­â­â­ | â­â­ | â­â­â­ | â­â­â­â­ |
| **ì‘ë‹µ ì‹œê°„** | â­â­â­ | â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­ |
| **ì—ëŸ¬ìœ¨** | â­â­ | â­ | â­â­ | â­â­â­â­ | â­â­â­ |
| **í ê¸¸ì´** | â­â­â­ | â­â­ | â­â­ | â­â­â­ | â­â­â­ |
| **ë™ì‹œ ì‚¬ìš©ì** | â­â­ | â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­ |

#### ğŸ’» ë³µí•© ë©”íŠ¸ë¦­ ìŠ¤ì½”ì–´ë§

```python
class CompositeMetricEvaluator:
    def __init__(self):
        self.weights = {
            'cpu_usage': 0.25,
            'memory_usage': 0.15,
            'response_time': 0.30,
            'error_rate': 0.15,
            'queue_length': 0.10,
            'concurrent_users': 0.05
        }
        
        self.thresholds = {
            'cpu_usage': {'scale_up': 70, 'scale_down': 30},
            'memory_usage': {'scale_up': 80, 'scale_down': 40},
            'response_time': {'scale_up': 2.0, 'scale_down': 0.5},
            'error_rate': {'scale_up': 5.0, 'scale_down': 1.0},
            'queue_length': {'scale_up': 100, 'scale_down': 10},
            'concurrent_users': {'scale_up': 1000, 'scale_down': 200}
        }
    
    def calculate_scaling_score(self, metrics):
        """ë³µí•© ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ì ìˆ˜ ê³„ì‚°"""
        scale_up_score = 0
        scale_down_score = 0
        
        for metric_name, value in metrics.items():
            if metric_name not in self.weights:
                continue
                
            weight = self.weights[metric_name]
            thresholds = self.thresholds[metric_name]
            
            # í™•ì¥ ì ìˆ˜ ê³„ì‚°
            if value > thresholds['scale_up']:
                urgency = min((value - thresholds['scale_up']) / thresholds['scale_up'], 1.0)
                scale_up_score += weight * urgency
            
            # ì¶•ì†Œ ì ìˆ˜ ê³„ì‚°  
            elif value < thresholds['scale_down']:
                opportunity = min((thresholds['scale_down'] - value) / thresholds['scale_down'], 1.0)
                scale_down_score += weight * opportunity
        
        return {
            'scale_up_score': scale_up_score,
            'scale_down_score': scale_down_score,
            'recommendation': self.get_recommendation(scale_up_score, scale_down_score)
        }
    
    def get_recommendation(self, up_score, down_score):
        """ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­"""
        if up_score > 0.6:
            return {'action': 'scale_up', 'urgency': 'high'}
        elif up_score > 0.3:
            return {'action': 'scale_up', 'urgency': 'medium'}
        elif down_score > 0.5:
            return {'action': 'scale_down', 'urgency': 'low'}
        else:
            return {'action': 'maintain', 'urgency': 'none'}
```

### ğŸšï¸ ì„ê³„ê°’ ì„¤ì •

#### ğŸ“Š ì ì‘í˜• ì„ê³„ê°’ ì‹œìŠ¤í…œ

```python
import numpy as np
from collections import deque
import statistics

class AdaptiveThresholdManager:
    def __init__(self, window_size=288):  # 24ì‹œê°„ * 12 (5ë¶„ ë‹¨ìœ„)
        self.window_size = window_size
        self.metric_history = {
            'cpu_usage': deque(maxlen=window_size),
            'response_time': deque(maxlen=window_size),
            'request_rate': deque(maxlen=window_size)
        }
        
    def update_history(self, metrics):
        """ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        for metric_name, value in metrics.items():
            if metric_name in self.metric_history:
                self.metric_history[metric_name].append(value)
    
    def calculate_adaptive_thresholds(self, metric_name):
        """ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚°"""
        if metric_name not in self.metric_history:
            return self.get_default_thresholds(metric_name)
        
        history = list(self.metric_history[metric_name])
        if len(history) < 50:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            return self.get_default_thresholds(metric_name)
        
        # í†µê³„ì  ë¶„ì„
        mean = statistics.mean(history)
        std_dev = statistics.stdev(history)
        p95 = np.percentile(history, 95)
        p05 = np.percentile(history, 5)
        
        # ê³„ì ˆì„± ê³ ë ¤ (ê°™ì€ ì‹œê°„ëŒ€ ê¸°ë¡)
        seasonal_data = self.get_seasonal_data(metric_name)
        seasonal_mean = statistics.mean(seasonal_data) if seasonal_data else mean
        
        # ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚°
        scale_up_threshold = max(
            seasonal_mean + 2 * std_dev,
            p95 * 0.9,
            self.get_default_thresholds(metric_name)['scale_up']
        )
        
        scale_down_threshold = min(
            seasonal_mean - std_dev,
            p05 * 1.2,
            self.get_default_thresholds(metric_name)['scale_down']
        )
        
        return {
            'scale_up': scale_up_threshold,
            'scale_down': scale_down_threshold,
            'confidence': self.calculate_confidence(history),
            'seasonal_adjustment': seasonal_mean - mean
        }
    
    def get_seasonal_data(self, metric_name, hours_back=24):
        """ê°™ì€ ì‹œê°„ëŒ€ì˜ ê³¼ê±° ë°ì´í„° ì¶”ì¶œ"""
        history = list(self.metric_history[metric_name])
        current_hour = datetime.now().hour
        
        # 24ì‹œê°„ ì£¼ê¸°ë¡œ ê°™ì€ ì‹œê°„ëŒ€ ë°ì´í„° ì¶”ì¶œ
        seasonal_indices = []
        for i in range(len(history)):
            data_hour = (current_hour - (len(history) - i) * 5 // 60) % 24
            if data_hour == current_hour:
                seasonal_indices.append(i)
        
        return [history[i] for i in seasonal_indices]
    
    def get_default_thresholds(self, metric_name):
        """ê¸°ë³¸ ì„ê³„ê°’"""
        defaults = {
            'cpu_usage': {'scale_up': 70, 'scale_down': 30},
            'response_time': {'scale_up': 2.0, 'scale_down': 0.5},
            'request_rate': {'scale_up': 1000, 'scale_down': 200}
        }
        return defaults.get(metric_name, {'scale_up': 70, 'scale_down': 30})
```

### â±ï¸ ì¿¨ë‹¤ìš´ ê¸°ê°„ (Cooldown Period)

> [!warning] ì¿¨ë‹¤ìš´ì˜ ì¤‘ìš”ì„±
> ì¿¨ë‹¤ìš´ ê¸°ê°„ì€ í™•ì¥ í›„ ì•ˆì •í™” ì‹œê°„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë„ˆë¬´ ë¹ˆë²ˆí•œ í™•ì¥/ì¶•ì†Œë¥¼ ë°©ì§€í•˜ê³  ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

#### âš™ï¸ ì§€ëŠ¥í˜• ì¿¨ë‹¤ìš´ ì‹œìŠ¤í…œ

```python
import time
from enum import Enum

class ScalingAction(Enum):
    SCALE_UP = "scale_up"
    SCALE_DOWN = "scale_down"
    MAINTAIN = "maintain"

class IntelligentCooldownManager:
    def __init__(self):
        self.scaling_history = deque(maxlen=50)
        self.base_cooldown = {
            ScalingAction.SCALE_UP: 300,    # 5ë¶„
            ScalingAction.SCALE_DOWN: 900   # 15ë¶„ (ë³´ìˆ˜ì )
        }
        
    def calculate_dynamic_cooldown(self, proposed_action, current_metrics):
        """ë™ì  ì¿¨ë‹¤ìš´ ì‹œê°„ ê³„ì‚°"""
        base_time = self.base_cooldown.get(proposed_action, 300)
        
        # ìµœê·¼ ìŠ¤ì¼€ì¼ë§ ë¹ˆë„ í™•ì¸
        recent_scalings = self.get_recent_scalings(600)  # 10ë¶„ê°„
        frequency_factor = min(len(recent_scalings) * 0.5, 2.0)
        
        # ë©”íŠ¸ë¦­ ê¸´ê¸‰ë„ í™•ì¸
        urgency_factor = self.calculate_urgency_factor(current_metrics)
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ ìƒíƒœ í™•ì¸
        load_factor = self.calculate_load_factor(current_metrics)
        
        # ì¿¨ë‹¤ìš´ ì‹œê°„ ì¡°ì •
        if proposed_action == ScalingAction.SCALE_UP:
            # í™•ì¥ì€ ê¸´ê¸‰ë„ì— ë”°ë¼ ì¿¨ë‹¤ìš´ ë‹¨ì¶• ê°€ëŠ¥
            cooldown = base_time * frequency_factor / max(urgency_factor, 0.5)
        else:
            # ì¶•ì†ŒëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì²˜ë¦¬
            cooldown = base_time * frequency_factor * load_factor
        
        return max(cooldown, 60), {
            'base_time': base_time,
            'frequency_factor': frequency_factor,
            'urgency_factor': urgency_factor,
            'load_factor': load_factor
        }
    
    def calculate_urgency_factor(self, metrics):
        """ê¸´ê¸‰ë„ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ë¹ ë¥¸ ëŒ€ì‘ í•„ìš”)"""
        urgency_score = 0
        
        # CPU ì‚¬ìš©ë¥  ê¸°ë°˜ ê¸´ê¸‰ë„
        if metrics.get('cpu_usage', 0) > 90:
            urgency_score += 2.0
        elif metrics.get('cpu_usage', 0) > 80:
            urgency_score += 1.0
        
        # ì‘ë‹µì‹œê°„ ê¸°ë°˜ ê¸´ê¸‰ë„
        if metrics.get('response_time', 0) > 5.0:
            urgency_score += 2.0
        elif metrics.get('response_time', 0) > 2.0:
            urgency_score += 1.0
        
        # ì—ëŸ¬ìœ¨ ê¸°ë°˜ ê¸´ê¸‰ë„
        if metrics.get('error_rate', 0) > 10:
            urgency_score += 3.0
        elif metrics.get('error_rate', 0) > 5:
            urgency_score += 1.5
        
        return max(urgency_score, 1.0)
    
    def is_cooldown_active(self, action_type):
        """ì¿¨ë‹¤ìš´ í™œì„± ìƒíƒœ í™•ì¸"""
        if not self.scaling_history:
            return False
        
        last_action = self.scaling_history[-1]
        if last_action['action'] != action_type:
            return False
        
        elapsed_time = time.time() - last_action['timestamp']
        return elapsed_time < last_action['cooldown_duration']
    
    def record_scaling_action(self, action, instance_change, cooldown_duration, reason=""):
        """ìŠ¤ì¼€ì¼ë§ ì•¡ì…˜ ê¸°ë¡"""
        self.scaling_history.append({
            'action': action,
            'instance_change': instance_change,
            'timestamp': time.time(),
            'cooldown_duration': cooldown_duration,
            'reason': reason
        })
```

### ğŸšï¸ í™•ì¥ ì†ë„ ì œì–´

#### ğŸ“Š ë‹¨ê³„ì  í™•ì¥ ì „ëµ

```python
class GradualScalingManager:
    def __init__(self):
        self.scaling_steps = {
            'conservative': {
                'scale_up_increment': 1,
                'scale_down_increment': 1,
                'max_instances_per_step': 2
            },
            'moderate': {
                'scale_up_increment': 2,
                'scale_down_increment': 1,
                'max_instances_per_step': 5
            },
            'aggressive': {
                'scale_up_increment': 4,
                'scale_down_increment': 2,
                'max_instances_per_step': 10
            }
        }
    
    def calculate_scaling_step(self, current_instances, target_instances, urgency_level):
        """ë‹¨ê³„ì  í™•ì¥ ê³„ì‚°"""
        strategy = self.get_strategy_by_urgency(urgency_level)
        difference = target_instances - current_instances
        
        if difference > 0:  # Scale up
            step_size = min(
                strategy['scale_up_increment'],
                strategy['max_instances_per_step'],
                abs(difference)
            )
            return current_instances + step_size
        
        elif difference < 0:  # Scale down
            step_size = min(
                strategy['scale_down_increment'],
                abs(difference)
            )
            return current_instances - step_size
        
        return current_instances
    
    def get_strategy_by_urgency(self, urgency_level):
        """ê¸´ê¸‰ë„ë³„ ì „ëµ ì„ íƒ"""
        if urgency_level >= 3.0:
            return self.scaling_steps['aggressive']
        elif urgency_level >= 1.5:
            return self.scaling_steps['moderate']
        else:
            return self.scaling_steps['conservative']

# í”„ë¡œê·¸ë ˆì‹œë¸Œ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„
class ProgressiveScaler:
    def __init__(self):
        self.scaling_phases = [
            {'threshold': 70, 'action': '+20%', 'wait_time': 300},   # 1ë‹¨ê³„: 20% ì¦ê°€
            {'threshold': 80, 'action': '+50%', 'wait_time': 180},   # 2ë‹¨ê³„: 50% ì¦ê°€  
            {'threshold': 90, 'action': '+100%', 'wait_time': 60},   # 3ë‹¨ê³„: 100% ì¦ê°€
            {'threshold': 95, 'action': 'emergency', 'wait_time': 30}  # ì‘ê¸‰ë‹¨ê³„
        ]
    
    def get_scaling_phase(self, cpu_usage, duration_seconds):
        """í˜„ì¬ ìƒí™©ì— ë§ëŠ” ìŠ¤ì¼€ì¼ë§ ë‹¨ê³„ ë°˜í™˜"""
        for phase in self.scaling_phases:
            if cpu_usage >= phase['threshold']:
                if duration_seconds >= phase['wait_time']:
                    return phase
        
        return None  # ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”
```

---

## 6. ì‹¤ì œ ì ìš© ì‚¬ë¡€

### ğŸŒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜

> [!example] ì¼ë°˜ì ì¸ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í™•ì¥
> ë¡œë“œ ë°¸ëŸ°ì„œ ë’¤ì— ì—¬ëŸ¬ ì›¹ ì„œë²„ë¥¼ ë°°ì¹˜í•˜ê³ , íŠ¸ë˜í”½ì— ë”°ë¼ ì„œë²„ ìˆ˜ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.

#### ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì„±

```yaml
# ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ Auto Scaling ì•„í‚¤í…ì²˜
architecture:
  load_balancer:
    type: "Application Load Balancer"
    health_check:
      path: "/health"
      interval: 30
      timeout: 5
      healthy_threshold: 2
      unhealthy_threshold: 3
    
  web_servers:
    auto_scaling_group:
      min_size: 3
      max_size: 30
      desired_capacity: 6
      instance_type: "t3.medium"
      
    scaling_policies:
      scale_up:
        metric: "CPUUtilization"
        threshold: 70
        comparison: "GreaterThanThreshold"
        evaluation_periods: 2
        period: 300
        scaling_adjustment: 2
        
      scale_down:
        metric: "CPUUtilization" 
        threshold: 30
        comparison: "LessThanThreshold"
        evaluation_periods: 3
        period: 300
        scaling_adjustment: -1
    
  session_store:
    type: "Redis Cluster"
    purpose: "Stateless web servers"
    configuration:
      nodes: 3
      replication: true
      persistence: true
```

#### ğŸ’» Stateless ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬í˜„

```python
# Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜ˆì‹œ (Stateless ì„¤ê³„)
from flask import Flask, session, request, jsonify
import redis
import json
import os

app = Flask(__name__)

# Redis ì„¸ì…˜ ìŠ¤í† ì–´ ì„¤ì •
redis_client = redis.Redis(
    host=os.environ.get('REDIS_HOST', 'localhost'),
    port=int(os.environ.get('REDIS_PORT', 6379)),
    db=0,
    decode_responses=True
)

class StatelessSessionInterface:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.session_timeout = 3600  # 1ì‹œê°„
    
    def get_session(self, session_id):
        """Redisì—ì„œ ì„¸ì…˜ ë°ì´í„° ì¡°íšŒ"""
        session_data = self.redis.get(f"session:{session_id}")
        return json.loads(session_data) if session_data else {}
    
    def set_session(self, session_id, data):
        """Redisì— ì„¸ì…˜ ë°ì´í„° ì €ì¥"""
        self.redis.setex(
            f"session:{session_id}", 
            self.session_timeout, 
            json.dumps(data)
        )
    
    def delete_session(self, session_id):
        """ì„¸ì…˜ ì‚­ì œ"""
        self.redis.delete(f"session:{session_id}")

session_interface = StatelessSessionInterface(redis_client)

@app.route('/health')
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # Redis ì—°ê²° í™•ì¸
        redis_client.ping()
        return jsonify({
            'status': 'healthy',
            'instance_id': os.environ.get('INSTANCE_ID', 'unknown'),
            'timestamp': time.time()
        }), 200
    except:
        return jsonify({'status': 'unhealthy'}), 503

@app.route('/api/user/profile')
def get_user_profile():
    """ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (ì„¸ì…˜ ê¸°ë°˜)"""
    session_id = request.headers.get('Session-ID')
    if not session_id:
        return jsonify({'error': 'Session required'}), 401
    
    session_data = session_interface.get_session(session_id)
    user_id = session_data.get('user_id')
    
    if not user_id:
        return jsonify({'error': 'Invalid session'}), 401
    
    # ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ)
    user_profile = get_user_from_db(user_id)
    
    return jsonify({
        'user': user_profile,
        'served_by': os.environ.get('INSTANCE_ID', 'unknown')
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### ğŸ”¬ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

#### ğŸ­ ì„œë¹„ìŠ¤ë³„ ë…ë¦½ í™•ì¥

```yaml
# ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë³„ HPA ì„¤ì •
microservices:
  user_service:
    scaling_policy:
      min_replicas: 2
      max_replicas: 20
      target_cpu: 60
      target_memory: 70
    custom_metrics:
      - name: "user_login_rate"
        target_value: "100"
      - name: "user_registration_rate"
        target_value: "50"
  
  product_service:
    scaling_policy:
      min_replicas: 3
      max_replicas: 50
      target_cpu: 70
      target_memory: 80
    custom_metrics:
      - name: "product_search_rate"
        target_value: "500"
      - name: "product_view_rate"
        target_value: "1000"
  
  order_service:
    scaling_policy:
      min_replicas: 5
      max_replicas: 100
      target_cpu: 50  # ë” ë³´ìˆ˜ì 
      target_memory: 70
    custom_metrics:
      - name: "order_creation_rate"
        target_value: "200"
      - name: "payment_processing_queue"
        target_value: "50"
  
  notification_service:
    scaling_policy:
      min_replicas: 1
      max_replicas: 10
      target_cpu: 80  # ë†’ì€ ì‚¬ìš©ë¥  í—ˆìš©
    custom_metrics:
      - name: "notification_queue_length"
        target_value: "100"
```

#### ğŸ“Š ì„œë¹„ìŠ¤ ë©”ì‹œ ê¸°ë°˜ ë©”íŠ¸ë¦­

```yaml
# Istio Service Mesh ë©”íŠ¸ë¦­ ê¸°ë°˜ Auto Scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 3
  maxReplicas: 50
  metrics:
  # CPU ê¸°ë°˜
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  
  # ìš”ì²­ ê¸°ë°˜ (Istio ë©”íŠ¸ë¦­)
  - type: Object
    object:
      metric:
        name: istio_requests_per_second
      target:
        type: Value
        value: "500"
      describedObject:
        apiVersion: v1
        kind: Service
        name: order-service
  
  # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ (Istio ë©”íŠ¸ë¦­)
  - type: Object
    object:
      metric:
        name: istio_request_duration_p99
      target:
        type: Value
        value: "2000"  # 2ì´ˆ
      describedObject:
        apiVersion: v1
        kind: Service
        name: order-service
```

### ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤

#### ğŸ“Š ì½ê¸° ë³µì œë³¸ í™•ì¥

```python
class DatabaseScalingManager:
    def __init__(self, rds_client):
        self.rds = rds_client
        self.read_replica_config = {
            'db_instance_class': 'db.t3.medium',
            'publicly_accessible': False,
            'auto_minor_version_upgrade': True
        }
    
    def monitor_database_load(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë¶€í•˜ ëª¨ë‹ˆí„°ë§"""
        metrics = self.get_db_metrics()
        
        read_load = metrics['read_iops']
        write_load = metrics['write_iops']
        cpu_usage = metrics['cpu_utilization']
        connection_count = metrics['database_connections']
        
        # ì½ê¸° ë¶€í•˜ê°€ ë†’ìœ¼ë©´ ì½ê¸° ë³µì œë³¸ ì¶”ê°€
        if read_load > 1000 and cpu_usage > 70:
            self.scale_read_replicas_up()
        
        # ì“°ê¸° ë¶€í•˜ê°€ ë†’ìœ¼ë©´ ë” í° ì¸ìŠ¤í„´ìŠ¤ë¡œ ìˆ˜ì§ í™•ì¥
        elif write_load > 500 and cpu_usage > 80:
            self.scale_master_instance_up()
    
    def scale_read_replicas_up(self):
        """ì½ê¸° ë³µì œë³¸ ì¶”ê°€"""
        current_replicas = self.get_read_replica_count()
        
        if current_replicas < 5:  # ìµœëŒ€ 5ê°œ ì œí•œ
            replica_id = f"read-replica-{current_replicas + 1}"
            
            response = self.rds.create_db_instance_read_replica(
                DBInstanceIdentifier=replica_id,
                SourceDBInstanceIdentifier='master-db',
                **self.read_replica_config
            )
            
            print(f"Created read replica: {replica_id}")
            return response
    
    def scale_master_instance_up(self):
        """ë§ˆìŠ¤í„° ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì§ í™•ì¥"""
        current_class = self.get_master_instance_class()
        next_class = self.get_next_instance_class(current_class)
        
        if next_class:
            response = self.rds.modify_db_instance(
                DBInstanceIdentifier='master-db',
                DBInstanceClass=next_class,
                ApplyImmediately=True
            )
            
            print(f"Scaled master from {current_class} to {next_class}")
            return response
```

#### ğŸ”€ Aurora Serverless í™œìš©

```yaml
# Aurora Serverless v2 ì„¤ì •
Resources:
  AuroraServerlessCluster:
    Type: AWS::RDS::DBCluster
    Properties:
      Engine: aurora-mysql
      EngineMode: provisioned
      EngineVersion: '8.0.mysql_aurora.3.02.0'
      DatabaseName: myapp
      MasterUsername: admin
      MasterUserPassword: !Ref DatabasePassword
      
      # Serverless v2 ì„¤ì •
      ServerlessV2ScalingConfiguration:
        MinCapacity: 0.5      # ìµœì†Œ 0.5 ACU
        MaxCapacity: 16       # ìµœëŒ€ 16 ACU
      
      # ë°±ì—… ì„¤ì •
      BackupRetentionPeriod: 7
      PreferredBackupWindow: "03:00-04:00"
      PreferredMaintenanceWindow: "sun:04:00-sun:05:00"
      
      # ë³´ì•ˆ ì„¤ì •
      VpcSecurityGroupIds: [!Ref DatabaseSecurityGroup]
      DBSubnetGroupName: !Ref DBSubnetGroup
```

### ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬

#### âš™ï¸ í ê¸°ë°˜ ì›Œì»¤ í™•ì¥

```python
import boto3
from kubernetes import client, config

class BatchProcessingScaler:
    def __init__(self):
        self.sqs = boto3.client('sqs')
        self.k8s_apps = client.AppsV1Api()
        
    def scale_workers_by_queue_length(self, queue_url, deployment_name):
        """í ê¸¸ì´ ê¸°ë°˜ ì›Œì»¤ í™•ì¥"""
        # SQS í ê¸¸ì´ í™•ì¸
        response = self.sqs.get_queue_attributes(
            QueueUrl=queue_url,
            AttributeNames=['ApproximateNumberOfMessages', 
                          'ApproximateNumberOfMessagesNotVisible']
        )
        
        visible_messages = int(response['Attributes']['ApproximateNumberOfMessages'])
        in_flight_messages = int(response['Attributes']['ApproximateNumberOfMessagesNotVisible'])
        total_messages = visible_messages + in_flight_messages
        
        # ì›Œì»¤ ìˆ˜ ê³„ì‚° (ë©”ì‹œì§€ 10ê°œë‹¹ ì›Œì»¤ 1ê°œ)
        target_workers = max(1, min(total_messages // 10, 20))  # ìµœì†Œ 1ê°œ, ìµœëŒ€ 20ê°œ
        
        # í˜„ì¬ ì›Œì»¤ ìˆ˜ í™•ì¸
        deployment = self.k8s_apps.read_namespaced_deployment(
            name=deployment_name,
            namespace='default'
        )
        current_workers = deployment.spec.replicas
        
        # ì›Œì»¤ ìˆ˜ ì¡°ì •
        if target_workers != current_workers:
            deployment.spec.replicas = target_workers
            self.k8s_apps.patch_namespaced_deployment(
                name=deployment_name,
                namespace='default',
                body=deployment
            )
            
            print(f"Scaled workers from {current_workers} to {target_workers} "
                  f"(queue length: {total_messages})")
        
        return {
            'queue_length': total_messages,
            'current_workers': current_workers,
            'target_workers': target_workers,
            'action': 'scaled' if target_workers != current_workers else 'no_change'
        }
```

#### â° ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œìš©í•œ ë¹„ìš© íš¨ìœ¨ í™•ì¥

```yaml
# Spot Instance ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ Auto Scaling Group
Resources:
  BatchProcessingASG:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      AutoScalingGroupName: BatchProcessing-ASG
      VPCZoneIdentifier: [!Ref PrivateSubnet1, !Ref PrivateSubnet2]
      
      # Mixed Instance Policy (ì˜¨ë””ë§¨ë“œ + ìŠ¤íŒŸ)
      MixedInstancesPolicy:
        InstancesDistribution:
          OnDemandBaseCapacity: 0           # ê¸°ë³¸ ì˜¨ë””ë§¨ë“œ ì¸ìŠ¤í„´ìŠ¤ 0ê°œ
          OnDemandPercentageAboveBaseCapacity: 20  # 20%ë§Œ ì˜¨ë””ë§¨ë“œ
          SpotAllocationStrategy: diversified
          SpotInstancePools: 4
          SpotMaxPrice: "0.10"              # ì‹œê°„ë‹¹ ìµœëŒ€ $0.10
          
        LaunchTemplate:
          LaunchTemplateSpecification:
            LaunchTemplateId: !Ref BatchProcessingLaunchTemplate
            Version: !GetAtt BatchProcessingLaunchTemplate.LatestVersionNumber
          Overrides:
          - InstanceType: c5.large
            WeightedCapacity: 1
          - InstanceType: c5.xlarge
            WeightedCapacity: 2
          - InstanceType: m5.large
            WeightedCapacity: 1
          - InstanceType: m5.xlarge
            WeightedCapacity: 2
      
      MinSize: 0
      MaxSize: 50
      DesiredCapacity: 0
      
      # ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ì²˜ë¦¬
      TerminationPolicies: 
        - "OldestInstance"
        - "Default"

  # SQS í ê¸¸ì´ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ì •ì±…
  QueueLengthScalingPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      AutoScalingGroupName: !Ref BatchProcessingASG
      PolicyType: TargetTrackingScaling
      TargetTrackingConfiguration:
        CustomMetricSpecification:
          MetricName: ApproximateNumberOfVisibleMessages
          Namespace: AWS/SQS
          Statistic: Average
          Dimensions:
          - Name: QueueName
            Value: !GetAtt ProcessingQueue.QueueName
        TargetValue: 30.0         # ë©”ì‹œì§€ 30ê°œë‹¹ ì¸ìŠ¤í„´ìŠ¤ 1ê°œ
        ScaleOutCooldown: 300     # 5ë¶„ í™•ì¥ ì¿¨ë‹¤ìš´
        ScaleInCooldown: 900      # 15ë¶„ ì¶•ì†Œ ì¿¨ë‹¤ìš´
```

---

## 7. ì¥ì ê³¼ ê³¼ì œ

### âœ… ì¥ì 

#### ğŸ’° ë¹„ìš© íš¨ìœ¨ì„±

> [!tip] ë¹„ìš© ì ˆê° íš¨ê³¼
> í•„ìš”í•œ ë§Œí¼ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê·  40-60% ë¹„ìš© ì ˆê°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
# ë¹„ìš© ì ˆê° ë¶„ì„ ë„êµ¬
class CostSavingsAnalyzer:
    def __init__(self):
        self.instance_cost_per_hour = {
            't3.medium': 0.0416,
            't3.large': 0.0832,
            't3.xlarge': 0.1664,
            'm5.large': 0.096,
            'm5.xlarge': 0.192
        }
        
    def calculate_savings(self, baseline_scenario, autoscaling_scenario):
        """Auto Scaling vs ê³ ì • ìš©ëŸ‰ ë¹„ìš© ë¹„êµ"""
        
        # ê³ ì • ìš©ëŸ‰ ë¹„ìš© (í”¼í¬ ìš©ëŸ‰ ê¸°ì¤€)
        fixed_cost = self.calculate_fixed_capacity_cost(baseline_scenario)
        
        # Auto Scaling ë¹„ìš©
        dynamic_cost = self.calculate_dynamic_capacity_cost(autoscaling_scenario)
        
        savings = fixed_cost - dynamic_cost
        savings_percentage = (savings / fixed_cost) * 100
        
        return {
            'fixed_cost': fixed_cost,
            'dynamic_cost': dynamic_cost,
            'savings': savings,
            'savings_percentage': savings_percentage,
            'monthly_savings': savings * 24 * 30  # ì›”ê°„ ì ˆì•½ì•¡
        }
    
    def calculate_fixed_capacity_cost(self, scenario):
        """ê³ ì • ìš©ëŸ‰ ë¹„ìš© ê³„ì‚°"""
        instance_type = scenario['instance_type']
        peak_instances = scenario['peak_instances']
        hours_per_day = 24
        
        return (self.instance_cost_per_hour[instance_type] * 
                peak_instances * hours_per_day)
    
    def calculate_dynamic_capacity_cost(self, scenario):
        """ë™ì  ìš©ëŸ‰ ë¹„ìš© ê³„ì‚°"""
        total_cost = 0
        
        for period in scenario['usage_patterns']:
            instance_type = period['instance_type']
            instance_count = period['instance_count']
            duration_hours = period['duration_hours']
            
            cost = (self.instance_cost_per_hour[instance_type] * 
                   instance_count * duration_hours)
            total_cost += cost
        
        return total_cost

# ì‚¬ìš© ì˜ˆì‹œ
analyzer = CostSavingsAnalyzer()

baseline = {
    'instance_type': 'm5.large',
    'peak_instances': 20  # í”¼í¬ ê¸°ì¤€ìœ¼ë¡œ í•­ìƒ 20ê°œ ìœ ì§€
}

autoscaling = {
    'usage_patterns': [
        {'instance_type': 'm5.large', 'instance_count': 5, 'duration_hours': 8},   # ì•¼ê°„
        {'instance_type': 'm5.large', 'instance_count': 12, 'duration_hours': 8},  # ì¼ë°˜
        {'instance_type': 'm5.large', 'instance_count': 20, 'duration_hours': 4},  # í”¼í¬
        {'instance_type': 'm5.large', 'instance_count': 8, 'duration_hours': 4}    # ì €ë…
    ]
}

savings = analyzer.calculate_savings(baseline, autoscaling)
print(f"Monthly savings: ${savings['monthly_savings']:.2f} ({savings['savings_percentage']:.1f}%)")
```

#### ğŸ›¡ï¸ ê°€ìš©ì„±ê³¼ ì•ˆì •ì„±

```yaml
# Multi-AZ Auto Scalingìœ¼ë¡œ ê³ ê°€ìš©ì„± í™•ë³´
high_availability_config:
  availability_zones:
    - us-west-2a
    - us-west-2b  
    - us-west-2c
  
  distribution_strategy: "balanced"
  
  health_checks:
    - type: "EC2"
      grace_period: 300
    - type: "ELB" 
      grace_period: 300
  
  failure_recovery:
    auto_replace_unhealthy: true
    max_instance_lifetime: 604800  # 7ì¼
    
  disaster_recovery:
    cross_region_backup: true
    rto: "< 5 minutes"     # Recovery Time Objective
    rpo: "< 1 minute"      # Recovery Point Objective
```

#### âš¡ ìš´ì˜ íš¨ìœ¨ì„±

- **ìˆ˜ë™ ê°œì… ê°ì†Œ**: 95% ìë™í™”ë¡œ ìš´ì˜ ë¶€ë‹´ ëŒ€í­ ê°ì†Œ
- **24/7 ëª¨ë‹ˆí„°ë§**: ë¬´ì¸ ìƒíƒœì—ì„œë„ ì•ˆì •ì  ìš´ì˜
- **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥**: SLA ëª©í‘œ ë‹¬ì„±ë¥  99.9% ì´ìƒ

### âš ï¸ ê³¼ì œì™€ ê³ ë ¤ì‚¬í•­

#### ğŸ”„ ìƒíƒœ ê´€ë¦¬

> [!warning] ìƒíƒœ ê´€ë¦¬ì˜ ë³µì¡ì„±
> ì¸ìŠ¤í„´ìŠ¤ê°€ ì¶”ê°€/ì œê±°ë˜ë¯€ë¡œ ë¡œì»¬ ìƒíƒœì— ì˜ì¡´í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.

#### ğŸ“Š Stateless ì•„í‚¤í…ì²˜ íŒ¨í„´

```python
# ìƒíƒœ ì™¸ë¶€í™” ì˜ˆì‹œ
class StatelessWebApplication:
    def __init__(self):
        # ì„¸ì…˜ ì €ì¥ì†Œ
        self.session_store = RedisSessionStore()
        
        # íŒŒì¼ ì €ì¥ì†Œ  
        self.file_store = S3FileStore()
        
        # ìºì‹œ ì €ì¥ì†Œ
        self.cache = RedisCache()
        
        # ë°ì´í„°ë² ì´ìŠ¤
        self.database = DatabaseConnection()
    
    def handle_request(self, request):
        """ìƒíƒœê°€ ì—†ëŠ” ìš”ì²­ ì²˜ë¦¬"""
        
        # ì„¸ì…˜ì€ Redisì—ì„œ ì¡°íšŒ
        session = self.session_store.get_session(request.session_id)
        
        # íŒŒì¼ì€ S3ì—ì„œ ì¡°íšŒ
        if request.file_id:
            file_data = self.file_store.get_file(request.file_id)
        
        # ìºì‹œëŠ” Redisì—ì„œ ì¡°íšŒ
        cache_key = f"user:{session.user_id}"
        cached_data = self.cache.get(cache_key)
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬ (ë¬´ìƒíƒœ)
        response = self.process_business_logic(request, session, cached_data)
        
        # ì„¸ì…˜ ì—…ë°ì´íŠ¸
        self.session_store.update_session(session)
        
        return response

# Session Store êµ¬í˜„
class RedisSessionStore:
    def __init__(self):
        self.redis = redis.Redis(
            host='redis-cluster.cache.amazonaws.com',
            port=6379,
            decode_responses=True
        )
    
    def get_session(self, session_id):
        session_data = self.redis.hgetall(f"session:{session_id}")
        return Session(session_data) if session_data else None
    
    def update_session(self, session):
        session_key = f"session:{session.id}"
        self.redis.hmset(session_key, session.to_dict())
        self.redis.expire(session_key, 3600)  # 1ì‹œê°„ TTL
```

#### â±ï¸ í™•ì¥ ì§€ì—°

> [!info] ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œ
> ìƒˆ ì¸ìŠ¤í„´ìŠ¤ê°€ ì‹œì‘ë˜ê³  ì¤€ë¹„ë˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë ¤ ê¸‰ê²©í•œ íŠ¸ë˜í”½ ì¦ê°€ì—ëŠ” ëŒ€ì‘ì´ ëŠ¦ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ğŸƒâ€â™‚ï¸ ì›œì—… ì „ëµ êµ¬í˜„

```python
class WarmupStrategy:
    def __init__(self):
        self.warmup_pool = WarmupPool()
        self.predictive_scaler = PredictiveScaler()
        
    def maintain_warm_instances(self):
        """ì›œì—… ì¸ìŠ¤í„´ìŠ¤ ìœ ì§€"""
        predicted_load = self.predictive_scaler.predict_next_hour()
        current_capacity = self.get_current_capacity()
        
        if predicted_load > current_capacity * 1.2:
            # ì˜ˆìƒ ë¶€í•˜ê°€ í˜„ì¬ ìš©ëŸ‰ë³´ë‹¤ 20% ë†’ìœ¼ë©´ ë¯¸ë¦¬ í™•ì¥
            additional_instances = int((predicted_load - current_capacity) * 1.1)
            self.warmup_pool.prepare_instances(additional_instances)
    
    def fast_scale_from_warmup(self):
        """ì›œì—… í’€ì—ì„œ ë¹ ë¥¸ í™•ì¥"""
        warm_instances = self.warmup_pool.get_ready_instances()
        
        for instance in warm_instances:
            # ë¡œë“œë°¸ëŸ°ì„œì— ì¦‰ì‹œ ë“±ë¡
            self.register_to_load_balancer(instance)
            
        return len(warm_instances)

class WarmupPool:
    def __init__(self):
        self.warm_instances = []
        self.target_warm_count = 3
        
    def prepare_instances(self, count):
        """ì¸ìŠ¤í„´ìŠ¤ ë¯¸ë¦¬ ì¤€ë¹„"""
        for _ in range(count):
            instance = self.launch_instance()
            
            # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ì „ ë¡œë”©
            self.preload_application(instance)
            
            # ìºì‹œ ì‚¬ì „ ì›Œë°
            self.warm_cache(instance)
            
            self.warm_instances.append(instance)
    
    def preload_application(self, instance):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ì „ ë¡œë”©"""
        # JVM ì›Œë°ì—…, ì˜ì¡´ì„± ë¡œë”© ë“±
        requests.get(f"http://{instance.private_ip}/warmup")
    
    def warm_cache(self, instance):
        """ìºì‹œ ì‚¬ì „ ì›Œë°"""
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ìºì‹œ ë¡œë”©
        requests.post(f"http://{instance.private_ip}/cache/warm")
```

#### ğŸ’¸ ë¹„ìš© ê´€ë¦¬

> [!danger] ë¹„ìš© í­ì¦ ìœ„í—˜
> ì˜ëª»ëœ ì •ì±…ìœ¼ë¡œ ê³¼ë„í•˜ê²Œ í™•ì¥ë˜ë©´ ë¹„ìš©ì´ í­ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ğŸ›¡ï¸ ë¹„ìš© ë³´í˜¸ ë©”ì»¤ë‹ˆì¦˜

```python
class CostProtectionManager:
    def __init__(self):
        self.budget_limits = {
            'hourly': 100,    # ì‹œê°„ë‹¹ $100
            'daily': 1000,    # ì¼ì¼ $1000  
            'monthly': 25000  # ì›”ê°„ $25000
        }
        
        self.emergency_contacts = [
            'ops-team@company.com',
            'cto@company.com'
        ]
        
    def check_cost_limits(self, proposed_scaling):
        """ë¹„ìš© í•œë„ í™•ì¸"""
        projected_cost = self.calculate_scaling_cost(proposed_scaling)
        current_costs = self.get_current_costs()
        
        # ì‹œê°„ë‹¹ ë¹„ìš© ì²´í¬
        if current_costs['hourly'] + projected_cost > self.budget_limits['hourly']:
            return self.handle_cost_limit_exceeded('hourly', projected_cost)
        
        # ì¼ì¼ ë¹„ìš© ì²´í¬
        if current_costs['daily'] + projected_cost > self.budget_limits['daily']:
            return self.handle_cost_limit_exceeded('daily', projected_cost)
        
        return {'approved': True}
    
    def handle_cost_limit_exceeded(self, period, projected_cost):
        """ë¹„ìš© í•œë„ ì´ˆê³¼ ì²˜ë¦¬"""
        # ì•Œë¦¼ ë°œì†¡
        self.send_cost_alert(period, projected_cost)
        
        # í™•ì¥ ê±°ë¶€ ë˜ëŠ” ì œí•œ
        if period == 'hourly':
            return {'approved': False, 'reason': 'Hourly budget exceeded'}
        elif period == 'daily':
            # ì œí•œì  í™•ì¥ í—ˆìš©
            return {
                'approved': True, 
                'max_instances': self.calculate_safe_instance_count(),
                'reason': 'Limited scaling due to daily budget'
            }
    
    def emergency_cost_control(self):
        """ê¸´ê¸‰ ë¹„ìš© ì œì–´"""
        current_cost = self.get_current_hourly_cost()
        
        if current_cost > self.budget_limits['hourly'] * 1.5:
            # ë¹„ì¤‘ìš” ì„œë¹„ìŠ¤ ì¦‰ì‹œ ìŠ¤ì¼€ì¼ ë‹¤ìš´
            self.emergency_scale_down([
                'dev-environment',
                'staging-environment', 
                'analytics-service'
            ])
            
            # ìš´ì˜íŒ€ ì¦‰ì‹œ ì•Œë¦¼
            self.send_emergency_alert(current_cost)
            
    def implement_budget_alerts(self):
        """ì˜ˆì‚° ì•Œë¦¼ ì„¤ì •"""
        alert_thresholds = [0.5, 0.8, 0.9, 1.0]  # 50%, 80%, 90%, 100%
        
        for threshold in alert_thresholds:
            self.set_cloudwatch_alarm(
                metric_name='EstimatedCharges',
                threshold=self.budget_limits['daily'] * threshold,
                alarm_actions=[
                    self.get_sns_topic_arn('cost-alerts')
                ]
            )
```

#### ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë³‘ëª©

> [!info] DB ë³‘ëª© í•´ê²°
> ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë²„ë§Œ í™•ì¥í•˜ë©´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë³‘ëª©ì´ ë˜ë¯€ë¡œ, ì¢…í•©ì ì¸ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥ ì „ëµ

```python
class DatabaseBottleneckManager:
    def __init__(self):
        self.connection_pools = {}
        self.read_replicas = []
        self.cache_layers = ['redis', 'memcached']
        
    def handle_database_scaling(self, app_scale_event):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ í™•ì¥ì— ë”°ë¥¸ DB ì²˜ë¦¬"""
        
        # 1. ì—°ê²° í’€ í™•ì¥
        self.scale_connection_pools(app_scale_event.new_instance_count)
        
        # 2. ì½ê¸° ë³µì œë³¸ í™•ì¸/ì¶”ê°€
        if app_scale_event.new_instance_count > 10:
            self.ensure_read_replicas()
        
        # 3. ìºì‹œ ë ˆì´ì–´ ê°•í™”
        self.optimize_cache_strategy()
        
        # 4. ì¿¼ë¦¬ ìµœì í™” ì ìš©
        self.apply_query_optimizations()
    
    def scale_connection_pools(self, instance_count):
        """ì—°ê²° í’€ ë™ì  ì¡°ì •"""
        # ì¸ìŠ¤í„´ìŠ¤ë‹¹ 20ê°œ ì—°ê²° í• ë‹¹
        target_connections_per_instance = 20
        max_db_connections = 1000  # DB ìµœëŒ€ ì—°ê²° ìˆ˜
        
        total_needed = instance_count * target_connections_per_instance
        
        if total_needed > max_db_connections:
            # ì—°ê²° ìˆ˜ ì œí•œ ë° ì—°ê²° í’€ ìµœì í™”
            connections_per_instance = max_db_connections // instance_count
            self.update_connection_pool_config({
                'max_connections': connections_per_instance,
                'connection_timeout': 5,
                'idle_timeout': 60
            })
        
    def ensure_read_replicas(self):
        """ì½ê¸° ë³µì œë³¸ í™•ë³´"""
        current_read_load = self.get_read_load_metrics()
        
        if current_read_load > 70:  # ì½ê¸° ë¶€í•˜ 70% ì´ˆê³¼
            if len(self.read_replicas) < 3:
                self.create_read_replica()
                
        # ì½ê¸° íŠ¸ë˜í”½ ë¶„ì‚°
        self.update_read_routing_weights()
    
    def optimize_cache_strategy(self):
        """ìºì‹œ ì „ëµ ìµœì í™”"""
        cache_strategies = {
            'user_sessions': {'ttl': 3600, 'pattern': 'write_through'},
            'product_catalog': {'ttl': 7200, 'pattern': 'write_behind'},
            'user_preferences': {'ttl': 1800, 'pattern': 'cache_aside'},
            'search_results': {'ttl': 900, 'pattern': 'cache_aside'}
        }
        
        for data_type, strategy in cache_strategies.items():
            self.configure_cache_layer(data_type, strategy)
```

---

## ğŸ¯ ì‹¤ì „ ì˜ˆì‹œ

### ğŸ“ ì¢…í•© Auto Scaling ì‹œìŠ¤í…œ êµ¬ì¶•

> [!example] ì‹¤ì œ êµ¬í˜„ ì‹œë‚˜ë¦¬ì˜¤
> **ìƒí™©**: ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ì—ì„œ ë¸”ë™í”„ë¼ì´ë°ì´ì™€ ê°™ì€ ëŒ€ê·œëª¨ ì´ë²¤íŠ¸ë¥¼ ëŒ€ë¹„í•œ ë™ì  í™•ì¥ ì‹œìŠ¤í…œ êµ¬ì¶•

#### ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```yaml
# ì¢…í•© Auto Scaling ì•„í‚¤í…ì²˜
comprehensive_autoscaling_architecture:
  
  # 1. ì›¹ í‹°ì–´
  web_tier:
    load_balancer:
      type: "Application Load Balancer"
      multi_az: true
      ssl_termination: true
      
    auto_scaling_group:
      min_size: 5
      max_size: 200
      desired_capacity: 15
      instance_types: ["t3.medium", "t3.large", "m5.large"]
      spot_percentage: 60
      
    scaling_policies:
      - metric: "CPUUtilization"
        threshold: 60
        action: "scale_up"
        adjustment: "+20%"
      - metric: "RequestCountPerTarget"
        threshold: 1000
        action: "scale_up"
        adjustment: "+10 instances"
      - metric: "ResponseTime"
        threshold: 2.0
        action: "scale_up"
        urgency: "high"
  
  # 2. API í‹°ì–´  
  api_tier:
    container_orchestration: "EKS"
    
    horizontal_pod_autoscaler:
      min_replicas: 10
      max_replicas: 500
      target_cpu: 50
      target_memory: 70
      
    custom_metrics:
      - name: "orders_per_second"
        target: 100
      - name: "inventory_check_rate" 
        target: 500
      - name: "payment_processing_rate"
        target: 200
    
    cluster_autoscaler:
      min_nodes: 3
      max_nodes: 100
      scale_down_utilization_threshold: 0.5
      
  # 3. ë°ì´í„°ë² ì´ìŠ¤ í‹°ì–´
  database_tier:
    primary:
      instance_class: "db.r5.2xlarge"
      auto_scaling: false
      
    read_replicas:
      min_count: 2
      max_count: 10
      auto_scaling: true
      scaling_metric: "ReadLatency"
      
    cache_layer:
      redis_cluster:
        node_count: 6
        auto_scaling: true
        memory_threshold: 80
  
  # 4. ë°°ì¹˜ ì²˜ë¦¬ í‹°ì–´
  batch_tier:
    queue_based_scaling:
      sqs_queue: "processing-queue"
      target_queue_length: 50
      
    worker_instances:
      min_size: 0
      max_size: 100
      instance_types: ["c5.large", "c5.xlarge"]
      spot_percentage: 90
```

#### ğŸ’» í†µí•© Auto Scaling ì»¨íŠ¸ë¡¤ëŸ¬

```python
import boto3
import time
import threading
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class ScalingMetrics:
    timestamp: datetime
    cpu_utilization: float
    memory_utilization: float
    request_count: int
    response_time: float
    error_rate: float
    queue_length: int
    active_users: int

@dataclass
class ScalingDecision:
    component: str
    action: str  # scale_up, scale_down, maintain
    target_capacity: int
    current_capacity: int
    confidence: float
    reason: str

class ComprehensiveAutoScalingController:
    def __init__(self):
        # AWS í´ë¼ì´ì–¸íŠ¸
        self.ec2 = boto3.client('ec2')
        self.autoscaling = boto3.client('autoscaling')
        self.elbv2 = boto3.client('elbv2')
        self.cloudwatch = boto3.client('cloudwatch')
        self.sqs = boto3.client('sqs')
        
        # êµ¬ì„± ìš”ì†Œ
        self.components = {
            'web_tier': WebTierScaler(self.autoscaling, self.cloudwatch),
            'api_tier': ApiTierScaler(),
            'database_tier': DatabaseTierScaler(boto3.client('rds')),
            'batch_tier': BatchTierScaler(self.sqs, self.autoscaling)
        }
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
        self.metrics_collector = MetricsCollector()
        
        # ê²°ì • ì—”ì§„
        self.decision_engine = ScalingDecisionEngine()
        
        # ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        self.is_running = False
        
    def start_auto_scaling_loop(self):
        """Auto Scaling ë©”ì¸ ë£¨í”„ ì‹œì‘"""
        self.is_running = True
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        threading.Thread(target=self._scaling_loop, daemon=True).start()
        
        print("Auto Scaling Controller started")
    
    def _scaling_loop(self):
        """ë©”ì¸ ìŠ¤ì¼€ì¼ë§ ë£¨í”„"""
        while self.is_running:
            try:
                # 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                current_metrics = self.metrics_collector.collect_all_metrics()
                
                # 2. ê° ì»´í¬ë„ŒíŠ¸ë³„ ìŠ¤ì¼€ì¼ë§ ê²°ì •
                scaling_decisions = []
                
                for component_name, scaler in self.components.items():
                    decision = scaler.evaluate_scaling(current_metrics)
                    if decision:
                        scaling_decisions.append(decision)
                
                # 3. ì¢…í•© ê²°ì • ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
                final_decisions = self.decision_engine.prioritize_decisions(
                    scaling_decisions, current_metrics
                )
                
                # 4. ê²°ì • ì‹¤í–‰
                self.execute_scaling_decisions(final_decisions)
                
                # 5. ê²°ê³¼ ë¡œê¹…
                self.log_scaling_activity(current_metrics, final_decisions)
                
                # 6. ë‹¤ìŒ ì‚¬ì´í´ê¹Œì§€ ëŒ€ê¸°
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ í‰ê°€
                
            except Exception as e:
                print(f"Auto Scaling error: {e}")
                time.sleep(30)  # ì˜¤ë¥˜ ì‹œ 30ì´ˆ í›„ ì¬ì‹œë„
    
    def execute_scaling_decisions(self, decisions: List[ScalingDecision]):
        """ìŠ¤ì¼€ì¼ë§ ê²°ì • ì‹¤í–‰"""
        for decision in decisions:
            try:
                component = self.components[decision.component]
                result = component.execute_scaling(decision)
                
                if result.get('success'):
                    print(f"âœ… {decision.component}: {decision.action} "
                          f"from {decision.current_capacity} to {decision.target_capacity}")
                else:
                    print(f"âŒ {decision.component}: Scaling failed - {result.get('error')}")
                    
            except Exception as e:
                print(f"âŒ {decision.component}: Execution error - {e}")

class WebTierScaler:
    def __init__(self, autoscaling_client, cloudwatch_client):
        self.autoscaling = autoscaling_client
        self.cloudwatch = cloudwatch_client
        self.asg_name = 'web-servers-asg'
        
        # ìŠ¤ì¼€ì¼ë§ ì •ì±…
        self.scaling_config = {
            'min_instances': 5,
            'max_instances': 200,
            'target_cpu': 60,
            'target_response_time': 2.0,
            'target_rps_per_instance': 100
        }
        
    def evaluate_scaling(self, metrics: ScalingMetrics) -> Optional[ScalingDecision]:
        """ì›¹ í‹°ì–´ ìŠ¤ì¼€ì¼ë§ í‰ê°€"""
        current_capacity = self.get_current_capacity()
        
        # ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê¸°ë°˜ í‰ê°€
        cpu_pressure = metrics.cpu_utilization / self.scaling_config['target_cpu']
        response_pressure = metrics.response_time / self.scaling_config['target_response_time']
        rps_pressure = metrics.request_count / (current_capacity * self.scaling_config['target_rps_per_instance'])
        
        # ìµœê³  ì••ë ¥ ì§€í‘œ ì‚¬ìš©
        max_pressure = max(cpu_pressure, response_pressure, rps_pressure)
        
        if max_pressure > 1.2:  # 20% ì´ˆê³¼
            # í™•ì¥ í•„ìš”
            target_capacity = min(
                int(current_capacity * max_pressure * 1.1),  # 10% ë²„í¼
                self.scaling_config['max_instances']
            )
            
            return ScalingDecision(
                component='web_tier',
                action='scale_up',
                target_capacity=target_capacity,
                current_capacity=current_capacity,
                confidence=min(max_pressure - 1.0, 1.0),
                reason=f"High pressure: {max_pressure:.2f} (CPU:{cpu_pressure:.2f}, "
                      f"RT:{response_pressure:.2f}, RPS:{rps_pressure:.2f})"
            )
            
        elif max_pressure < 0.6:  # 60% ë¯¸ë§Œ
            # ì¶•ì†Œ ê°€ëŠ¥
            target_capacity = max(
                int(current_capacity * 0.9),  # 10% ì¶•ì†Œ
                self.scaling_config['min_instances']
            )
            
            if target_capacity < current_capacity:
                return ScalingDecision(
                    component='web_tier',
                    action='scale_down',
                    target_capacity=target_capacity,
                    current_capacity=current_capacity,
                    confidence=0.8,
                    reason=f"Low pressure: {max_pressure:.2f}"
                )
        
        return None  # í˜„ìƒ ìœ ì§€
    
    def execute_scaling(self, decision: ScalingDecision) -> Dict:
        """ì›¹ í‹°ì–´ ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰"""
        try:
            response = self.autoscaling.set_desired_capacity(
                AutoScalingGroupName=self.asg_name,
                DesiredCapacity=decision.target_capacity,
                HonorCooldown=False
            )
            
            return {'success': True, 'response': response}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_current_capacity(self) -> int:
        """í˜„ì¬ ìš©ëŸ‰ ì¡°íšŒ"""
        response = self.autoscaling.describe_auto_scaling_groups(
            AutoScalingGroupNames=[self.asg_name]
        )
        
        asg = response['AutoScalingGroups'][0]
        return asg['DesiredCapacity']

class ApiTierScaler:
    def __init__(self):
        # Kubernetes API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        from kubernetes import client, config
        config.load_incluster_config()  # í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì‹¤í–‰
        self.k8s_apps = client.AppsV1Api()
        self.k8s_autoscaling = client.AutoscalingV2Api()
        
    def evaluate_scaling(self, metrics: ScalingMetrics) -> Optional[ScalingDecision]:
        """API í‹°ì–´ ìŠ¤ì¼€ì¼ë§ í‰ê°€"""
        # Kubernetes HPA ìƒíƒœ í™•ì¸
        hpa_status = self.get_hpa_status()
        
        if not hpa_status:
            return None
        
        current_replicas = hpa_status['current_replicas']
        target_replicas = hpa_status['desired_replicas']
        
        # HPAê°€ ì´ë¯¸ ìŠ¤ì¼€ì¼ë§ ì¤‘ì´ë©´ ê°œì…í•˜ì§€ ì•ŠìŒ
        if abs(current_replicas - target_replicas) > 0:
            return None
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê¸°ë°˜ ì¶”ê°€ ìŠ¤ì¼€ì¼ë§ í‰ê°€
        orders_per_second = metrics.request_count / 60  # ê°„ë‹¨íˆ ê³„ì‚°
        
        if orders_per_second > 50:  # ì´ˆë‹¹ 50ê°œ ì£¼ë¬¸ ì´ˆê³¼
            # ì¶”ê°€ í™•ì¥ ê¶Œì¥
            recommended_replicas = min(
                int(current_replicas * 1.5),
                100  # ìµœëŒ€ 100ê°œ
            )
            
            return ScalingDecision(
                component='api_tier',
                action='scale_up',
                target_capacity=recommended_replicas,
                current_capacity=current_replicas,
                confidence=0.9,
                reason=f"High order rate: {orders_per_second:.1f}/sec"
            )
        
        return None
    
    def get_hpa_status(self) -> Optional[Dict]:
        """HPA ìƒíƒœ ì¡°íšŒ"""
        try:
            hpa = self.k8s_autoscaling.read_namespaced_horizontal_pod_autoscaler(
                name='api-service-hpa',
                namespace='default'
            )
            
            return {
                'current_replicas': hpa.status.current_replicas or 0,
                'desired_replicas': hpa.status.desired_replicas or 0,
                'max_replicas': hpa.spec.max_replicas,
                'min_replicas': hpa.spec.min_replicas
            }
        except Exception as e:
            print(f"Error getting HPA status: {e}")
            return None

class MetricsCollector:
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        
    def collect_all_metrics(self) -> ScalingMetrics:
        """ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=5)
        
        # CloudWatchì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics_data = {}
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_data = self.get_cloudwatch_metric(
            'AWS/EC2', 'CPUUtilization', 
            start_time, end_time, 300
        )
        metrics_data['cpu_utilization'] = cpu_data[-1]['Average'] if cpu_data else 0
        
        # ìš”ì²­ ìˆ˜
        request_data = self.get_cloudwatch_metric(
            'AWS/ApplicationELB', 'RequestCount',
            start_time, end_time, 300
        )
        metrics_data['request_count'] = request_data[-1]['Sum'] if request_data else 0
        
        # ì‘ë‹µ ì‹œê°„
        response_time_data = self.get_cloudwatch_metric(
            'AWS/ApplicationELB', 'TargetResponseTime',
            start_time, end_time, 300
        )
        metrics_data['response_time'] = response_time_data[-1]['Average'] if response_time_data else 0
        
        return ScalingMetrics(
            timestamp=datetime.utcnow(),
            cpu_utilization=metrics_data.get('cpu_utilization', 0),
            memory_utilization=75.0,  # ìƒ˜í”Œ ê°’
            request_count=int(metrics_data.get('request_count', 0)),
            response_time=metrics_data.get('response_time', 0),
            error_rate=2.0,  # ìƒ˜í”Œ ê°’
            queue_length=10,  # ìƒ˜í”Œ ê°’
            active_users=500  # ìƒ˜í”Œ ê°’
        )
    
    def get_cloudwatch_metric(self, namespace, metric_name, start_time, end_time, period):
        """CloudWatch ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            response = self.cloudwatch.get_metric_statistics(
                Namespace=namespace,
                MetricName=metric_name,
                StartTime=start_time,
                EndTime=end_time,
                Period=period,
                Statistics=['Average', 'Sum']
            )
            return sorted(response['Datapoints'], key=lambda x: x['Timestamp'])
        except Exception as e:
            print(f"Error getting CloudWatch metric {metric_name}: {e}")
            return []

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # Auto Scaling ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™” ë° ì‹œì‘
    controller = ComprehensiveAutoScalingController()
    controller.start_auto_scaling_loop()
    
    # ì„œë²„ ì‹¤í–‰ (ì›¹ ëŒ€ì‹œë³´ë“œ, API ë“±)
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        controller.is_running = False
        print("Auto Scaling Controller stopped")
```

#### ğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
# Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì • (JSON)
grafana_dashboard = {
    "dashboard": {
        "title": "Auto Scaling Monitoring",
        "panels": [
            {
                "title": "Instance Count by Tier",
                "type": "graph",
                "targets": [
                    {
                        "expr": "aws_autoscaling_group_desired_capacity{name='web-servers-asg'}",
                        "legend": "Web Tier"
                    },
                    {
                        "expr": "kube_deployment_status_replicas{deployment='api-service'}",
                        "legend": "API Tier"
                    }
                ]
            },
            {
                "title": "Scaling Events Timeline",
                "type": "logs",
                "targets": [
                    {
                        "expr": "scaling_events{component=~'.*'}"
                    }
                ]
            },
            {
                "title": "Cost Impact",
                "type": "stat",
                "targets": [
                    {
                        "expr": "sum(instance_cost_per_hour * instance_count)",
                        "legend": "Hourly Cost"
                    }
                ]
            }
        ]
    }
}
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ë³¸ ê°œë…
- [ ] ë™ì  í™•ì¥ê³¼ ì •ì  í™•ì¥ì˜ ì°¨ì´ ì´í•´
- [ ] ìˆ˜í‰ í™•ì¥ vs ìˆ˜ì§ í™•ì¥ íŠ¹ì„± íŒŒì•…
- [ ] ë°˜ì‘í˜•/ì˜ˆì¸¡í˜•/ìŠ¤ì¼€ì¤„í˜• í™•ì¥ ë°©ì‹ ì´í•´
- [ ] íƒ„ë ¥ì  ìì› ìš©ëŸ‰ê³¼ì˜ ì—°ê´€ì„± íŒŒì•…

### ì„¤ê³„ ë° êµ¬í˜„
- [ ] ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì—”ì§„ ì„¤ì •
- [ ] ë¡œë“œ ë°¸ëŸ°ì„œ êµ¬ì„±
- [ ] í™•ì¥ ì •ì±… ì„¤ê³„

### ìš´ì˜ ë° ìµœì í™”
- [ ] ë©”íŠ¸ë¦­ ì„ íƒ ë° ì„ê³„ê°’ ì„¤ì •
- [ ] ì¿¨ë‹¤ìš´ ê¸°ê°„ ìµœì í™”
- [ ] í™•ì¥ ì†ë„ ì œì–´
- [ ] ë¹„ìš© ê´€ë¦¬ ì²´ê³„ êµ¬ì¶•

### ê³ ê¸‰ ì£¼ì œ
- [ ] Stateless ì•„í‚¤í…ì²˜ ì„¤ê³„
- [ ] í™•ì¥ ì§€ì—° ìµœì†Œí™” ì „ëµ
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë³‘ëª© í•´ê²°
- [ ] ì¢…í•© Auto Scaling ì‹œìŠ¤í…œ êµ¬ì¶•

---

> [!tip] í•µì‹¬ í¬ì¸íŠ¸
> ë™ì  í™•ì¥ ì•„í‚¤í…ì²˜ëŠ” í˜„ëŒ€ í´ë¼ìš°ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í•µì‹¬ ì—­ëŸ‰ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ CPU ê¸°ë°˜ í™•ì¥ì„ ë„˜ì–´ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ê³¼ ì˜ˆì¸¡ ë¶„ì„ì„ í™œìš©í•œ ì§€ëŠ¥í˜• í™•ì¥ìœ¼ë¡œ ë°œì „ì‹œí‚¤ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ìƒíƒœ ê´€ë¦¬, ë¹„ìš© ì œì–´, ì„±ëŠ¥ ìµœì í™”ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ í™•ì¥ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”.

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "GO Day1 \ubb38\uc11c\ub97c \uc8fc\uc81c\ubcc4\ub85c \ubd84\ud560 \uc815\ub9ac", "status": "completed", "priority": "high", "id": "1"}, {"content": "\ub3d9\uc801 \ud655\uc7a5 \uc544\ud0a4\ud14d\ucc98 \ubb38\uc11c \uc0dd\uc131", "status": "completed", "priority": "high", "id": "2"}]