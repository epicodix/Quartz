---
title: íƒ„ë ¥ì  ìì› ìš©ëŸ‰(Elastic Resource Capacity) ì•„í‚¤í…ì²˜
tags:
  - architecture
  - elastic-scaling
  - cloud-computing
  - resource-management
  - cost-optimization
  - devops
  - infrastructure
aliases:
  - Elastic-Resource-Capacity
  - íƒ„ë ¥ì -ìš©ëŸ‰
  - Auto-Scaling-Architecture
date: 2025-11-26
category: êµìœ¡/êµ¬ë¦„ë”¥ë‹¤ì´ë¸Œ/ì•„í‚¤í…ì²˜
status: ì™„ì„±
priority: ë†’ìŒ
---

# ğŸ¯ íƒ„ë ¥ì  ìì› ìš©ëŸ‰(Elastic Resource Capacity) ì•„í‚¤í…ì²˜

## ğŸ“‘ ëª©ì°¨
- [[#1. í•µì‹¬ ê°œë…|1. í•µì‹¬ ê°œë…]]
- [[#2. íƒ„ë ¥ì„±ì˜ ì°¨ì›|2. íƒ„ë ¥ì„±ì˜ ì°¨ì›]]
- [[#3. ì•„í‚¤í…ì²˜ íŒ¨í„´|3. ì•„í‚¤í…ì²˜ íŒ¨í„´]]
- [[#4. êµ¬í˜„ ì „ëµ|4. êµ¬í˜„ ì „ëµ]]
- [[#5. ìš©ëŸ‰ ê³„íš|5. ìš©ëŸ‰ ê³„íš]]
- [[#6. ë¹„ìš© ìµœì í™” ì „ëµ|6. ë¹„ìš© ìµœì í™” ì „ëµ]]
- [[#7. ëª¨ë‹ˆí„°ë§ê³¼ ì œì–´|7. ëª¨ë‹ˆí„°ë§ê³¼ ì œì–´]]
- [[#ğŸ¯ ì‹¤ì „ ì˜ˆì‹œ|ì‹¤ì „ ì˜ˆì‹œ]]

---

## 1. í•µì‹¬ ê°œë…

> [!note] íƒ„ë ¥ì  ìì› ìš©ëŸ‰ì´ë€?
> ì›Œí¬ë¡œë“œì˜ ì‹¤ì‹œê°„ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ìœ ì—°í•˜ê²Œ ì¦ê°ì‹œí‚¤ëŠ” ì„¤ê³„ íŒ¨í„´ì…ë‹ˆë‹¤. ë™ì  í™•ì¥ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ë” ë„“ì€ ë²”ìœ„ì—ì„œ ì¸í”„ë¼ ì „ì²´ì˜ ìš©ëŸ‰ì„ íƒ„ë ¥ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ê°œë…ì…ë‹ˆë‹¤.

### ğŸ’¡ íƒ„ë ¥ì„±(Elasticity)ì˜ ì •ì˜

**ğŸ”‘ í•µì‹¬**: ìˆ˜ìš” ë³€í™”ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤ë¥¼ ë¹ ë¥´ê²Œ í™•ì¥í•˜ê±°ë‚˜ ì¶•ì†Œí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥

íƒ„ë ¥ì„±ì€ ë‹¨ìˆœíˆ í™•ì¥ë§Œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **í•„ìš” ì—†ì„ ë•ŒëŠ” ì¦‰ì‹œ ì¶•ì†Œí•˜ì—¬ ë¹„ìš©ì„ ì ˆê°**í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„± ì¤‘ í•˜ë‚˜ë¡œ, **ì˜¨ë””ë§¨ë“œ ë°©ì‹ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©**ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

#### ğŸ“‹ íƒ„ë ¥ì„± vs í™•ì¥ì„± ë¹„êµ

| êµ¬ë¶„ | í™•ì¥ì„±(Scalability) | íƒ„ë ¥ì„±(Elasticity) |
|------|-------------------|-------------------|
| **ë°©í–¥** | ì£¼ë¡œ í™•ì¥(Scale-up/out) | í™•ì¥ + ì¶•ì†Œ |
| **ì†ë„** | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ | ë¹ ë¥¸ ë°˜ì‘ |
| **ìë™í™”** | ìˆ˜ë™ ë˜ëŠ” ì¤€ìë™ | ì™„ì „ ìë™ |
| **ë¹„ìš©** | ê³ ì •ì  ìš©ëŸ‰ ìœ ì§€ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ |

---

## 2. íƒ„ë ¥ì„±ì˜ ì°¨ì›

### ğŸ’» ì»´í“¨íŒ… íƒ„ë ¥ì„±

> [!example] ì‹¤í–‰ í™˜ê²½ì˜ íƒ„ë ¥ì  ì¡°ì •
> ì„œë²„ ì¸ìŠ¤í„´ìŠ¤, ì»¨í…Œì´ë„ˆ, ì„œë²„ë¦¬ìŠ¤ í•¨ìˆ˜ ë“±ì˜ ì‹¤í–‰ í™˜ê²½ì„ íƒ„ë ¥ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.

#### ğŸ“Š êµ¬í˜„ ì‚¬ë¡€

```yaml
# ğŸ”§ Kubernetes HPA ì„¤ì •
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2              # ìµœì†Œ íŒŒë“œ ìˆ˜
  maxReplicas: 50             # ìµœëŒ€ íŒŒë“œ ìˆ˜
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU 70% ì‹œ í™•ì¥
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # ë©”ëª¨ë¦¬ 80% ì‹œ í™•ì¥
```

#### ğŸ“Š AWS EC2 Auto Scaling ì˜ˆì‹œ

```json
{
  "AutoScalingGroupName": "web-servers-asg",
  "MinSize": 2,
  "MaxSize": 20,
  "DesiredCapacity": 4,
  "TargetGroupARNs": ["arn:aws:elasticloadbalancing:..."],
  "HealthCheckType": "ELB",
  "HealthCheckGracePeriod": 300
}
```

### ğŸ’¾ ìŠ¤í† ë¦¬ì§€ íƒ„ë ¥ì„±

> [!info] ì €ì¥ ìš©ëŸ‰ì˜ ë™ì  í™•ì¥
> ì €ì¥ ìš©ëŸ‰ì„ í•„ìš”ì— ë”°ë¼ í™•ì¥í•©ë‹ˆë‹¤.

#### ğŸ“‹ ìŠ¤í† ë¦¬ì§€ íƒ„ë ¥ì„± ìœ í˜•

- **Amazon S3**: ë¬´ì œí•œ ìš©ëŸ‰ ì œê³µ
- **EBS ë³¼ë¥¨**: ì˜¨ë¼ì¸ìœ¼ë¡œ í¬ê¸° ì¡°ì • ê°€ëŠ¥  
- **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í† ë¦¬ì§€**: ìë™ ì¦ê°€ (Aurora, RDS)

#### ğŸ“Š Aurora Serverless v2 ì„¤ì •

```yaml
# Aurora Serverless v2 ìš©ëŸ‰ ì„¤ì •
DatabaseCluster:
  Engine: aurora-mysql
  EngineMode: provisioned
  ServerlessV2ScalingConfiguration:
    MinCapacity: 0.5    # ìµœì†Œ ACU (Aurora Capacity Units)
    MaxCapacity: 16     # ìµœëŒ€ ACU
    AutoPause: true     # ë¹„í™œì„± ì‹œ ì¼ì‹œì¤‘ì§€
    SecondsUntilAutoPause: 300
```

### ğŸŒ ë„¤íŠ¸ì›Œí¬ íƒ„ë ¥ì„±

> [!tip] ëŒ€ì—­í­ê³¼ ì—°ê²°ì˜ ë™ì  ì¡°ì •
> ëŒ€ì—­í­ê³¼ ì—°ê²°ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.

#### ğŸ“Š ë„¤íŠ¸ì›Œí¬ íƒ„ë ¥ì„± êµ¬í˜„

```yaml
# Application Load Balancer ì„¤ì •
LoadBalancer:
  Type: application
  Scheme: internet-facing
  SecurityGroups: [sg-12345678]
  Subnets: [subnet-12345678, subnet-87654321]
  
# Target Group with Auto Scaling
TargetGroup:
  Protocol: HTTP
  Port: 80
  HealthCheckPath: /health
  HealthCheckIntervalSeconds: 30
  UnhealthyThresholdCount: 3
  
# CloudFront CDN (ìë™ ì—£ì§€ ë…¸ë“œ í™œì„±í™”)
CloudFrontDistribution:
  PriceClass: PriceClass_All
  CacheBehaviors:
    - PathPattern: "*.js"
      Compress: true
      CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad
```

### ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ íƒ„ë ¥ì„±

#### ğŸ“Š DynamoDB Auto Scaling

```yaml
# DynamoDB í…Œì´ë¸” Auto Scaling ì„¤ì •
TableName: user-sessions
BillingMode: ON_DEMAND    # ì™„ì „ íƒ„ë ¥ì  ìš©ëŸ‰

# ë˜ëŠ” í”„ë¡œë¹„ì €ë‹ëœ ëª¨ë“œì—ì„œ Auto Scaling
BillingMode: PROVISIONED
ProvisionedThroughput:
  ReadCapacityUnits: 5
  WriteCapacityUnits: 5
  
# Auto Scaling ì •ì±…
ReadCapacityAutoScaling:
  MinCapacity: 5
  MaxCapacity: 1000
  TargetTrackingScalingPolicyConfiguration:
    TargetValue: 70.0
    MetricType: DynamoDBReadCapacityUtilization
```

---

## 3. ì•„í‚¤í…ì²˜ íŒ¨í„´

### âš¡ ë²„ìŠ¤íŠ¸ ê°€ëŠ¥ ì›Œí¬ë¡œë“œ (Burstable Workloads)

> [!example] ìˆœê°„ì  í”¼í¬ ëŒ€ì‘
> í‰ì†Œì—ëŠ” ë‚®ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ìœ ì§€í•˜ë‹¤ê°€ ìˆœê°„ì ì¸ í”¼í¬ì— ëŒ€ì‘í•©ë‹ˆë‹¤.

#### ğŸ” ì ìš© ì‚¬ë¡€
- **ë‰´ìŠ¤ ì‚¬ì´íŠ¸**: ì†ë³´ ë°œí–‰ ì‹œ íŠ¸ë˜í”½ ê¸‰ì¦
- **ì´ì»¤ë¨¸ìŠ¤**: í”Œë˜ì‹œ ì„¸ì¼, ë¸”ë™í”„ë¼ì´ë°ì´
- **ì†Œì…œ ë¯¸ë””ì–´**: ë°”ì´ëŸ´ ì½˜í…ì¸  í™•ì‚°

#### ğŸ“Š T ì‹œë¦¬ì¦ˆ EC2 ì¸ìŠ¤í„´ìŠ¤ í™œìš©

```bash
# T3 ì¸ìŠ¤í„´ìŠ¤ CPU í¬ë ˆë”§ ëª¨ë‹ˆí„°ë§
aws cloudwatch get-metric-statistics \
  --namespace AWS/EC2 \
  --metric-name CPUCreditBalance \
  --dimensions Name=InstanceId,Value=i-1234567890abcdef0 \
  --start-time 2025-11-26T00:00:00Z \
  --end-time 2025-11-26T23:59:59Z \
  --period 3600 \
  --statistics Average
```

### ğŸ”„ ì£¼ê¸°ì  ì›Œí¬ë¡œë“œ (Periodic Workloads)

> [!info] ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´
> ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ì„ ê°€ì§‘ë‹ˆë‹¤.

#### ğŸ“‹ íŒ¨í„´ ì˜ˆì‹œ
- **ì—…ë¬´ ì‹œê°„ëŒ€ íŠ¸ë˜í”½**: ì˜¤ì „ 9ì‹œ~ì˜¤í›„ 6ì‹œ
- **ì›”ë§ ë°°ì¹˜ ì‘ì—…**: ë§¤ì›” ë§ì¼ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
- **ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±**: ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „

#### ğŸ“Š ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í™•ì¥

```yaml
# Kubernetes CronJob + HPA
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-up-workday
spec:
  schedule: "0 8 * * 1-5"  # í‰ì¼ ì˜¤ì „ 8ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: scaler
            image: kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl patch hpa my-app-hpa -p '{"spec":{"minReplicas":10}}'
```

### ğŸ“¡ ì´ë²¤íŠ¸ ê¸°ë°˜ ì›Œí¬ë¡œë“œ (Event-Driven Workloads)

> [!tip] íŠ¹ì • ì´ë²¤íŠ¸ ë°œìƒ ì‹œ ë¦¬ì†ŒìŠ¤ í• ë‹¹
> íŠ¹ì • ì´ë²¤íŠ¸ ë°œìƒ ì‹œ ë¦¬ì†ŒìŠ¤ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.

#### ğŸ“Š ì„œë²„ë¦¬ìŠ¤ ì´ë²¤íŠ¸ ì²˜ë¦¬

```yaml
# AWS Lambda + SQS ì´ë²¤íŠ¸ ê¸°ë°˜ í™•ì¥
Resources:
  ImageProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.9
      Handler: lambda_function.lambda_handler
      ReservedConcurrencyLimit: 100  # ìµœëŒ€ ë™ì‹œ ì‹¤í–‰
      
  ImageUploadQueue:
    Type: AWS::SQS::Queue
    Properties:
      VisibilityTimeoutSeconds: 60
      MessageRetentionPeriod: 1209600  # 14ì¼
      
  EventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt ImageUploadQueue.Arn
      FunctionName: !Ref ImageProcessorFunction
      BatchSize: 10  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë©”ì‹œì§€ ìˆ˜
```

#### ğŸ“Š KEDAë¥¼ í™œìš©í•œ ì´ë²¤íŠ¸ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

```yaml
# KEDA ScaledObject ì„¤ì •
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: rabbitmq-consumer-scaler
spec:
  scaleTargetRef:
    name: message-processor
  minReplicaCount: 0      # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì¶•ì†Œ
  maxReplicaCount: 30
  triggers:
  - type: rabbitmq
    metadata:
      queueName: task-queue
      queueLength: '10'     # íì— 10ê°œ ë©”ì‹œì§€ë‹¹ 1ê°œ íŒŒë“œ
      connectionString: "amqp://user:pass@rabbitmq:5672/vhost"
```

### ğŸ² ì–¸í”„ë ˆë”•í„°ë¸” ì›Œí¬ë¡œë“œ (Unpredictable Workloads)

> [!warning] íŒ¨í„´ ì˜ˆì¸¡ ì–´ë ¤ì›€
> íŒ¨í„´ì„ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ì›Œí¬ë¡œë“œì…ë‹ˆë‹¤.

#### ğŸ“‹ íŠ¹ì§• ë° ëŒ€ì‘
- **ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ**: ê°‘ì‘ìŠ¤ëŸ° ë°”ì´ëŸ´
- **ê°‘ì‘ìŠ¤ëŸ° ë‰´ìŠ¤ ì´ë²¤íŠ¸**: ì˜ˆìƒì¹˜ ëª»í•œ íŠ¸ë˜í”½
- **ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ ë¡ ì¹­**: ì´ˆê¸° ì‚¬ìš©ì ë°˜ì‘ ë¶ˆëª…í™•

#### ğŸ“Š ë¹ ë¥¸ ë°˜ì‘í˜• ìŠ¤ì¼€ì¼ë§

```yaml
# ê³µê²©ì ì¸ í™•ì¥ ì •ì±…
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: aggressive-hpa
spec:
  scaleTargetRef:
    name: unpredictable-app
  minReplicas: 5              # ë†’ì€ ê¸°ë³¸ ìš©ëŸ‰
  maxReplicas: 200            # ë§¤ìš° í° ìµœëŒ€ ìš©ëŸ‰
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30   # 30ì´ˆ ë‚´ ë¹ ë¥¸ í™•ì¥
      policies:
      - type: Percent
        value: 200            # 200% í™•ì¥ (2ë°°)
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 300  # 5ë¶„ í›„ ì¶•ì†Œ (ë³´ìˆ˜ì )
      policies:
      - type: Percent
        value: 10             # 10%ì”© ì ì§„ì  ì¶•ì†Œ
        periodSeconds: 60
```

---

## 4. êµ¬í˜„ ì „ëµ

### ğŸš€ ì„œë²„ë¦¬ìŠ¤ ìš°ì„  (Serverless-First) ì ‘ê·¼

> [!note] ì™„ì „í•œ íƒ„ë ¥ì„± ì œê³µ
> ê°€ëŠ¥í•œ í•œ ì„œë²„ë¦¬ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

#### âœ… ì¥ì 
- ì¸í”„ë¼ ê´€ë¦¬ ë¶€ë‹´ ì—†ìŒ
- ì§„ì •í•œ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ
- ìë™ìœ¼ë¡œ ì™„ì „í•œ íƒ„ë ¥ì„± ì œê³µ

#### âš ï¸ ì œí•œì‚¬í•­
- **ì½œë“œ ìŠ¤íƒ€íŠ¸**: ì²« ì‹¤í–‰ ì‹œ ì§€ì—°
- **ì‹¤í–‰ ì‹œê°„ ì œí•œ**: LambdaëŠ” 15ë¶„ ì œí•œ
- **ìƒíƒœ ìœ ì§€ ë¶ˆê°€**: Stateless ì•„í‚¤í…ì²˜ í•„ìš”

#### ğŸ“Š ì„œë²„ë¦¬ìŠ¤ ìŠ¤íƒ ì˜ˆì‹œ

```yaml
# Serverless Framework ì„¤ì •
service: elastic-api

provider:
  name: aws
  runtime: python3.9
  stage: ${opt:stage, 'dev'}
  
functions:
  api:
    handler: app.lambda_handler
    events:
      - http:
          path: /{proxy+}
          method: ANY
    reservedConcurrency: 100    # ë™ì‹œ ì‹¤í–‰ ì œí•œ
    provisionedConcurrency: 5   # ì›œ ì¸ìŠ¤í„´ìŠ¤ ìœ ì§€
    
  processor:
    handler: processor.lambda_handler
    events:
      - sqs:
          arn: arn:aws:sqs:region:account:queue-name
          batchSize: 10
    timeout: 300               # 5ë¶„ íƒ€ì„ì•„ì›ƒ

resources:
  Resources:
    ApiGateway:
      Type: AWS::ApiGateway::RestApi
      Properties:
        EndpointConfiguration:
          Types: [EDGE]         # ê¸€ë¡œë²Œ ì—£ì§€ ìµœì í™”
```

### ğŸ³ ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

> [!info] Kubernetes/ECS í™œìš©
> Kubernetesë‚˜ ECSë¥¼ í™œìš©í•œ ì»¨í…Œì´ë„ˆ ê¸°ë°˜ íƒ„ë ¥ì„±ì…ë‹ˆë‹¤.

#### ğŸ“Š Kubernetes ì¢…í•© ìŠ¤ì¼€ì¼ë§

```yaml
# 1. Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    name: web-app
  minReplicas: 3
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

---
# 2. Vertical Pod Autoscaler  
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: web-container
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi

---
# 3. Cluster Autoscaler ì„¤ì •
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/kubernetes-cluster-name
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
```

#### ğŸ“Š ECS Service Auto Scaling

```yaml
# ECS Service with Auto Scaling
Resources:
  ECSService:
    Type: AWS::ECS::Service
    Properties:
      ServiceName: web-service
      Cluster: !Ref ECSCluster
      TaskDefinition: !Ref TaskDefinition
      DesiredCount: 3
      
  ServiceScalingTarget:
    Type: AWS::ApplicationAutoScaling::ScalableTarget
    Properties:
      MaxCapacity: 50
      MinCapacity: 2
      ResourceId: !Sub service/${ECSCluster}/${ECSService.Name}
      RoleARN: !GetAtt ApplicationAutoScalingRole.Arn
      ServiceNamespace: ecs
      ScalableDimension: ecs:service:DesiredCount
      
  ServiceScalingPolicy:
    Type: AWS::ApplicationAutoScaling::ScalingPolicy
    Properties:
      PolicyName: cpu-scaling-policy
      PolicyType: TargetTrackingScaling
      ScalingTargetId: !Ref ServiceScalingTarget
      TargetTrackingScalingPolicyConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: ECSServiceAverageCPUUtilization
        TargetValue: 70.0
        ScaleOutCooldown: 300
        ScaleInCooldown: 300
```

### ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

> [!tip] ì—¬ëŸ¬ ë°©ì‹ í˜¼í•©
> ì—¬ëŸ¬ ë°©ì‹ì„ í˜¼í•©í•˜ì—¬ ìµœì ì˜ íƒ„ë ¥ì„±ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

#### ğŸ“‹ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ

| ìš©ëŸ‰ ìœ í˜• | êµ¬í˜„ ë°©ì‹ | ë¹„ìš© íŠ¹ì„± | ì‚¬ìš© ëª©ì  |
|-----------|----------|-----------|----------|
| **ê¸°ë³¸ ìš©ëŸ‰** | ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ | ê³ ì • ë¹„ìš©, í• ì¸ | ìµœì†Œ ë³´ì¥ ìš©ëŸ‰ |
| **ë³€ë™ ìš©ëŸ‰** | ì˜¨ë””ë§¨ë“œ ì¸ìŠ¤í„´ìŠ¤ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ | ì¼ë°˜ì  í™•ì¥ |
| **í”¼í¬ ìš©ëŸ‰** | ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ | ìµœëŒ€ 90% í• ì¸ | ì¤‘ë‹¨ ê°€ëŠ¥ ì›Œí¬ë¡œë“œ |
| **ê·¹í•œ í”¼í¬** | ì„œë²„ë¦¬ìŠ¤ | ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ | ë‹¨ë°œì„± ë¶€í•˜ |

#### ğŸ“Š Mixed Instance Type Auto Scaling Group

```yaml
# AWS ASG with Mixed Instance Types
LaunchTemplate:
  ImageId: ami-12345678
  InstanceType: t3.medium
  SecurityGroupIds: [sg-12345678]
  IamInstanceProfile: 
    Name: !Ref EC2InstanceProfile

AutoScalingGroup:
  VPCZoneIdentifier: [subnet-12345, subnet-67890]
  MinSize: 2
  MaxSize: 100
  DesiredCapacity: 5
  
  # Mixed Instance Policy
  MixedInstancesPolicy:
    InstancesDistribution:
      OnDemandAllocationStrategy: prioritized
      OnDemandBaseCapacity: 2              # ìµœì†Œ ì˜¨ë””ë§¨ë“œ ì¸ìŠ¤í„´ìŠ¤
      OnDemandPercentageAboveBaseCapacity: 20  # 20%ëŠ” ì˜¨ë””ë§¨ë“œ
      SpotAllocationStrategy: diversified   # ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ë‹¤ì–‘í™”
      SpotInstancePools: 4
      SpotMaxPrice: "0.10"                 # ìµœëŒ€ ìŠ¤íŒŸ ê°€ê²©
      
    LaunchTemplate:
      LaunchTemplateSpecification:
        LaunchTemplateId: !Ref LaunchTemplate
        Version: $Latest
      Overrides:
      - InstanceType: t3.medium
        WeightedCapacity: 1
      - InstanceType: t3.large  
        WeightedCapacity: 2
      - InstanceType: m5.large
        WeightedCapacity: 2
      - InstanceType: c5.large
        WeightedCapacity: 2
```

---

## 5. ìš©ëŸ‰ ê³„íš

### ğŸ“Š ê¸°ì¤€ì„  ìš©ëŸ‰ (Baseline Capacity)

> [!note] í•­ìƒ í•„ìš”í•œ ìµœì†Œ ë¦¬ì†ŒìŠ¤
> í•­ìƒ í•„ìš”í•œ ìµœì†Œ ë¦¬ì†ŒìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

#### ğŸ“‹ ê²°ì • ìš”ì†Œ

- **ìµœì € íŠ¸ë˜í”½ ì‹œê°„ëŒ€** ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
- **í•µì‹¬ ì„œë¹„ìŠ¤** ìµœì†Œ ìš”êµ¬ì‚¬í•­ ê³ ë ¤  
- **ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤**ë¡œ ë¹„ìš© íš¨ìœ¨ì  í™•ë³´

#### ğŸ“Š ê¸°ì¤€ì„  ë¶„ì„ ì˜ˆì‹œ

```python
# ê¸°ì¤€ì„  ìš©ëŸ‰ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
import boto3
import pandas as pd
from datetime import datetime, timedelta

def analyze_baseline_capacity():
    cloudwatch = boto3.client('cloudwatch')
    
    # ì§€ë‚œ 30ì¼ CPU ì‚¬ìš©ë¥  ë°ì´í„°
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=30)
    
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/EC2',
        MetricName='CPUUtilization',
        Dimensions=[
            {'Name': 'AutoScalingGroupName', 'Value': 'web-servers-asg'}
        ],
        StartTime=start_time,
        EndTime=end_time,
        Period=3600,  # 1ì‹œê°„ ë‹¨ìœ„
        Statistics=['Average', 'Minimum']
    )
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(response['Datapoints'])
    
    # ìµœì € ì‚¬ìš©ë¥  ë¶„ì„ (ìƒˆë²½ 2-6ì‹œ)
    df['hour'] = pd.to_datetime(df['Timestamp']).dt.hour
    night_hours = df[df['hour'].between(2, 6)]
    
    baseline_cpu = night_hours['Average'].quantile(0.95)  # 95 í¼ì„¼íƒ€ì¼
    
    print(f"ê¶Œì¥ ê¸°ì¤€ì„  ìš©ëŸ‰ (CPU ê¸°ì¤€): {baseline_cpu:.1f}%")
    return baseline_cpu

# ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ê³„ì‚°
def calculate_baseline_instances(baseline_cpu, target_cpu=30):
    """ê¸°ì¤€ì„  CPUë¥¼ ëª©í‘œ CPUë¡œ ë¶„ì‚°"""
    baseline_instances = max(2, int(baseline_cpu / target_cpu))
    return baseline_instances
```

### ğŸ”„ ë²„í¼ ìš©ëŸ‰ (Buffer Capacity)

> [!warning] ì˜ˆìƒì¹˜ ëª»í•œ ì¦ê°€ ëŒ€ë¹„
> ì˜ˆìƒì¹˜ ëª»í•œ ì¦ê°€ë¥¼ ìœ„í•œ ì—¬ìœ ë¶„ì…ë‹ˆë‹¤.

#### ğŸ“‹ ë²„í¼ ìš©ëŸ‰ ì „ëµ

- ê¸°ì¤€ì„ ì˜ **20-30%** ì¶”ê°€ ìœ ì§€
- **ë¹ ë¥¸ í™•ì¥ì´ ë¶ˆê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸**ì— íŠ¹íˆ ì¤‘ìš”
- ë°ì´í„°ë² ì´ìŠ¤, ë¡œë“œë°¸ëŸ°ì„œ ë“± ì¸í”„ë¼ ë ˆë²¨

#### ğŸ“Š ë²„í¼ ìš©ëŸ‰ ì„¤ì •

```yaml
# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ë²„í¼ ì„¤ì •
spring:
  datasource:
    hikari:
      minimum-idle: 10          # ê¸°ì¤€ì„ 
      maximum-pool-size: 50     # ê¸°ì¤€ì„  + ë²„í¼ + í”¼í¬
      
      # ë²„í¼ ê´€ë ¨ ì„¤ì •
      idle-timeout: 300000      # 5ë¶„ ìœ íœ´ íƒ€ì„ì•„ì›ƒ
      max-lifetime: 1800000     # 30ë¶„ ìµœëŒ€ ìƒì¡´
      connection-timeout: 30000 # 30ì´ˆ ì—°ê²° íƒ€ì„ì•„ì›ƒ
      leak-detection-threshold: 60000  # 1ë¶„ ëˆ„ìˆ˜ ê°ì§€

# ë¡œë“œë°¸ëŸ°ì„œ ë²„í¼ ì„¤ì •  
load_balancer:
  target_groups:
    web_servers:
      healthy_threshold: 2
      unhealthy_threshold: 3
      interval: 30
      timeout: 5
      deregistration_delay: 30  # ë¹ ë¥¸ ë“œë ˆì´ë‹
```

### âš¡ í”¼í¬ ìš©ëŸ‰ (Peak Capacity)

> [!info] ìµœëŒ€ ë¶€í•˜ ì‹œ í•„ìš” ë¦¬ì†ŒìŠ¤
> ìµœëŒ€ ë¶€í•˜ ì‹œ í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ì…ë‹ˆë‹¤.

#### ğŸ“‹ í”¼í¬ ìš©ëŸ‰ ê³„íš

- **ê³¼ê±° ë°ì´í„°** ê¸°ë°˜ ì˜ˆì¸¡
- **ì•ˆì „ ë§ˆì§„** í¬í•¨ (ë³´í†µ 20% ì¶”ê°€)
- **ì˜¨ë””ë§¨ë“œ/ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤**ë¡œ ëŒ€ì‘

#### ğŸ“Š í”¼í¬ íŠ¸ë˜í”½ ë¶„ì„

```python
# í”¼í¬ ìš©ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

def predict_peak_capacity():
    # ê³¼ê±° íŠ¸ë˜í”½ íŒ¨í„´ (ì‹œê°„ë‹¹ ìš”ì²­ ìˆ˜)
    historical_data = {
        'black_friday': 50000,
        'christmas': 45000, 
        'new_year': 40000,
        'regular_peak': 25000,
        'weekend_peak': 15000
    }
    
    # ì¦ê°€ íŠ¸ë Œë“œ ê³ ë ¤ (ë…„ 10% ì„±ì¥)
    growth_rate = 1.1
    
    # ì˜ˆìƒ ìµœëŒ€ í”¼í¬ (ë¸”ë™í”„ë¼ì´ë°ì´ + ì„±ì¥ë¥  + ì•ˆì „ë§ˆì§„)
    predicted_peak = historical_data['black_friday'] * growth_rate * 1.2
    
    # RPSë¥¼ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ë¡œ ë³€í™˜ (ì¸ìŠ¤í„´ìŠ¤ë‹¹ 100 RPS ì²˜ë¦¬)
    instances_needed = int(predicted_peak / 100)
    
    return {
        'predicted_rps': predicted_peak,
        'instances_needed': instances_needed,
        'safety_margin': '20%'
    }

# í”¼í¬ ì‹œê°„ëŒ€ ìŠ¤ì¼€ì¼ë§ ìŠ¤ì¼€ì¤„
def create_peak_schedule():
    return {
        'workday_morning': {
            'time': '08:00',
            'scale_to': 15,
            'timezone': 'Asia/Seoul'
        },
        'lunch_time': {
            'time': '12:00', 
            'scale_to': 20,
            'timezone': 'Asia/Seoul'
        },
        'evening_peak': {
            'time': '18:00',
            'scale_to': 25, 
            'timezone': 'Asia/Seoul'
        },
        'night_scale_down': {
            'time': '22:00',
            'scale_to': 5,
            'timezone': 'Asia/Seoul'
        }
    }
```

---

## 6. ë¹„ìš© ìµœì í™” ì „ëµ

### â° ì ì‹œ í”„ë¡œë¹„ì €ë‹ (Just-in-Time Provisioning)

> [!tip] í•„ìš”í•œ ìˆœê°„ì—ë§Œ í• ë‹¹
> í•„ìš”í•œ ìˆœê°„ì—ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.

#### ğŸ“‹ ì ìš© ì‚¬ë¡€
- **ê°œë°œ í™˜ê²½**: ì—…ë¬´ ì‹œê°„ì—ë§Œ ê°€ë™
- **í…ŒìŠ¤íŠ¸ í™˜ê²½**: ì‚¬ìš© ì‹œì—ë§Œ ìƒì„±
- **ë°°ì¹˜ ì‘ì—…**: ì™„ë£Œ í›„ ì¦‰ì‹œ í•´ì œ

#### ğŸ“Š ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ìë™í™”

```bash
#!/bin/bash
# ê°œë°œ í™˜ê²½ ìë™ ì‹œì‘/ì¢…ë£Œ ìŠ¤í¬ë¦½íŠ¸

# í‰ì¼ ì˜¤ì „ 8ì‹œ ì‹œì‘
start_dev_environment() {
    echo "Starting development environment..."
    
    # RDS ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘
    aws rds start-db-instance --db-instance-identifier dev-database
    
    # EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘  
    aws ec2 start-instances --instance-ids i-dev01 i-dev02
    
    # ECS ì„œë¹„ìŠ¤ ìŠ¤ì¼€ì¼ ì—…
    aws ecs update-service \
        --cluster dev-cluster \
        --service dev-api-service \
        --desired-count 2
        
    echo "Development environment started"
}

# í‰ì¼ ì˜¤í›„ 8ì‹œ ì¢…ë£Œ
stop_dev_environment() {
    echo "Stopping development environment..."
    
    # ECS ì„œë¹„ìŠ¤ ìŠ¤ì¼€ì¼ ë‹¤ìš´
    aws ecs update-service \
        --cluster dev-cluster \
        --service dev-api-service \
        --desired-count 0
    
    # EC2 ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
    aws ec2 stop-instances --instance-ids i-dev01 i-dev02
    
    # RDS ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
    aws rds stop-db-instance --db-instance-identifier dev-database
    
    echo "Development environment stopped"
}

# Cron ì„¤ì •
# 0 8 * * 1-5 /path/to/start_dev_environment.sh
# 0 20 * * 1-5 /path/to/stop_dev_environment.sh
```

### ğŸ’° ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ í™œìš©

> [!info] ìµœëŒ€ 90% ë¹„ìš© ì ˆê°
> ì¤‘ë‹¨ ê°€ëŠ¥í•œ ì›Œí¬ë¡œë“œì— ì €ë ´í•œ ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### ğŸ“‹ ìŠ¤íŒŸ ì í•© ì›Œí¬ë¡œë“œ
- **ë°°ì¹˜ ì²˜ë¦¬**: ì¤‘ë‹¨ë˜ì–´ë„ ì¬ì‹œì‘ ê°€ëŠ¥
- **ë¹…ë°ì´í„° ë¶„ì„**: ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¼ë¶€ ì‹¤íŒ¨ í—ˆìš©  
- **CI/CD íŒŒì´í”„ë¼ì¸**: ì‹¤íŒ¨ ì‹œ ì¬ì‹¤í–‰ ê°€ëŠ¥

#### ğŸ“Š ìŠ¤íŒŸ í”Œë¦¿ êµ¬ì„±

```yaml
# Spot Fleet ì„¤ì •
Resources:
  SpotFleet:
    Type: AWS::EC2::SpotFleet
    Properties:
      SpotFleetRequestConfig:
        IamFleetRole: !GetAtt SpotFleetRole.Arn
        TargetCapacity: 10
        AllocationStrategy: diversified  # ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë‹¤ì–‘í™”
        
        LaunchSpecifications:
        # ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
        - ImageId: ami-12345678
          InstanceType: m5.large
          KeyName: my-key-pair
          SecurityGroups: [sg-12345678]
          SubnetId: subnet-12345678
          UserData: !Base64
            Fn::Sub: |
              #!/bin/bash
              # ìŠ¤íŒŸ ì¢…ë£Œ ì•Œë¦¼ ëª¨ë‹ˆí„°ë§
              while sleep 5; do
                if curl -s http://169.254.169.254/latest/meta-data/spot/instance-action; then
                  # 2ë¶„ ì „ ì¢…ë£Œ ì•Œë¦¼ ì‹œ ê·¸ë ˆì´ìŠ¤í’€ ì…§ë‹¤ìš´
                  /opt/graceful-shutdown.sh
                  break
                fi
              done &
              
              # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
              /opt/start-application.sh
              
        - ImageId: ami-12345678
          InstanceType: m5.xlarge  
          KeyName: my-key-pair
          SecurityGroups: [sg-12345678]
          SubnetId: subnet-87654321
          
        - ImageId: ami-12345678
          InstanceType: c5.large
          KeyName: my-key-pair  
          SecurityGroups: [sg-12345678]
          SubnetId: subnet-13579246
```

### ğŸ“Š í‹°ì–´ë³„ ì°¨ë“± í™•ì¥

> [!warning] ì›Œí¬ë¡œë“œ ì¤‘ìš”ë„ë³„ ëŒ€ì‘
> ì›Œí¬ë¡œë“œ ì¤‘ìš”ë„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ëŒ€ì‘í•©ë‹ˆë‹¤.

#### ğŸ“‹ í‹°ì–´ë³„ ì •ì±…

```yaml
# ì¤‘ìš”ë„ë³„ HPA ì„¤ì •

# Tier 1: ë§¤ì¶œ ê´€ë ¨ (ì ê·¹ì  í™•ì¥)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: payment-service-hpa
spec:
  scaleTargetRef:
    name: payment-service
  minReplicas: 5              # ë†’ì€ ìµœì†Œê°’
  maxReplicas: 100            # ë†’ì€ ìµœëŒ€ê°’
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # ë‚®ì€ ì„ê³„ê°’ (ë¹ ë¥¸ í™•ì¥)
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 15   # ë§¤ìš° ë¹ ë¥¸ í™•ì¥
      policies:
      - type: Percent
        value: 100            # ì¦‰ì‹œ 2ë°° í™•ì¥
        periodSeconds: 15

---
# Tier 2: ì¼ë°˜ ì„œë¹„ìŠ¤ (ë³´í†µ í™•ì¥)  
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-service-hpa
spec:
  scaleTargetRef:
    name: api-service
  minReplicas: 3
  maxReplicas: 30
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60   # 1ë¶„ ì•ˆì •í™”
      policies:
      - type: Percent
        value: 50             # 50% ì¦ê°€
        periodSeconds: 60

---
# Tier 3: ë‚´ë¶€ ë„êµ¬ (ë³´ìˆ˜ì  í™•ì¥)
apiVersion: autoscaling/v2  
kind: HorizontalPodAutoscaler
metadata:
  name: internal-tool-hpa
spec:
  scaleTargetRef:
    name: internal-tool
  minReplicas: 1              # ë‚®ì€ ìµœì†Œê°’
  maxReplicas: 5              # ì œí•œëœ ìµœëŒ€ê°’
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80  # ë†’ì€ ì„ê³„ê°’
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300  # 5ë¶„ ì•ˆì •í™”
      policies:
      - type: Pods
        value: 1              # í•œ ë²ˆì— 1ê°œì”©ë§Œ
        periodSeconds: 300
```

### ğŸ¯ ë¦¬ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„

#### ğŸ“Š ì˜ˆì‚° ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ í• ë‹¹

```python
# ë¦¬ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ê´€ë¦¬ ì‹œìŠ¤í…œ
class ResourcePriorityManager:
    def __init__(self):
        self.budget_allocation = {
            'tier1_services': {
                'budget_percentage': 60,
                'services': ['payment', 'auth', 'checkout'],
                'min_instances': 5,
                'max_instances': 100
            },
            'tier2_services': {
                'budget_percentage': 30, 
                'services': ['api', 'search', 'recommendation'],
                'min_instances': 2,
                'max_instances': 20
            },
            'tier3_services': {
                'budget_percentage': 10,
                'services': ['analytics', 'reporting', 'monitoring'],
                'min_instances': 1,
                'max_instances': 5
            }
        }
    
    def allocate_resources(self, total_budget, current_usage):
        """ì˜ˆì‚° ë‚´ì—ì„œ ìš°ì„ ìˆœìœ„ë³„ ë¦¬ì†ŒìŠ¤ í• ë‹¹"""
        allocation = {}
        
        # Tier 1ì´ ìš°ì„ 
        for tier, config in self.budget_allocation.items():
            tier_budget = total_budget * (config['budget_percentage'] / 100)
            
            # í˜„ì¬ ì‚¬ìš©ëŸ‰ì´ ì˜ˆì‚°ì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            if current_usage.get(tier, 0) > tier_budget:
                if tier == 'tier1_services':
                    # Tier 1ì€ ì˜ˆì‚° ì´ˆê³¼ í—ˆìš© (ë§¤ì¶œ ê´€ë ¨)
                    allocation[tier] = 'unlimited'
                else:
                    # ë‹¤ë¥¸ í‹°ì–´ëŠ” ì œí•œ
                    allocation[tier] = 'limited'
            else:
                allocation[tier] = 'normal'
                
        return allocation

# AWS ë¹„ìš© ëª¨ë‹ˆí„°ë§ê³¼ ì—°ë™
def monitor_costs_and_scale():
    """ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì¼€ì¼ë§ ì¡°ì •"""
    import boto3
    
    ce_client = boto3.client('ce')  # Cost Explorer
    
    # ì¼ì¼ ë¹„ìš© ì¡°íšŒ
    response = ce_client.get_cost_and_usage(
        TimePeriod={
            'Start': '2025-11-26',
            'End': '2025-11-27'
        },
        Granularity='DAILY',
        Metrics=['BlendedCost'],
        GroupBy=[
            {'Type': 'DIMENSION', 'Key': 'SERVICE'}
        ]
    )
    
    # ì˜ˆì‚° ì´ˆê³¼ ì‹œ ì•Œë¦¼ ë° ì¡°ì¹˜
    daily_budget = 1000  # $1000
    current_cost = float(response['ResultsByTime'][0]['Total']['BlendedCost']['Amount'])
    
    if current_cost > daily_budget * 0.8:  # 80% ë„ë‹¬ ì‹œ
        # ë¹„ì¤‘ìš” ì„œë¹„ìŠ¤ ìŠ¤ì¼€ì¼ ë‹¤ìš´
        scale_down_tier3_services()
        send_alert(f"Daily budget 80% reached: ${current_cost}")
```

---

## 7. ëª¨ë‹ˆí„°ë§ê³¼ ì œì–´

### ğŸ“Š ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

> [!note] íƒ„ë ¥ì„± ê²°ì •ì˜ ê¸°ë°˜
> ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì´ íƒ„ë ¥ì  ìì› ê´€ë¦¬ì˜ ê¸°ë°˜ì…ë‹ˆë‹¤.

#### ğŸ“‹ í•µì‹¬ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ ë¶„ë¥˜ | êµ¬ì²´ì  ì§€í‘œ | ìˆ˜ì§‘ ë„êµ¬ |
|-------------|-------------|-----------|
| **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ** | CPU, Memory, Disk I/O, Network | CloudWatch, Prometheus |
| **ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥** | ì‘ë‹µì‹œê°„, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨ | APM (New Relic, DataDog) |
| **ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­** | í™œì„± ì‚¬ìš©ì, ì£¼ë¬¸ëŸ‰, ë§¤ì¶œ | Custom Metrics |
| **ë¹„ìš© ë©”íŠ¸ë¦­** | ì‹œê°„ë‹¹ ë¹„ìš©, ì„œë¹„ìŠ¤ë³„ ë¹„ìš© | AWS Cost Explorer |

#### ğŸ“Š ì¢…í•© ëª¨ë‹ˆí„°ë§ ì„¤ì •

```yaml
# Prometheus + Grafana ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      
    rule_files:
      - "alert.rules"
      
    scrape_configs:
    # Kubernetes ë©”íŠ¸ë¦­
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'
        target_label: __address__
        replacement: '${1}:9100'
        
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­
    - job_name: 'web-app'
      static_configs:
      - targets: ['web-app:8080']
      metrics_path: '/actuator/prometheus'
      scrape_interval: 5s
      
    # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
    - job_name: 'business-metrics'
      static_configs:
      - targets: ['metrics-exporter:9090']

---
# Custom Metrics Server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-metrics-exporter
spec:
  replicas: 2
  selector:
    matchLabels:
      app: metrics-exporter
  template:
    spec:
      containers:
      - name: exporter
        image: my-company/metrics-exporter:v1.0
        ports:
        - containerPort: 9090
        env:
        - name: DB_CONNECTION
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: connection-string
```

### ğŸ§  ì˜ˆì¸¡ ë¶„ì„

> [!tip] ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡
> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ íŠ¸ë˜í”½ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ë¯¸ë˜ ìš©ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

#### ğŸ“Š ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„

```python
# íŠ¸ë˜í”½ ì˜ˆì¸¡ ëª¨ë¸
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import joblib

class TrafficPredictor:
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100)
        self.scaler = StandardScaler()
        
    def prepare_features(self, df):
        """ì‹œê³„ì—´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        df['month'] = df.index.month
        df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)
        df['is_holiday'] = self.is_holiday(df.index)
        
        # ì´ë™ í‰ê·  (íŠ¸ë Œë“œ)
        df['ma_24h'] = df['requests'].rolling(24).mean()
        df['ma_7d'] = df['requests'].rolling(24*7).mean()
        
        # ê³„ì ˆì„± (ì£¼ê°„, ì¼ê°„)
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        
        return df.dropna()
    
    def train(self, historical_data):
        """ëª¨ë¸ í›ˆë ¨"""
        df = self.prepare_features(historical_data)
        
        features = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_holiday',
                   'ma_24h', 'ma_7d', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']
        
        X = df[features]
        y = df['requests']
        
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.model, 'traffic_model.pkl')
        joblib.dump(self.scaler, 'traffic_scaler.pkl')
    
    def predict_next_24h(self, current_data):
        """ë‹¤ìŒ 24ì‹œê°„ íŠ¸ë˜í”½ ì˜ˆì¸¡"""
        predictions = []
        
        for hour in range(24):
            # ì˜ˆì¸¡ ì‹œì  íŠ¹ì„± ìƒì„±
            future_time = pd.Timestamp.now() + pd.Timedelta(hours=hour)
            features = self.extract_features_for_time(future_time, current_data)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            features_scaled = self.scaler.transform([features])
            prediction = self.model.predict(features_scaled)[0]
            
            predictions.append({
                'time': future_time,
                'predicted_requests': prediction,
                'recommended_instances': self.calculate_instances(prediction)
            })
            
        return predictions
    
    def calculate_instances(self, predicted_requests):
        """ì˜ˆìƒ ìš”ì²­ ìˆ˜ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ë¡œ ë³€í™˜"""
        requests_per_instance = 100  # ì¸ìŠ¤í„´ìŠ¤ë‹¹ ì²˜ë¦¬ ê°€ëŠ¥ RPS
        min_instances = 2
        
        needed = max(min_instances, int(predicted_requests / requests_per_instance))
        return needed * 1.2  # 20% ì•ˆì „ ë§ˆì§„
```

### ğŸ¤– ìë™í™”ëœ ëŒ€ì‘

#### ğŸ“Š ìë™ ìŠ¤ì¼€ì¼ë§ ì›Œí¬í”Œë¡œ

```yaml
# AWS Step Functionsë¡œ ìë™í™”ëœ ìŠ¤ì¼€ì¼ë§ ì›Œí¬í”Œë¡œ
{
  "Comment": "Intelligent Auto Scaling Workflow",
  "StartAt": "CollectMetrics",
  "States": {
    "CollectMetrics": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:collect-metrics",
      "Next": "PredictLoad"
    },
    "PredictLoad": {
      "Type": "Task", 
      "Resource": "arn:aws:lambda:region:account:function:predict-traffic",
      "Next": "CheckBudget"
    },
    "CheckBudget": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:check-budget",
      "Next": "DecideAction"
    },
    "DecideAction": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.action",
          "StringEquals": "scale_up",
          "Next": "ScaleUp"
        },
        {
          "Variable": "$.action", 
          "StringEquals": "scale_down",
          "Next": "ScaleDown"
        }
      ],
      "Default": "NoAction"
    },
    "ScaleUp": {
      "Type": "Parallel",
      "Branches": [
        {
          "StartAt": "ScaleEC2",
          "States": {
            "ScaleEC2": {
              "Type": "Task",
              "Resource": "arn:aws:lambda:region:account:function:scale-ec2",
              "End": true
            }
          }
        },
        {
          "StartAt": "ScaleECS", 
          "States": {
            "ScaleECS": {
              "Type": "Task",
              "Resource": "arn:aws:lambda:region:account:function:scale-ecs",
              "End": true
            }
          }
        },
        {
          "StartAt": "NotifyTeam",
          "States": {
            "NotifyTeam": {
              "Type": "Task",
              "Resource": "arn:aws:lambda:region:account:function:send-notification",
              "End": true
            }
          }
        }
      ],
      "Next": "UpdateMetrics"
    },
    "ScaleDown": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:gradual-scale-down", 
      "Next": "UpdateMetrics"
    },
    "NoAction": {
      "Type": "Pass",
      "Next": "UpdateMetrics"
    },
    "UpdateMetrics": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:update-dashboards",
      "End": true
    }
  }
}
```

### ğŸ’° ë¹„ìš© ê±°ë²„ë„ŒìŠ¤

#### ğŸ“Š ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë° ì œì–´

```python
# ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë° ì œì–´
class CostGovernance:
    def __init__(self):
        self.budget_limits = {
            'daily': 1000,
            'weekly': 6000, 
            'monthly': 25000
        }
        self.alert_thresholds = [0.5, 0.8, 0.9, 1.0]  # 50%, 80%, 90%, 100%
        
    def monitor_costs(self):
        """ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§"""
        import boto3
        
        ce = boto3.client('ce')
        
        # ì˜¤ëŠ˜ì˜ ë¹„ìš©
        today_cost = self.get_daily_cost(ce)
        
        # ì„ê³„ê°’ ì²´í¬
        for threshold in self.alert_thresholds:
            if today_cost >= self.budget_limits['daily'] * threshold:
                self.trigger_cost_action(threshold, today_cost)
                
    def trigger_cost_action(self, threshold, current_cost):
        """ì„ê³„ê°’ë³„ ìë™ ì•¡ì…˜"""
        if threshold == 0.5:
            # 50% ë„ë‹¬: ì•Œë¦¼ë§Œ
            self.send_notification(f"Daily budget 50% reached: ${current_cost}")
            
        elif threshold == 0.8:
            # 80% ë„ë‹¬: Tier 3 ì„œë¹„ìŠ¤ ìŠ¤ì¼€ì¼ ë‹¤ìš´
            self.scale_down_tier3()
            self.send_notification(f"Daily budget 80% reached: ${current_cost}. Tier 3 services scaled down.")
            
        elif threshold == 0.9:
            # 90% ë„ë‹¬: Tier 2 ì„œë¹„ìŠ¤ë„ ì œí•œ
            self.limit_tier2_scaling()
            self.send_notification(f"Daily budget 90% reached: ${current_cost}. Tier 2 scaling limited.")
            
        elif threshold >= 1.0:
            # 100% ì´ˆê³¼: ê¸´ê¸‰ ì¡°ì¹˜
            self.emergency_cost_control()
            self.send_alert(f"URGENT: Daily budget exceeded: ${current_cost}")
    
    def emergency_cost_control(self):
        """ê¸´ê¸‰ ë¹„ìš© ì œì–´"""
        # ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì™¸ ëª¨ë“  ì˜¨ë””ë§¨ë“œ ì¸ìŠ¤í„´ìŠ¤ ìŠ¤ì¼€ì¼ ë‹¤ìš´
        # ë¹„ì¤‘ìš” ì„œë¹„ìŠ¤ ì™„ì „ ì¤‘ì§€
        # ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤‘ì§€
        pass

# ë¦¬ì†ŒìŠ¤ íƒœê¹… ê¸°ë°˜ ë¹„ìš© ì¶”ì 
def tag_resources_for_cost_tracking():
    """ë¹„ìš© ì¶”ì ì„ ìœ„í•œ ë¦¬ì†ŒìŠ¤ íƒœê¹…"""
    tagging_strategy = {
        'Environment': ['production', 'staging', 'development'],
        'Service': ['web', 'api', 'database', 'cache'],
        'Team': ['platform', 'backend', 'frontend', 'data'],
        'CostCenter': ['engineering', 'marketing', 'sales'],
        'Project': ['v2-migration', 'mobile-app', 'analytics']
    }
    
    # ëª¨ë“  ë¦¬ì†ŒìŠ¤ì— íƒœê·¸ ì ìš©
    # ë¹„ìš© í• ë‹¹ ë³´ê³ ì„œ ìƒì„±
    # íŒ€ë³„/í”„ë¡œì íŠ¸ë³„ ë¹„ìš© ì¶”ì 
    pass
```

---

## ğŸŒ ì‹¤ì œ ì‚¬ë¡€

### ğŸ“º Netflix: í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ íƒ„ë ¥ì„±ì˜ ì„ êµ¬ì

> [!example] ê¸€ë¡œë²Œ ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ì˜ íƒ„ë ¥ì„±
> NetflixëŠ” ì „ ì„¸ê³„ 200ì—¬ ê°œêµ­ì—ì„œ ì‹œì²­ íŒ¨í„´ì— ë”°ë¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸í”„ë¼ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.

#### ğŸ“Š Netflixì˜ íƒ„ë ¥ì„± ì „ëµ

```yaml
# Netflix-style Multi-Region Scaling
regions:
  us_west_2:
    prime_time: "19:00-23:00 PST"
    peak_scaling: "300% of baseline"
    content_cache: "US popular content"
    
  eu_west_1:  
    prime_time: "19:00-23:00 CET"
    peak_scaling: "250% of baseline"
    content_cache: "EU popular content"
    
  ap_southeast_1:
    prime_time: "19:00-23:00 SGT" 
    peak_scaling: "200% of baseline"
    content_cache: "APAC popular content"

# ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ í™œìš© ì „ëµ
spot_strategy:
  encoding_jobs:
    spot_percentage: 80      # 80%ëŠ” ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤
    fault_tolerance: high    # ì¤‘ë‹¨ í—ˆìš©
    
  recommendation_engine:
    spot_percentage: 60      # 60%ëŠ” ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤  
    checkpointing: enabled   # ì¤‘ê°„ ì €ì¥
    
  streaming_delivery:
    spot_percentage: 20      # 20%ë§Œ ìŠ¤íŒŸ ì‚¬ìš©
    availability: critical   # ë†’ì€ ê°€ìš©ì„± í•„ìš”
```

#### ğŸ’¡ í•µì‹¬ ê¸°ë²•
- **ì§€ì—­ë³„ ì‹œì²­ íŒ¨í„´** ë¶„ì„ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì´ë™
- **ì½˜í…ì¸  ì¸ê¸°ë„** ê¸°ë°˜ CDN ìš©ëŸ‰ ì¡°ì •
- **ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ 80% í™œìš©**ìœ¼ë¡œ ëŒ€ê·œëª¨ ë¹„ìš© ì ˆê°
- **Chaos Engineering**ìœ¼ë¡œ íƒ„ë ¥ì„± ì§€ì† í…ŒìŠ¤íŠ¸

### ğŸ  Airbnb: ì˜ˆì•½ íŒ¨í„´ ê¸°ë°˜ íƒ„ë ¥ì„±

> [!info] ì—¬í–‰ ì‚°ì—…ì˜ ê³„ì ˆì„±ê³¼ ì´ë²¤íŠ¸ ëŒ€ì‘
> AirbnbëŠ” ì—¬í–‰ ì‹œì¦Œ, ì´ë²¤íŠ¸, íœ´ê°€ì² ì— ë”°ë¥¸ ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ì„ í™œìš©í•©ë‹ˆë‹¤.

#### ğŸ“Š Airbnbì˜ ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

```python
# Airbnb-style ì˜ˆì¸¡ ëª¨ë¸
class AirbnbScalingPredictor:
    def __init__(self):
        self.seasonal_patterns = {
            'summer_vacation': {
                'months': [6, 7, 8],
                'scaling_factor': 2.5,
                'regions': ['europe', 'north_america']
            },
            'winter_holidays': {
                'months': [12, 1],
                'scaling_factor': 2.0,
                'regions': ['ski_destinations', 'tropical']
            },
            'spring_break': {
                'weeks': ['week_12', 'week_13'],
                'scaling_factor': 3.0,
                'regions': ['beach_destinations']
            }
        }
        
    def predict_capacity_needs(self, region, timeframe):
        """ì§€ì—­ë³„, ì‹œê¸°ë³„ ìš©ëŸ‰ ìš”êµ¬ì‚¬í•­ ì˜ˆì¸¡"""
        base_capacity = self.get_base_capacity(region)
        
        # ê³„ì ˆì„± ìš”ì¸
        seasonal_factor = self.calculate_seasonal_factor(timeframe)
        
        # ì´ë²¤íŠ¸ ìš”ì¸ (ì˜¬ë¦¼í”½, ì›”ë“œì»µ ë“±)
        event_factor = self.check_major_events(region, timeframe)
        
        # ê²½ì œ ìš”ì¸ (í™˜ìœ¨, ê²½ì œ ìƒí™©)
        economic_factor = self.assess_economic_factors(region)
        
        predicted_capacity = (
            base_capacity * 
            seasonal_factor * 
            event_factor * 
            economic_factor
        )
        
        return {
            'predicted_bookings': predicted_capacity,
            'search_load_multiplier': seasonal_factor * 1.5,  # ê²€ìƒ‰ì€ ì˜ˆì•½ë³´ë‹¤ ë†’ìŒ
            'payment_load_multiplier': seasonal_factor * 0.8   # ê²°ì œëŠ” ì˜ˆì•½ë³´ë‹¤ ë‚®ìŒ
        }
```

### ğŸµ Spotify: ê¸€ë¡œë²Œ ìŒì•… ìŠ¤íŠ¸ë¦¬ë°

> [!tip] ì‹œê°„ëŒ€ì™€ ë¬¸í™”ì  íŒ¨í„´ í™œìš©
> SpotifyëŠ” ì§€ì—­ë³„ ì‹œê°„ëŒ€ì™€ ìŒì•… ì²­ì·¨ íŒ¨í„´ì„ í™œìš©í•´ ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

#### ğŸ“Š Spotifyì˜ ê¸€ë¡œë²Œ ë¦¬ì†ŒìŠ¤ ì´ë™

```yaml
# Spotify-style Global Resource Migration
global_capacity_scheduler:
  
  # ì•„ì‹œì•„ íƒœí‰ì–‘ (07:00-12:00 UTC)
  apac_morning:
    time_range: "07:00-12:00 UTC"
    primary_regions: [ap-southeast-1, ap-northeast-1]
    scaling_target: 150%
    music_catalog: [k-pop, j-pop, bollywood]
    
  # ìœ ëŸ½ (12:00-18:00 UTC)  
  europe_afternoon:
    time_range: "12:00-18:00 UTC"
    primary_regions: [eu-west-1, eu-central-1]
    scaling_target: 200%
    music_catalog: [pop, rock, classical]
    
  # ë¶ë¯¸ (18:00-02:00 UTC)
  americas_evening:
    time_range: "18:00-02:00 UTC"  
    primary_regions: [us-west-2, us-east-1]
    scaling_target: 250%
    music_catalog: [pop, hip-hop, country]

# ì‹ ê·œ ì•¨ë²” ì¶œì‹œ ëŒ€ì‘
album_release_scaling:
  taylor_swift_release:
    pre_scale_hours: 2        # 2ì‹œê°„ ì „ ë¯¸ë¦¬ í™•ì¥
    capacity_multiplier: 5    # 5ë°° ìš©ëŸ‰ í™•ë³´
    duration_hours: 24        # 24ì‹œê°„ ìœ ì§€
    
  normal_release:
    pre_scale_hours: 0.5      # 30ë¶„ ì „ í™•ì¥  
    capacity_multiplier: 1.5  # 1.5ë°° ìš©ëŸ‰
    duration_hours: 4         # 4ì‹œê°„ ìœ ì§€
```

---

## âš ï¸ ë„ì „ ê³¼ì œ

### â±ï¸ í™•ì¥ ì†ë„ ë¬¸ì œ

> [!warning] í™•ì¥ ì§€ì—° ì‹œê°„
> ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ê³¼ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”ì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤.

#### ğŸ“‹ í™•ì¥ ì†ë„ ê°œì„  ë°©ë²•

| ë°©ë²• | ì„¤ëª… | íš¨ê³¼ |
|------|------|------|
| **ì›œ í’€** | ë¯¸ë¦¬ ì´ˆê¸°í™”ëœ ì¸ìŠ¤í„´ìŠ¤ ìœ ì§€ | 90% ì‹œê°„ ë‹¨ì¶• |
| **AMI ìµœì í™”** | ì• í”Œë¦¬ì¼€ì´ì…˜ í¬í•¨ ì´ë¯¸ì§€ | 60% ì‹œê°„ ë‹¨ì¶• |
| **ì»¨í…Œì´ë„ˆ ì‚¬ìš©** | ë¹ ë¥¸ ì‹œì‘ ì‹œê°„ | 80% ì‹œê°„ ë‹¨ì¶• |
| **ì‚¬ì „ í™•ì¥** | ì˜ˆì¸¡ ê¸°ë°˜ ë¯¸ë¦¬ í™•ì¥ | í™•ì¥ ì§€ì—° ì œê±° |

#### ğŸ“Š ì›œ í’€ êµ¬í˜„

```bash
#!/bin/bash
# AWS EC2 ì›œ í’€ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸

create_warm_pool() {
    # Launch Template ê¸°ë°˜ ì›œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    aws ec2 run-instances \
        --launch-template LaunchTemplateId=lt-12345678,Version=1 \
        --min-count 3 \
        --max-count 3 \
        --tag-specifications 'ResourceType=instance,Tags=[{Key=Purpose,Value=WarmPool}]'
    
    echo "Warm pool instances created"
}

promote_from_warm_pool() {
    # ì›œ í’€ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í™œì„± í’€ë¡œ ì´ë™
    warm_instances=$(aws ec2 describe-instances \
        --filters "Name=tag:Purpose,Values=WarmPool" \
        --query 'Reservations[*].Instances[*].InstanceId' \
        --output text)
    
    for instance_id in $warm_instances; do
        # ë¡œë“œë°¸ëŸ°ì„œì— ë“±ë¡
        aws elbv2 register-targets \
            --target-group-arn arn:aws:elasticloadbalancing:region:account:targetgroup/web-servers \
            --targets Id=$instance_id
        
        # íƒœê·¸ ë³€ê²½
        aws ec2 create-tags \
            --resources $instance_id \
            --tags Key=Purpose,Value=Active
            
        echo "Instance $instance_id promoted to active"
        break  # í•œ ë²ˆì— í•˜ë‚˜ì”©
    done
}

# ìƒˆë¡œìš´ ì›œ ì¸ìŠ¤í„´ìŠ¤ë¡œ êµì²´
replace_warm_instance() {
    create_warm_pool
    echo "Warm pool replenished"
}
```

### ğŸ”„ ìƒíƒœ ê´€ë¦¬ ë³µì¡ì„±

> [!info] ë¶„ì‚° í™˜ê²½ì—ì„œì˜ ìƒíƒœ ì²˜ë¦¬
> ì„¸ì…˜, ìºì‹œ, ë¡œì»¬ íŒŒì¼ì„ ì™¸ë¶€í™”í•´ì•¼ í•©ë‹ˆë‹¤.

#### ğŸ“Š Stateless ì•„í‚¤í…ì²˜ íŒ¨í„´

```yaml
# ì™¸ë¶€ ìƒíƒœ ì €ì¥ì†Œ ì„¤ì •
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # ì„¸ì…˜ ì €ì¥ì†Œ
  session_store: "redis://redis-cluster:6379"
  
  # íŒŒì¼ ì €ì¥ì†Œ  
  file_storage: "s3://app-files-bucket"
  
  # ìºì‹œ ì €ì¥ì†Œ
  cache_store: "elasticache://cache-cluster:11211"

---
# Redis ì„¸ì…˜ í´ëŸ¬ìŠ¤í„°
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  serviceName: redis-cluster
  replicas: 3
  template:
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: redis-data
          mountPath: /data
        command:
        - redis-server
        - --appendonly yes
        - --cluster-enabled yes
        - --cluster-config-file nodes.conf
        - --cluster-node-timeout 5000
  volumeClaimTemplates:
  - metadata:
      name: redis-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

#### ğŸ“Š ë¶„ì‚° ì„¸ì…˜ ê´€ë¦¬

```java
// Spring Boot ë¶„ì‚° ì„¸ì…˜ ì„¤ì •
@Configuration
@EnableRedisHttpSession
public class SessionConfig {
    
    @Bean
    public LettuceConnectionFactory connectionFactory() {
        RedisClusterConfiguration clusterConfig = new RedisClusterConfiguration();
        clusterConfig.clusterNode("redis-0.redis-cluster", 6379);
        clusterConfig.clusterNode("redis-1.redis-cluster", 6379); 
        clusterConfig.clusterNode("redis-2.redis-cluster", 6379);
        
        return new LettuceConnectionFactory(clusterConfig);
    }
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate() {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory());
        
        // JSON ì§ë ¬í™” ì„¤ì •
        template.setDefaultSerializer(new GenericJackson2JsonRedisSerializer());
        template.setKeySerializer(new StringRedisSerializer());
        
        return template;
    }
}

// ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œì—ì„œ ì‚¬ìš©
@RestController
public class ApiController {
    
    @GetMapping("/user-data")
    public ResponseEntity<UserData> getUserData(HttpSession session) {
        // ì„¸ì…˜ì€ ìë™ìœ¼ë¡œ Redisì— ì €ì¥/ì¡°íšŒë¨
        UserData userData = (UserData) session.getAttribute("userData");
        
        if (userData == null) {
            userData = userService.loadUserData();
            session.setAttribute("userData", userData);
        }
        
        return ResponseEntity.ok(userData);
    }
}
```

### ğŸ”— ì˜ì¡´ì„± ë³‘ëª©

> [!danger] í™•ì¥ë˜ì§€ ì•ŠëŠ” ì»´í¬ë„ŒíŠ¸
> ë°ì´í„°ë² ì´ìŠ¤, ì™¸ë¶€ API, ë ˆê±°ì‹œ ì‹œìŠ¤í…œì´ ì „ì²´ ì‹œìŠ¤í…œì˜ ë³‘ëª©ì´ ë©ë‹ˆë‹¤.

#### ğŸ“Š ì˜ì¡´ì„± ë³‘ëª© í•´ê²° ì „ëµ

```yaml
# 1. ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥
database_scaling:
  primary_db:
    type: "write_operations"  
    scaling: "vertical_only"    # ì“°ê¸°ëŠ” ìˆ˜ì§ í™•ì¥ë§Œ
    
  read_replicas:
    type: "read_operations"
    scaling: "horizontal"       # ì½ê¸°ëŠ” ìˆ˜í‰ í™•ì¥
    auto_scaling:
      min_replicas: 2
      max_replicas: 10
      cpu_threshold: 70
      
  caching_layer:
    type: "redis_cluster"
    purpose: "reduce_db_load"
    ttl: 300                    # 5ë¶„ ìºì‹œ
    hit_ratio_target: 85        # 85% ìºì‹œ íˆíŠ¸ìœ¨

# 2. ì™¸ë¶€ API ë³´í˜¸  
external_api_protection:
  rate_limiting:
    requests_per_second: 100
    burst_capacity: 200
    
  circuit_breaker:
    failure_threshold: 5        # 5íšŒ ì‹¤íŒ¨ ì‹œ ì°¨ë‹¨
    timeout: 30s               # 30ì´ˆ í›„ ì¬ì‹œë„
    
  fallback_strategy:
    cached_response: true       # ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©
    degraded_service: true      # ì œí•œëœ ê¸°ëŠ¥ ì œê³µ

# 3. ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ê²©ë¦¬
legacy_system_adapter:
  connection_pool:
    max_connections: 50         # ì—°ê²° ìˆ˜ ì œí•œ
    timeout: 5s                # ë¹ ë¥¸ íƒ€ì„ì•„ì›ƒ
    
  async_processing:
    enabled: true              # ë¹„ë™ê¸° ì²˜ë¦¬
    queue: "rabbitmq"          # ë©”ì‹œì§€ í ì‚¬ìš©
    
  monitoring:
    response_time: true        # ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§
    error_rate: true           # ì—ëŸ¬ìœ¨ ì¶”ì 
```

### ğŸ’¸ ë¹„ìš© ì˜ˆì¸¡ ì–´ë ¤ì›€

#### ğŸ“Š ë¹„ìš© ì˜ˆì¸¡ ëª¨ë¸

```python
# ë™ì  ë¹„ìš© ì˜ˆì¸¡ ì‹œìŠ¤í…œ
class DynamicCostPredictor:
    def __init__(self):
        self.pricing_models = {
            'ec2_on_demand': 0.10,      # ì‹œê°„ë‹¹ USD
            'ec2_reserved': 0.06,       # 1ë…„ ì˜ˆì•½ í• ì¸
            'ec2_spot': 0.03,          # í‰ê·  ìŠ¤íŒŸ ê°€ê²©
            'lambda_requests': 0.0000002, # ìš”ì²­ë‹¹ USD
            'lambda_duration': 0.0000166667, # GB-ì´ˆë‹¹ USD
        }
        
    def predict_monthly_cost(self, usage_patterns):
        """ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ì›”ê°„ ë¹„ìš© ì˜ˆì¸¡"""
        total_cost = 0
        
        # ì»´í“¨íŒ… ë¹„ìš©
        compute_hours = usage_patterns.get('compute_hours', 0)
        on_demand_ratio = usage_patterns.get('on_demand_ratio', 0.3)
        spot_ratio = usage_patterns.get('spot_ratio', 0.7)
        
        on_demand_cost = (
            compute_hours * on_demand_ratio * 
            self.pricing_models['ec2_on_demand']
        )
        
        spot_cost = (
            compute_hours * spot_ratio * 
            self.pricing_models['ec2_spot']
        )
        
        # ì„œë²„ë¦¬ìŠ¤ ë¹„ìš©
        lambda_invocations = usage_patterns.get('lambda_invocations', 0)
        lambda_duration_gb_seconds = usage_patterns.get('lambda_duration', 0)
        
        lambda_cost = (
            lambda_invocations * self.pricing_models['lambda_requests'] +
            lambda_duration_gb_seconds * self.pricing_models['lambda_duration']
        )
        
        total_cost = on_demand_cost + spot_cost + lambda_cost
        
        # ë³€ë™ì„± ê³ ë ¤ (Â±30%)
        cost_range = {
            'minimum': total_cost * 0.7,
            'expected': total_cost,
            'maximum': total_cost * 1.3
        }
        
        return cost_range
    
    def create_budget_alerts(self, predicted_cost):
        """ì˜ˆì‚° ì•Œë¦¼ ì„¤ì •"""
        thresholds = [0.5, 0.8, 0.9, 1.0]
        
        alerts = []
        for threshold in thresholds:
            alert_amount = predicted_cost['expected'] * threshold
            alerts.append({
                'threshold': f"{threshold * 100}%",
                'amount': alert_amount,
                'action': self.get_action_for_threshold(threshold)
            })
            
        return alerts
    
    def get_action_for_threshold(self, threshold):
        """ì„ê³„ê°’ë³„ ìë™ ì•¡ì…˜"""
        actions = {
            0.5: "Send notification to team",
            0.8: "Scale down non-critical services",  
            0.9: "Limit auto-scaling for tier-2 services",
            1.0: "Emergency cost controls activated"
        }
        return actions.get(threshold, "Monitor closely")

# ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì 
def track_real_time_costs():
    """ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì  ë° ì¡°ì •"""
    import boto3
    
    ce = boto3.client('ce')
    
    # í˜„ì¬ê¹Œì§€ì˜ ì¼ì¼ ë¹„ìš©
    response = ce.get_cost_and_usage(
        TimePeriod={
            'Start': '2025-11-26',
            'End': '2025-11-27'
        },
        Granularity='DAILY',
        Metrics=['BlendedCost']
    )
    
    current_spend = float(response['ResultsByTime'][0]['Total']['BlendedCost']['Amount'])
    
    # í•˜ë£¨ ë‚¨ì€ ì‹œê°„ ê¸°ì¤€ ì˜ˆìƒ ë¹„ìš©
    import datetime
    now = datetime.datetime.now()
    hours_left_in_day = 24 - now.hour
    
    if hours_left_in_day > 0:
        projected_daily_spend = current_spend * (24 / (24 - hours_left_in_day))
    else:
        projected_daily_spend = current_spend
    
    return {
        'current_spend': current_spend,
        'projected_daily_spend': projected_daily_spend,
        'budget_utilization': projected_daily_spend / 1000,  # $1000 ì¼ì¼ ì˜ˆì‚°
        'recommendation': get_cost_recommendation(projected_daily_spend)
    }

def get_cost_recommendation(projected_spend):
    """ë¹„ìš© ê¸°ë°˜ ê¶Œì¥ì‚¬í•­"""
    daily_budget = 1000
    
    if projected_spend < daily_budget * 0.7:
        return "SAFE - Normal operations"
    elif projected_spend < daily_budget * 0.9:
        return "CAUTION - Monitor closely"
    elif projected_spend < daily_budget:
        return "WARNING - Consider scaling down non-critical services"
    else:
        return "CRITICAL - Immediate cost reduction needed"
```

---

## ğŸ¯ ëª¨ë²” ì‚¬ë¡€

### ğŸ“‹ ì„¤ê³„ ì›ì¹™

> [!tip] íƒ„ë ¥ì  ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™
> ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì²˜ìŒë¶€í„° íƒ„ë ¥ì„±ì„ ê³ ë ¤í•˜ì—¬ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.

#### ğŸ“Š 12-Factor App ê¸°ë°˜ íƒ„ë ¥ì  ì„¤ê³„

```yaml
# 12-Factor App ì›ì¹™ ì ìš©
twelve_factor_implementation:
  
  # 1. ì½”ë“œë² ì´ìŠ¤ (Codebase)
  codebase:
    principle: "í•˜ë‚˜ì˜ ì½”ë“œë² ì´ìŠ¤, ì—¬ëŸ¬ ë°°í¬"
    implementation: "Git ë¦¬í¬ì§€í† ë¦¬ í•˜ë‚˜, í™˜ê²½ë³„ ë°°í¬"
    
  # 2. ì˜ì¡´ì„± (Dependencies)  
  dependencies:
    principle: "ì˜ì¡´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸"
    implementation: "package.json, requirements.txt, go.mod"
    
  # 3. ì„¤ì • (Config)
  config:
    principle: "í™˜ê²½ë³„ ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì €ì¥"
    implementation: |
      # í™˜ê²½ë³„ ì„¤ì •
      - DATABASE_URL=postgres://...
      - REDIS_URL=redis://...
      - API_KEY=${SECRET_API_KEY}
      
  # 4. ë°±í‚¹ ì„œë¹„ìŠ¤ (Backing Services)
  backing_services:
    principle: "ë°±í‚¹ ì„œë¹„ìŠ¤ë¥¼ ì—°ê²°ëœ ë¦¬ì†ŒìŠ¤ë¡œ ì·¨ê¸‰"
    implementation: "DB, ìºì‹œ, íë¥¼ URLë¡œ ì—°ê²°"
    
  # 5. ë¹Œë“œ, ë¦´ë¦¬ìŠ¤, ì‹¤í–‰ (Build, Release, Run)
  build_release_run:
    principle: "ë¹Œë“œì™€ ì‹¤í–‰ ë‹¨ê³„ë¥¼ ì—„ê²©íˆ ë¶„ë¦¬"
    implementation: "CI/CD íŒŒì´í”„ë¼ì¸"
    
  # 6. í”„ë¡œì„¸ìŠ¤ (Processes)
  processes:
    principle: "ì•±ì„ í•˜ë‚˜ ì´ìƒì˜ ë¬´ìƒíƒœ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰"
    implementation: "ì„¸ì…˜ì„ Redisì—, íŒŒì¼ì„ S3ì— ì €ì¥"
```

#### ğŸ“Š ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ íƒ„ë ¥ì„± íŒ¨í„´

```yaml
# ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë³„ ë…ë¦½ ìŠ¤ì¼€ì¼ë§
services:
  user_service:
    scaling_policy:
      metric: "cpu_utilization"
      target: 70
      min_replicas: 2
      max_replicas: 20
    dependencies: [user_database, redis_cache]
    
  product_service:  
    scaling_policy:
      metric: "requests_per_second"
      target: 1000
      min_replicas: 3
      max_replicas: 50
    dependencies: [product_database, elasticsearch]
    
  order_service:
    scaling_policy:
      metric: "queue_depth" 
      target: 100
      min_replicas: 5
      max_replicas: 100
    dependencies: [order_database, payment_api, inventory_service]
    
  notification_service:
    scaling_policy:
      metric: "message_queue_size"
      target: 50
      min_replicas: 1
      max_replicas: 10
    dependencies: [notification_queue, email_api, sms_api]

# ì„œë¹„ìŠ¤ ë©”ì‹œ ì„¤ì • (Istio)
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: user-service-routing
spec:
  http:
  - match:
    - headers:
        user-type:
          exact: premium
    route:
    - destination:
        host: user-service
        subset: high-performance
      weight: 100
  - route:
    - destination:
        host: user-service  
        subset: standard
      weight: 100
```

### ğŸ”§ ë¹„ë™ê¸° ì²˜ë¦¬ì™€ ë©”ì‹œì§€ í

#### ğŸ“Š ì´ë²¤íŠ¸ ê¸°ë°˜ í™•ì¥

```yaml
# Apache Kafkaë¥¼ ì´ìš©í•œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: event-streaming-cluster
spec:
  kafka:
    replicas: 3
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      default.replication.factor: 3
      min.insync.replicas: 2
    storage:
      type: persistent-claim
      size: 100Gi
      
# KEDAë¥¼ ì´ìš©í•œ Kafka ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: kafka-consumer-scaler
spec:
  scaleTargetRef:
    name: order-processor
  minReplicaCount: 1
  maxReplicaCount: 50
  triggers:
  - type: kafka
    metadata:
      bootstrapServers: event-streaming-cluster-kafka-bootstrap:9092
      consumerGroup: order-processing-group
      topic: orders
      lagThreshold: '10'        # ë©”ì‹œì§€ 10ê°œë‹¹ 1ê°œ íŒŒë“œ

---
# RabbitMQë¥¼ ì´ìš©í•œ ì‘ì—… í
apiVersion: rabbitmq.com/v1beta1
kind: RabbitmqCluster
metadata:
  name: task-queue
spec:
  replicas: 3
  rabbitmq:
    additionalConfig: |
      cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
      cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      cluster_formation.node_cleanup.interval = 30
      cluster_formation.node_cleanup.only_log_warning = true
      cluster_partition_handling = autoheal
      queue_master_locator = min-masters
```

### ğŸš€ ë°°í¬ ì „ëµ

#### ğŸ“Š ì•ˆì „í•œ íƒ„ë ¥ì  ë°°í¬

```yaml
# Canary ë°°í¬ + Auto Scaling
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: web-app-rollout
spec:
  replicas: 10
  strategy:
    canary:
      steps:
      - setWeight: 10           # 10% íŠ¸ë˜í”½ìœ¼ë¡œ ì‹œì‘
      - pause: {duration: 30s}  # 30ì´ˆ ëŒ€ê¸°
      - setWeight: 25           # 25%ë¡œ ì¦ê°€
      - pause: {duration: 60s}  # 1ë¶„ ëŒ€ê¸°
      - setWeight: 50           # 50%ë¡œ ì¦ê°€
      - pause: {duration: 120s} # 2ë¶„ ëŒ€ê¸°
      - setWeight: 100          # 100% ì™„ë£Œ
        
      # ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°±
      abortScaleDownDelaySeconds: 30
      scaleDownDelaySeconds: 30
      
      # ë¶„ì„ ê¸°ë°˜ ìë™ ì§„í–‰/ë¡¤ë°±
      analysis:
        templates:
        - templateName: success-rate
          args:
          - name: service-name
            value: web-app
        - templateName: response-time
          args:
          - name: service-name  
            value: web-app

---
# ë¶„ì„ í…œí”Œë¦¿ (ì„±ê³µë¥ )
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: success-rate
spec:
  args:
  - name: service-name
  metrics:
  - name: success-rate
    interval: 30s
    count: 3
    successCondition: result[0] >= 0.95  # 95% ì´ìƒ ì„±ê³µë¥ 
    failureLimit: 2
    provider:
      prometheus:
        address: http://prometheus:9090
        query: |
          sum(
            rate(http_requests_total{job="{{args.service-name}}",status!~"5.."}[2m])
          ) / 
          sum(
            rate(http_requests_total{job="{{args.service-name}}"}[2m])
          )
```

### ğŸ§ª ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§

#### ğŸ“Š íƒ„ë ¥ì„± í…ŒìŠ¤íŠ¸

```yaml
# Chaos Meshë¥¼ ì´ìš©í•œ íƒ„ë ¥ì„± í…ŒìŠ¤íŠ¸
apiVersion: chaos-mesh.org/v1alpha1
kind: Schedule
metadata:
  name: auto-scaling-chaos-test
spec:
  schedule: '0 */6 * * *'  # 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
  type: PodChaos
  podChaos:
    mode: fixed-percent
    value: '50'            # 50% íŒŒë“œ ë¬´ì‘ìœ„ ì¢…ë£Œ
    action: pod-kill
    selector:
      namespaces: ['production']
      labelSelectors:
        'app': 'web-app'
    duration: '5m'

---
# ë„¤íŠ¸ì›Œí¬ ì§€ì—° í…ŒìŠ¤íŠ¸
apiVersion: chaos-mesh.org/v1alpha1  
kind: NetworkChaos
metadata:
  name: network-delay-test
spec:
  action: delay
  mode: all
  selector:
    namespaces: ['production']
    labelSelectors:
      'tier': 'api'
  delay:
    latency: '100ms'       # 100ms ì§€ì—° ì¶”ê°€
    correlation: '100'     # 100% ì ìš©
    jitter: '10ms'         # Â±10ms ë³€ë™
  duration: '10m'

---
# CPU ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos  
metadata:
  name: cpu-stress-test
spec:
  mode: fixed-percent
  value: '30'              # 30% ë…¸ë“œ ëŒ€ìƒ
  selector:
    namespaces: ['production']
  stressors:
    cpu:
      workers: 2           # 2ê°œ CPU ì›Œì»¤
      load: 80            # 80% CPU ì‚¬ìš©ë¥ 
  duration: '15m'
```

### ğŸ’° FinOps ë¬¸í™”

#### ğŸ“Š ì§€ì†ì ì¸ ë¹„ìš© ìµœì í™”

```python
# FinOps ìë™í™” ë„êµ¬
class FinOpsAutomation:
    def __init__(self):
        self.cost_optimization_rules = {
            'idle_resources': {
                'cpu_threshold': 5,      # CPU 5% ë¯¸ë§Œ
                'duration': 7200,       # 2ì‹œê°„ ì§€ì†
                'action': 'terminate'
            },
            'oversized_instances': {
                'cpu_threshold': 10,     # CPU 10% ë¯¸ë§Œ  
                'memory_threshold': 20,  # Memory 20% ë¯¸ë§Œ
                'duration': 86400,      # 24ì‹œê°„ ì§€ì†
                'action': 'downsize'
            },
            'unused_ebs_volumes': {
                'attachment_status': 'available',
                'age_days': 7,
                'action': 'delete'
            }
        }
    
    def daily_cost_optimization(self):
        """ì¼ì¼ ë¹„ìš© ìµœì í™” ì‘ì—…"""
        
        # 1. ìœ íœ´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        idle_instances = self.find_idle_instances()
        for instance in idle_instances:
            self.terminate_idle_instance(instance)
            
        # 2. ì˜¤ë²„ì‚¬ì´ì§•ëœ ì¸ìŠ¤í„´ìŠ¤ ì¡°ì •
        oversized = self.find_oversized_instances()
        for instance in oversized:
            self.recommend_right_sizing(instance)
            
        # 3. ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ê¸°íšŒ ë¶„ì„
        spot_opportunities = self.analyze_spot_opportunities()
        self.apply_spot_recommendations(spot_opportunities)
        
        # 4. ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ ìµœì í™”
        ri_recommendations = self.get_ri_recommendations()
        self.send_ri_purchase_recommendations(ri_recommendations)
        
        return {
            'idle_terminated': len(idle_instances),
            'rightsizing_recommendations': len(oversized),
            'spot_opportunities': len(spot_opportunities),
            'ri_recommendations': len(ri_recommendations)
        }
    
    def generate_cost_report(self):
        """ë¹„ìš© ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            'daily_spend': self.get_daily_spend(),
            'monthly_projection': self.project_monthly_spend(),
            'top_cost_drivers': self.get_top_cost_drivers(),
            'optimization_savings': self.calculate_optimization_savings(),
            'efficiency_score': self.calculate_efficiency_score()
        }

# Kubernetes ë¦¬ì†ŒìŠ¤ ìµœì í™”
def optimize_kubernetes_resources():
    """k8s ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ìµœì í™”"""
    
    # VPA ê¶Œì¥ì‚¬í•­ ì ìš©
    vpa_recommendations = get_vpa_recommendations()
    
    for recommendation in vpa_recommendations:
        apply_resource_limits(recommendation)
    
    # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ë¦¬
    cleanup_unused_namespaces()
    
    # PVC ì •ë¦¬ (ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë³¼ë¥¨)
    cleanup_orphaned_pvcs()
    
    # ConfigMap, Secret ì •ë¦¬
    cleanup_unused_configs()
```

---

## ğŸ¯ ì‹¤ì „ ì˜ˆì‹œ

### ğŸ’¡ ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ì˜ ì¢…í•© íƒ„ë ¥ì„±

> [!example] ì‹¤ì œ ì´ì»¤ë¨¸ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤
> **ë¬¸ì œ**: ë¸”ë™í”„ë¼ì´ë°ì´ ê°™ì€ ëŒ€ê·œëª¨ ì„¸ì¼ ì´ë²¤íŠ¸ì—ì„œ í‰ì†Œ íŠ¸ë˜í”½ì˜ 10ë°°ê°€ ëª°ë¦¬ëŠ” ìƒí™©

#### ğŸ“‹ êµ¬ì²´ì ì¸ ì‹œë‚˜ë¦¬ì˜¤

> [!warning] ì‹¤ì œ ìƒí™©
> 1. **í‰ìƒì‹œ**: ì´ˆë‹¹ 1,000ê±´ ì£¼ë¬¸, 100ê°œ ì¸ìŠ¤í„´ìŠ¤ë¡œ ìš´ì˜
> 2. **ì„¸ì¼ ì‹œì‘**: 30ë¶„ ë‚´ ì´ˆë‹¹ 10,000ê±´ìœ¼ë¡œ ê¸‰ì¦  
> 3. **ì˜ˆìƒ ë¬¸ì œ**: ì¥ë°”êµ¬ë‹ˆ/ê²°ì œ ì„œë²„ ê³¼ë¶€í•˜, DB ë³‘ëª©, ë¹„ìš© í­ì¦
> 4. **ìš”êµ¬ì‚¬í•­**: ì‚¬ìš©ì ê²½í—˜ ìœ ì§€í•˜ë©´ì„œ ë¹„ìš© ìµœì í™”

#### ğŸ“Š ì¢…í•© íƒ„ë ¥ì„± ì„¤ê³„

```yaml
# 1. ê³„ì¸µë³„ ìŠ¤ì¼€ì¼ë§ ì „ëµ
apiVersion: v1
kind: ConfigMap
metadata:
  name: black-friday-scaling-config
data:
  # í”„ë¡ íŠ¸ì—”ë“œ (CDN + Static)
  frontend_scaling: |
    CloudFront:
      price_class: PriceClass_All    # ì „ì„¸ê³„ ì—£ì§€
      cache_behavior:
        default_ttl: 3600           # 1ì‹œê°„ ìºì‹±
        max_ttl: 86400              # 24ì‹œê°„ ìµœëŒ€
      origin_shield: true           # Origin Shield í™œì„±í™”
      
  # ì›¹ ì„œë²„ (ìµœì „ë°©)  
  web_tier_scaling: |
    autoscaling:
      min_instances: 20             # í‰ìƒì‹œ 2ë°°
      max_instances: 500            # 5ë°° í™•ì¥ ê°€ëŠ¥
      target_cpu: 60               # ë³´ìˆ˜ì  ì„ê³„ê°’
      scale_up_cooldown: 60        # ë¹ ë¥¸ í™•ì¥
      scale_down_cooldown: 600     # ì²œì²œíˆ ì¶•ì†Œ
      
  # API ì„œë²„ (í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤)
  api_tier_scaling: |
    autoscaling:
      min_instances: 50             # ë†’ì€ ê¸°ë³¸ê°’
      max_instances: 1000           # ëŒ€ê·œëª¨ í™•ì¥
      target_cpu: 50               # ë§¤ìš° ë³´ìˆ˜ì 
      custom_metrics:
        - orders_per_second: 100    # ì£¼ë¬¸ ê¸°ì¤€ í™•ì¥
        - cart_operations: 500      # ì¥ë°”êµ¬ë‹ˆ ê¸°ì¤€
        
  # ë°ì´í„°ë² ì´ìŠ¤ (ë³‘ëª© ì§€ì )
  database_scaling: |
    read_replicas:
      min_replicas: 5               # í‰ìƒì‹œ 2ë°°
      max_replicas: 20              # ì½ê¸° ë¶„ì‚°
      auto_scaling: true
      
    connection_pooling:
      max_connections: 1000         # ì—°ê²° í’€ ì¦ê°€
      idle_timeout: 60             # ë¹ ë¥¸ íšŒì „
      
    caching_strategy:
      redis_cluster:
        nodes: 9                    # 3ë°° ì¦ê°€
        memory_per_node: 32gb       # ëŒ€ìš©ëŸ‰ ìºì‹œ
        ttl_strategy:
          product_catalog: 3600     # ìƒí’ˆ 1ì‹œê°„
          pricing: 300             # ê°€ê²© 5ë¶„
          inventory: 60            # ì¬ê³  1ë¶„
```

#### ğŸ“Š ì´ë²¤íŠ¸ ê¸°ë°˜ ìë™ í™•ì¥

```python
# ë¸”ë™í”„ë¼ì´ë°ì´ ìë™í™” ì‹œìŠ¤í…œ
class BlackFridayScalingOrchestrator:
    def __init__(self):
        self.phases = {
            'pre_event': {
                'start_time': '2025-11-28 18:00',
                'actions': ['pre_scale_infrastructure', 'validate_health_checks']
            },
            'event_start': {
                'start_time': '2025-11-29 00:00', 
                'actions': ['enable_aggressive_scaling', 'activate_monitoring']
            },
            'peak_hours': {
                'start_time': '2025-11-29 09:00',
                'actions': ['maximum_capacity', 'enable_queue_overflow']
            },
            'post_event': {
                'start_time': '2025-11-30 00:00',
                'actions': ['gradual_scale_down', 'cost_optimization']
            }
        }
        
    def execute_phase(self, phase_name):
        """ë‹¨ê³„ë³„ ìë™í™” ì‹¤í–‰"""
        phase = self.phases[phase_name]
        
        for action in phase['actions']:
            if action == 'pre_scale_infrastructure':
                self.pre_scale_all_tiers()
                
            elif action == 'enable_aggressive_scaling':
                self.update_hpa_policies(aggressive=True)
                
            elif action == 'maximum_capacity':
                self.scale_to_maximum_capacity()
                
            elif action == 'enable_queue_overflow':
                self.activate_overflow_queues()
                
            elif action == 'gradual_scale_down':
                self.gradual_scale_down()
                
            elif action == 'cost_optimization':
                self.optimize_costs()
    
    def pre_scale_all_tiers(self):
        """ì‚¬ì „ í™•ì¥ (ì´ë²¤íŠ¸ 6ì‹œê°„ ì „)"""
        scaling_targets = {
            'web-servers': {
                'desired_capacity': 100,     # í‰ìƒì‹œ 10ë°°  
                'health_check_grace': 300
            },
            'api-servers': {
                'desired_capacity': 200,     # í‰ìƒì‹œ 4ë°°
                'health_check_grace': 180
            },
            'worker-processes': {
                'desired_capacity': 50,      # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
                'health_check_grace': 60
            }
        }
        
        for service, config in scaling_targets.items():
            self.scale_service(service, config)
            
        # ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ì „ ì›Œë°ì—…
        self.warm_up_databases()
        
        # ìºì‹œ ì‚¬ì „ ë¡œë”©
        self.preload_caches()
    
    def activate_overflow_queues(self):
        """ì˜¤ë²„í”Œë¡œìš° ì²˜ë¦¬ ì‹œìŠ¤í…œ í™œì„±í™”"""
        
        # SQS ê¸°ë°˜ ì£¼ë¬¸ ì²˜ë¦¬ í í™•ì¥
        overflow_config = {
            'order_processing_queue': {
                'visibility_timeout': 300,
                'max_receive_count': 3,
                'dlq_enabled': True
            },
            'payment_processing_queue': {
                'visibility_timeout': 600,    # ê²°ì œëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒ
                'max_receive_count': 5
            },
            'inventory_update_queue': {
                'visibility_timeout': 60,     # ì¬ê³ ëŠ” ë¹ ë¥¸ ì²˜ë¦¬
                'max_receive_count': 2
            }
        }
        
        for queue_name, config in overflow_config.items():
            self.configure_sqs_queue(queue_name, config)
            
        # Lambda í•¨ìˆ˜ ë™ì‹œ ì‹¤í–‰ í•œë„ ì¦ê°€
        self.increase_lambda_concurrency()
    
    def monitor_and_respond(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ëŒ€ì‘"""
        metrics = self.collect_real_time_metrics()
        
        # ìœ„í—˜ ì‹ í˜¸ ê°ì§€
        if metrics['error_rate'] > 0.05:  # 5% ì—ëŸ¬ìœ¨
            self.emergency_scale_up()
            
        if metrics['response_time'] > 2000:  # 2ì´ˆ ì‘ë‹µì‹œê°„
            self.add_cache_instances()
            
        if metrics['queue_depth'] > 1000:  # í ì ì²´
            self.scale_up_workers()
            
        # ë¹„ìš© ëª¨ë‹ˆí„°ë§
        if metrics['hourly_cost'] > 500:  # ì‹œê°„ë‹¹ $500 ì´ˆê³¼
            self.cost_alert_and_optimize()

# ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì„¤ì •
dashboard_config = {
    'business_metrics': [
        'orders_per_minute',
        'revenue_per_minute', 
        'conversion_rate',
        'cart_abandonment_rate'
    ],
    'technical_metrics': [
        'response_time_p95',
        'error_rate',
        'cpu_utilization',
        'memory_utilization',
        'database_connections'
    ],
    'cost_metrics': [
        'hourly_spend',
        'cost_per_order',
        'infrastructure_efficiency'
    ]
}
```

#### ğŸ“Š ë¹„ìš© ìµœì í™” ì „ëµ

```yaml
# ë¸”ë™í”„ë¼ì´ë°ì´ ë¹„ìš© ìµœì í™”
cost_optimization_strategy:
  
  # 1. ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ì ê·¹ í™œìš© (60% ìŠ¤íŒŸ)
  spot_strategy:
    target_spot_percentage: 60
    diversification:
      instance_types: [m5.large, m5.xlarge, c5.large, c5.xlarge]
      availability_zones: [us-west-2a, us-west-2b, us-west-2c]
      
    # ìŠ¤íŒŸ ì¤‘ë‹¨ ëŒ€ì‘
    interruption_handling:
      enabled: true
      drain_timeout: 120        # 2ë¶„ ê·¸ë ˆì´ìŠ¤í’€ ë“œë ˆì´ë‹
      replacement_strategy: immediate
      
  # 2. ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ ê¸°ë³¸ ìš©ëŸ‰ (40% ì˜¨ë””ë§¨ë“œ)
  reserved_capacity:
    base_instances:
      web_tier: 20             # ê¸°ë³¸ ì›¹ ì„œë²„
      api_tier: 30             # ê¸°ë³¸ API ì„œë²„
      database: 2              # DB ì¸ìŠ¤í„´ìŠ¤
      
  # 3. ì„œë²„ë¦¬ìŠ¤ ì˜¤ë²„í”Œë¡œìš°
  serverless_overflow:
    lambda_functions:
      order_validation:
        reserved_concurrency: 1000
        provisioned_concurrency: 100   # ì›œ ìŠ¤íƒ€íŠ¸
        
      payment_processing:
        reserved_concurrency: 500
        timeout: 900                   # 15ë¶„ ì²˜ë¦¬ ì‹œê°„
        
    # API Gateway í™œìš©
    api_gateway:
      caching_enabled: true
      cache_ttl: 300                   # 5ë¶„ ìºì‹±
      throttling:
        burst_limit: 5000
        rate_limit: 2000

# ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì•Œë¦¼
cost_alerts:
  - threshold: 1000            # $1000/ì‹œê°„
    action: "Scale down non-critical services"
    
  - threshold: 1500            # $1500/ì‹œê°„  
    action: "Enable aggressive cost optimization"
    
  - threshold: 2000            # $2000/ì‹œê°„
    action: "Emergency cost controls + management alert"
```

#### ğŸ“Š ê²°ê³¼ ë¶„ì„

```python
# ë¸”ë™í”„ë¼ì´ë°ì´ ì„±ê³¼ ë¶„ì„
def analyze_event_performance():
    """ì´ë²¤íŠ¸ ì„±ê³¼ ë¶„ì„"""
    
    results = {
        'business_impact': {
            'total_orders': 2_500_000,      # í‰ìƒì‹œ 100ë°°
            'peak_orders_per_minute': 5000,  # í‰ìƒì‹œ 50ë°°  
            'revenue': 50_000_000,          # $50M ë§¤ì¶œ
            'conversion_rate': 0.125,       # 12.5% (í‰ìƒì‹œ ëŒ€ë¹„ ìœ ì§€)
            'customer_satisfaction': 0.94   # 94% ë§Œì¡±ë„
        },
        
        'technical_performance': {
            'uptime': 0.9998,               # 99.98% ê°€ìš©ì„±
            'avg_response_time': 850,       # 850ms í‰ê·  ì‘ë‹µ
            'p99_response_time': 2100,      # P99 2.1ì´ˆ
            'error_rate': 0.002,            # 0.2% ì—ëŸ¬ìœ¨
            'auto_scaling_events': 1247     # 1247íšŒ ìŠ¤ì¼€ì¼ë§
        },
        
        'cost_efficiency': {
            'total_event_cost': 75000,      # $75K (48ì‹œê°„)
            'cost_per_order': 0.03,         # $0.03/ì£¼ë¬¸
            'spot_instance_savings': 0.42,  # 42% ì ˆê°
            'peak_hour_cost': 2100,         # $2100/ì‹œê°„ (í”¼í¬)
            'cost_optimization_savings': 28000  # $28K ì ˆì•½
        }
    }
    
    # ROI ê³„ì‚°
    infrastructure_cost = results['cost_efficiency']['total_event_cost']
    revenue = results['business_impact']['revenue']
    roi = (revenue - infrastructure_cost) / infrastructure_cost * 100
    
    results['roi'] = f"{roi:.1f}%"  # 66,566% ROI
    
    return results

# í–¥í›„ ê°œì„ ì‚¬í•­ ë„ì¶œ
def generate_improvement_recommendations():
    """ë‹¤ìŒ ì´ë²¤íŠ¸ë¥¼ ìœ„í•œ ê°œì„ ì‚¬í•­"""
    
    return {
        'infrastructure': [
            "ë°ì´í„°ë² ì´ìŠ¤ ì½ê¸° ë³µì œë³¸ 2ë°° ì¦ì„¤",
            "Redis í´ëŸ¬ìŠ¤í„°ë¥¼ ì§€ì—­ë³„ ë¶„ì‚° ë°°ì¹˜", 
            "CDN ìºì‹œ íˆíŠ¸ìœ¨ì„ 95%ê¹Œì§€ ê°œì„ "
        ],
        'monitoring': [
            "ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§ ë„ì…",
            "ì˜ˆì¸¡ ëª¨ë¸ ì •í™•ë„ë¥¼ 85%ê¹Œì§€ í–¥ìƒ",
            "ì‹¤ì‹œê°„ ì½”ìŠ¤íŠ¸ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•"
        ],
        'cost_optimization': [
            "ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ë¹„ìœ¨ì„ 70%ê¹Œì§€ ì¦ê°€", 
            "ì„œë²„ë¦¬ìŠ¤ ìš°ì„  ì „ëµ í™•ëŒ€",
            "ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”"
        ]
    }
```

---

> [!tip] í•µì‹¬ í¬ì¸íŠ¸
> íƒ„ë ¥ì  ìì› ìš©ëŸ‰ ì•„í‚¤í…ì²˜ëŠ” í˜„ëŒ€ í´ë¼ìš°ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤ë¥¼ ìœ ì—°í•˜ê²Œ ì¡°ì •í•˜ê³ , ë¹„ìš©ì„ ìµœì í™”í•˜ë©°, ì‚¬ìš©ì ê²½í—˜ì„ ë³´ì¥í•˜ëŠ” ê²ƒì´ ì„±ê³µì˜ ì—´ì‡ ì…ë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§, ì˜ˆì¸¡ ë¶„ì„, ìë™í™”ë¥¼ í†µí•´ ì§„ì •í•œ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.