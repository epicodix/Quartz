---
title: M1 + M2 ë©€í‹° í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜ ì„¤ê³„
date: 2025-12-04
tags: [kubernetes, multi-cluster, distributed-systems, architecture, homelab]
status: ğŸ“‹ ê³„íš
difficulty: â­â­â­â­â­
related: [[íŠ¸ëŸ¬ë¸”ìŠˆíŒ…-2025-12-03]]
---

# ğŸ—ï¸ M1 + M2 ë©€í‹° í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜ ì„¤ê³„

> **ëª©í‘œ**: M1 ë§¥ë¯¸ë‹ˆ(16GB) + M2 ë§¥ë¶(8GB)ì„ í™œìš©í•œ ë¶„ì‚° Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
> **í•™ìŠµ ë²”ìœ„**: Multi-cluster networking, ë¶„ì‚°ì‹œìŠ¤í…œ ì´ë¡  ì‹¤ìŠµ, í•˜ë“œì›¨ì–´ ì œì•½ ê¸°ë°˜ ì›Œí¬ë¡œë“œ ìµœì í™”
> **ì˜ˆìƒ ê¸°ê°„**: 4-6ì£¼ (í”„ë¡œë©”í…Œìš°ìŠ¤ í•™ìŠµ ì™„ë£Œ í›„)

---

## ğŸ“Š í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼ ë¹„êµ

### ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤

| í•­ëª© | M2 ë§¥ë¶ (8GB) | M1 ë§¥ë¯¸ë‹ˆ (16GB) | ìŠ¹ì | ë¹„ê³  |
|------|--------------|-----------------|------|------|
| **Single-core ì„±ëŠ¥** | ~3.5 GHz (M2) | ~3.2 GHz (M1) | ğŸ† M2 | CPU ì§‘ì•½ ì‘ì—… ìœ ë¦¬ |
| **Multi-core íš¨ìœ¨** | 8 cores | 8 cores | ë™ë“± | ë³‘ë ¬ ì²˜ë¦¬ |
| **ë©”ëª¨ë¦¬ ìš©ëŸ‰** | 8GB (ì œì•½!) | 16GB (ì—¬ìœ ) | ğŸ† M1 | Stateful ì›Œí¬ë¡œë“œ |
| **ë©”ëª¨ë¦¬ ëŒ€ì—­í­** | 100 GB/s | 68 GB/s | ğŸ† M2 | ë°ì´í„° ì²˜ë¦¬ ì†ë„ |
| **ê°€ìƒí™” ì˜¤ë²„í—¤ë“œ** | ë‚®ìŒ (ì‹ í˜•) | ì•½ê°„ ë†’ìŒ | ğŸ† M2 | minikube/kind ì„±ëŠ¥ |
| **VM ê°œìˆ˜ (ì•ˆì •)** | 2-3ê°œ | 4-5ê°œ | ğŸ† M1 | í´ëŸ¬ìŠ¤í„° ê·œëª¨ |
| **íœ´ëŒ€ì„±** | âœ… ì´ë™ ê°€ëŠ¥ | âŒ ê³ ì • | ğŸ† M2 | Edge ì‹œë®¬ë ˆì´ì…˜ |
| **24ì‹œê°„ ìš´ì˜** | âŒ ë°°í„°ë¦¬/ë°œì—´ | âœ… ì•ˆì •ì  | ğŸ† M1 | Always-on ì„œë¹„ìŠ¤ |

### ì „ëµì  ì›Œí¬ë¡œë“œ ë°°ì¹˜

```
ë¶„ì‚°ì‹œìŠ¤í…œ ê´€ì ì˜ í´ëŸ¬ìŠ¤í„° ì„¤ê³„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

M2 ë§¥ë¶ (8GB) - "Edge / Compute Cluster"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì—­í• : ê°€ë³ê³  ë¹ ë¥¸ stateless ì›Œí¬ë¡œë“œ             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… ì í•©í•œ ì›Œí¬ë¡œë“œ:                              â”‚
â”‚   â€¢ API Gateway (stateless)                     â”‚
â”‚   â€¢ Compute-intensive (ML inference)            â”‚
â”‚   â€¢ Short-lived jobs (CI/CD runners)            â”‚
â”‚   â€¢ ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ (minikube/kind)            â”‚
â”‚   â€¢ Frontend apps (React, nginx)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âŒ ë¶€ì í•©í•œ ì›Œí¬ë¡œë“œ:                            â”‚
â”‚   â€¢ Database (ë©”ëª¨ë¦¬ ë§ì´ ë¨¹ìŒ)                 â”‚
â”‚   â€¢ Caching (Redis í° dataset)                 â”‚
â”‚   â€¢ Stateful apps (PV ë§ì´ í•„ìš”)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

M1 ë§¥ë¯¸ë‹ˆ (16GB) - "Core / Data Cluster"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì—­í• : ìƒíƒœ ìœ ì§€, ë©”ëª¨ë¦¬ ì§‘ì•½ ì›Œí¬ë¡œë“œ            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… ì í•©í•œ ì›Œí¬ë¡œë“œ:                              â”‚
â”‚   â€¢ Databases (PostgreSQL, MySQL)               â”‚
â”‚   â€¢ Message Queue (Kafka, RabbitMQ)             â”‚
â”‚   â€¢ Caching (Redis, Memcached)                 â”‚
â”‚   â€¢ Observability (Prometheus, Grafana)         â”‚
â”‚   â€¢ etcd, Control Plane (Kubernetes)           â”‚
â”‚   â€¢ Long-running services                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âŒ ë¶€ì í•©í•œ ì›Œí¬ë¡œë“œ:                            â”‚
â”‚   â€¢ CPU-heavy ML training (ë©”ëª¨ë¦¬ëŠ” ì¢‹ìŒ)       â”‚
â”‚   â€¢ ì¼ì‹œì  ì›Œí¬ë¡œë“œ (VM ì¼œë‘ê¸° ì•„ê¹Œì›€)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”€ 3ê°€ì§€ ì•„í‚¤í…ì²˜ íŒ¨í„´ ë¹„êµ

### íŒ¨í„´ 1: Active-Passive (DR ì‹œë‚˜ë¦¬ì˜¤)

```yaml
Architecture: ì£¼-ë°±ì—… êµ¬ì¡°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

M1 ë§¥ë¯¸ë‹ˆ (Primary) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  - ëª¨ë“  ì„œë¹„ìŠ¤ ì‹¤í–‰         â”‚
  - Prometheus + Grafana     â”‚  Replication
  - Database (Primary)       â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
  - 24ì‹œê°„ ìš´ì˜              â”‚
                              â”‚
M2 ë§¥ë¶ (Standby) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  - Database (Replica, Read-only)
  - í•„ìš”ì‹œ ì¥ì• ë³µêµ¬ìš©
  - í‰ì†Œì—” ê°œë°œ/ì‹¤í—˜ìš©
```

> [!info] íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
> **ì¥ì **:
> - âœ… ê°„ë‹¨í•œ êµ¬ì¡°, ê´€ë¦¬ ì‰¬ì›€
> - âœ… M2 ë°°í„°ë¦¬ ì ˆì•½ (í•„ìš”ì‹œë§Œ ì¼œê¸°)
>
> **ë‹¨ì **:
> - âŒ M2 ìì› í™œìš©ë„ ë‚®ìŒ
> - âŒ ë¶„ì‚°ì‹œìŠ¤í…œ í•™ìŠµ ê¸°íšŒ ì ìŒ

**ë¶„ì‚°ì‹œìŠ¤í…œ í•™ìŠµ í¬ì¸íŠ¸**:
- Replication lag ì²´í—˜
- Failover ë©”ì»¤ë‹ˆì¦˜ (ìˆ˜ë™/ìë™)
- Consensus ì—†ì´ ë‹¨ìˆœ ë³µì œ

---

### íŒ¨í„´ 2: Workload Partitioning (âœ¨ ì¶”ì²œ!)

```yaml
Architecture: ì—­í•  ê¸°ë°˜ ë¶„ì‚°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

M1 ë§¥ë¯¸ë‹ˆ (Data Plane)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Control Plane:               â”‚
â”‚ - etcd cluster (3 nodes)     â”‚
â”‚ - K8s API server             â”‚
â”‚                              â”‚
â”‚ Stateful Workloads:          â”‚
â”‚ - PostgreSQL (persistent)    â”‚
â”‚ - Kafka (message queue)      â”‚
â”‚ - Redis (cache, 4GB data)    â”‚
â”‚ - Prometheus (metrics store) â”‚
â”‚                              â”‚
â”‚ Long-running:                â”‚
â”‚ - Grafana                    â”‚
â”‚ - Jenkins controller         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†•ï¸ Service Mesh / VPN
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ M2 ë§¥ë¶ (Compute Plane)      â”‚
â”‚                              â”‚
â”‚ Stateless Workloads:         â”‚
â”‚ - API Gateway (NGINX)        â”‚
â”‚ - Microservices (REST APIs)  â”‚
â”‚ - Frontend (React apps)      â”‚
â”‚                              â”‚
â”‚ Burst Compute:               â”‚
â”‚ - CI/CD runners              â”‚
â”‚ - ML inference               â”‚
â”‚ - Image processing           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> [!success] íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
> **ì¥ì **:
> - âœ… ê° í•˜ë“œì›¨ì–´ì˜ ê°•ì  ê·¹ëŒ€í™”
> - âœ… M2ë¥¼ í•„ìš”í•  ë•Œë§Œ ì‚¬ìš© (ì „ë ¥ íš¨ìœ¨)
> - âœ… ë¶„ì‚°ì‹œìŠ¤í…œ íŒ¨í„´ í•™ìŠµ ìµœê³ 
>
> **ë‹¨ì **:
> - âš ï¸ ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„± ë†’ìŒ (M2 ì˜¤í”„ë¼ì¸ ì‹œ API ë¶ˆê°€)
> - âš ï¸ Service discovery ë³µì¡ë„ ì¦ê°€

**ë¶„ì‚°ì‹œìŠ¤í…œ í•™ìŠµ í¬ì¸íŠ¸**:
- Service mesh (Istio/Linkerd)
- Load balancing across clusters
- Circuit breaker (M2 ë‹¤ìš´ ì‹œ fallback)
- Distributed tracing (M1 â†’ M2 í˜¸ì¶œ ì¶”ì )

**ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤**:
```bash
# ì‚¬ìš©ì ìš”ì²­ íë¦„
Client
  â†’ M2 (API Gateway)
  â†’ M2 (Auth Service, stateless)
  â†’ M1 (PostgreSQL, read user data)
  â†’ M2 (Response ì¡°ë¦½)
  â†’ Client

# M2ê°€ êº¼ì ¸ìˆìœ¼ë©´?
â†’ M1ì˜ fallback API Gatewayë¡œ ë¼ìš°íŒ…
â†’ ëŠë¦¬ì§€ë§Œ ì„œë¹„ìŠ¤ëŠ” ìœ ì§€ (degraded mode)
```

---

### íŒ¨í„´ 3: Multi-Tenant Simulation (ê³ ê¸‰)

```yaml
Architecture: í…Œë„ŒíŠ¸ë³„ í´ëŸ¬ìŠ¤í„° ê²©ë¦¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

M1 ë§¥ë¯¸ë‹ˆ (Production Cluster)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tenant A (ê³ ê°ì‚¬ A)          â”‚
â”‚ - Dedicated namespace        â”‚
â”‚ - Resource quota: 8GB        â”‚
â”‚ - Production workloads       â”‚
â”‚                              â”‚
â”‚ Shared Services:             â”‚
â”‚ - Ingress controller         â”‚
â”‚ - Monitoring stack           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†•ï¸ Federation / Cross-cluster policy
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ M2 ë§¥ë¶ (Dev/Edge Cluster)   â”‚
â”‚                              â”‚
â”‚ Tenant B (ê³ ê°ì‚¬ B, ê°€ë²¼ì›€)  â”‚
â”‚ - ê°œë°œ í™˜ê²½                  â”‚
â”‚ - Edge computing use case    â”‚
â”‚                              â”‚
â”‚ Experimentation:             â”‚
â”‚ - Canary deployment testing  â”‚
â”‚ - New K8s version testing    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> [!warning] íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
> **ì¥ì **:
> - âœ… ë©€í‹° í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ê²½í—˜ (Rancher, ArgoCD)
> - âœ… Tenant isolation í•™ìŠµ
>
> **ë‹¨ì **:
> - âš ï¸ ë³µì¡ë„ ë§¤ìš° ë†’ìŒ
> - âš ï¸ 8GBë¡œëŠ” ì—¬ëŸ¬ tenant ì–´ë ¤ì›€

**ë¶„ì‚°ì‹œìŠ¤í…œ í•™ìŠµ í¬ì¸íŠ¸**:
- Kubernetes Federation (KubeFed)
- Multi-tenancy security
- Cross-cluster resource scheduling

---

## ğŸ§® ë¶„ì‚°ì‹œìŠ¤í…œ ì´ë¡  â†’ ì‹¤ìŠµ ë§¤í•‘

### 1. CAP Theorem ì²´í—˜í•˜ê¸°

```yaml
ì‹œë‚˜ë¦¬ì˜¤: M1ê³¼ M2 ì‚¬ì´ ë„¤íŠ¸ì›Œí¬ íŒŒí‹°ì…˜ ë°œìƒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Setup:
  - M1: etcd cluster (3 nodes)
  - M2: etcd client (read-only)
  - ì„œë¹„ìŠ¤: User profile ì¡°íšŒ API

Test 1: Network Partition (P)
  1. M1 â†” M2 ì‚¬ì´ ë°©í™”ë²½ ê·œì¹™ ì¶”ê°€
     â†’ iptables drop all from M2

  2. M2ì—ì„œ API í˜¸ì¶œ ì‹œë„
     â†’ Timeout ë°œìƒ!

  3. ì„ íƒì§€:
     - Consistency (C) ìš°ì„ : M2 API ì‘ë‹µ ê±°ë¶€ (503)
     - Availability (A) ìš°ì„ : M2 ë¡œì»¬ ìºì‹œë¡œ ì‘ë‹µ (stale data)

Test 2: Eventual Consistency
  - M1ì—ì„œ user profile ì—…ë°ì´íŠ¸
  - M2ëŠ” 1ë¶„ í›„ sync (Kafkaë¡œ ì „ë‹¬)
  - ê·¸ ì‚¬ì´ M2ëŠ” outdated ë°ì´í„° ë°˜í™˜
  â†’ ì‹¤ì œë¡œ "eventually consistent" ì²´ê°!
```

> [!tip] í•™ìŠµ í¬ì¸íŠ¸
> - CAP theoremì€ ì´ë¡ ì´ ì•„ë‹ˆë¼ **ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ í™˜ê²½**ì˜ ë¬¸ì œ
> - Consistency vs Availability íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì§ì ‘ ì„ íƒ
> - Partition toleranceëŠ” ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜

### 2. Consensus Algorithm (etcd, Raft)

```yaml
ì‹¤ìŠµ: etcd í´ëŸ¬ìŠ¤í„°ì˜ Leader Election
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Setup:
  M1: etcd-1 (Leader), etcd-2, etcd-3
  M2: etcd client

Test:
  1. etcd-1 (Leader) ê°•ì œ ì¢…ë£Œ
     â†’ watch -n 1 "etcdctl endpoint status"
     â†’ 2-3ì´ˆ ë‚´ etcd-2 ë˜ëŠ” etcd-3ì´ ìƒˆ Leader

  2. Quorum ê¹¨ê¸° (2/3 ë…¸ë“œ ë‹¤ìš´)
     â†’ etcd cluster unavailable!
     â†’ M2ì—ì„œ API í˜¸ì¶œ ëª¨ë‘ ì‹¤íŒ¨

  3. Split-brain ì‹œë®¬ë ˆì´ì…˜
     â†’ M1ê³¼ M2 ë„¤íŠ¸ì›Œí¬ ë¶„ë¦¬
     â†’ ê°ì ë…ë¦½ì ì¸ Leader ì„ ì¶œ ì‹œë„
     â†’ ì¬ì—°ê²° ì‹œ conflict resolution
```

> [!tip] í•™ìŠµ í¬ì¸íŠ¸
> - Raft consensusì˜ ì‹¤ì œ ë™ì‘
> - Quorumì˜ ì¤‘ìš”ì„± (majority voting)
> - Split-brain ë¬¸ì œì™€ í•´ê²°

### 3. Distributed Tracing

```yaml
ì‹¤ìŠµ: M1 â†” M2 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í˜¸ì¶œ ì¶”ì 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Setup:
  - Jaeger (M1ì— ë°°í¬)
  - OpenTelemetry instrumentation

Flow:
  1. Client â†’ M2 (API Gateway)
     â””â”€ Span ID: abc123

  2. M2 â†’ M1 (User Service)
     â””â”€ Parent Span: abc123, Span ID: def456

  3. M1 â†’ M1 (Database)
     â””â”€ Parent Span: def456, Span ID: ghi789

  4. Jaeger UIì—ì„œ ì „ì²´ trace ì‹œê°í™”
     â†’ M1 â†” M2 ë„¤íŠ¸ì›Œí¬ latency ì¸¡ì •
     â†’ ì–´ëŠ êµ¬ê°„ì´ ëŠë¦°ì§€ ë³‘ëª© ë¶„ì„
```

### 4. Load Balancing & Failover

```yaml
ì‹¤ìŠµ: M2 ë‹¤ìš´ ì‹œ M1ìœ¼ë¡œ ìë™ í˜ì¼ì˜¤ë²„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Setup:
  - M2: API Gateway (primary)
  - M1: API Gateway (secondary, standby)
  - External DNS: Round-robin or weighted

Test:
  1. ì •ìƒ ìƒíƒœ: 80% íŠ¸ë˜í”½ â†’ M2, 20% â†’ M1

  2. M2 ë‹¤ìš´:
     â†’ Health check ì‹¤íŒ¨ ê°ì§€ (3ì´ˆ ë‚´)
     â†’ ëª¨ë“  íŠ¸ë˜í”½ M1ìœ¼ë¡œ ìë™ ì „í™˜

  3. M2 ë³µêµ¬:
     â†’ Gradual rollback (10% â†’ 50% â†’ 80%)
```

---

## ğŸ¯ ìµœì¢… ì¶”ì²œ ì•„í‚¤í…ì²˜ (íŒ¨í„´ 2 ê¸°ë°˜)

### ì „ì²´ êµ¬ì„±ë„

```yaml
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ  í™ˆë„¤íŠ¸ì›Œí¬ ë¶„ì‚° Kubernetes í´ëŸ¬ìŠ¤í„°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

M1 ë§¥ë¯¸ë‹ˆ (16GB) - "Always-On Core"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Control Plane Cluster (kind, 3 nodes)      â”‚
â”‚ - RAM: 6GB (2GB Ã— 3)                       â”‚
â”‚ - etcd, kube-apiserver, scheduler          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Layer (Stateful Workloads)           â”‚
â”‚ - PostgreSQL: 2GB RAM                      â”‚
â”‚ - Redis: 1GB RAM                           â”‚
â”‚ - Kafka (single broker): 2GB RAM          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Observability Stack                        â”‚
â”‚ - Prometheus: 2GB RAM (long-term metrics)  â”‚
â”‚ - Grafana: 512MB                           â”‚
â”‚ - Jaeger: 1GB (distributed tracing)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì´ ì‚¬ìš©: ~14GB (ì—¬ìœ  2GB)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

M2 ë§¥ë¶ (8GB) - "On-Demand Compute"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Cluster (minikube, 1-2 nodes)       â”‚
â”‚ - RAM: 4GB (í•„ìš” ì‹œ ì¡°ì •)                  â”‚
â”‚ - Control plane: M1 í´ëŸ¬ìŠ¤í„° ì°¸ì¡°          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stateless Services                         â”‚
â”‚ - API Gateway (NGINX/Traefik): 256MB      â”‚
â”‚ - Backend APIs (Go/Node.js): 1GB          â”‚
â”‚ - Frontend (React bundle): 256MB          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Burst Workloads (í•„ìš” ì‹œ)                  â”‚
â”‚ - CI/CD runners: 1-2GB                    â”‚
â”‚ - ML inference: 2GB                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì´ ì‚¬ìš©: ~5-6GB (í˜¸ìŠ¤íŠ¸ 2GB í™•ë³´)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

```yaml
Physical Network: 192.168.1.0/24 (í™ˆ ë¼ìš°í„°)
â”œâ”€ M1: 192.168.1.10
â”‚   â”œâ”€ Control Plane: 10.89.0.0/24
â”‚   â”œâ”€ Pod Network: 10.244.0.0/16
â”‚   â””â”€ Service IPs: 10.96.0.0/16
â”‚
â”œâ”€ M2: 192.168.1.20
â”‚   â”œâ”€ Worker Node: M1ì˜ Pod Networkì— join
â”‚   â””â”€ Service: M1ì˜ Service IPs ê³µìœ 
â”‚
â””â”€ Inter-cluster Communication:
    - Option A: Submariner (L3 tunnel) â­ ì¶”ì²œ
    - Option B: Cilium Cluster Mesh
    - Option C: Simple External IP routing

Service Mesh: Istio (optional)
â”œâ”€ M1: Istiod (control plane)
â”œâ”€ M2: Envoy sidecars
â””â”€ mTLS between clusters
```

### ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµ

| ì»´í¬ë„ŒíŠ¸ | M1 í• ë‹¹ | M2 í• ë‹¹ | ì´ìœ  |
|----------|---------|---------|------|
| **etcd** | 3 nodes (6GB) | 0 | Quorum ë³´ì¥, ì•ˆì •ì„± |
| **API Server** | Primary | Proxy | í†µì‹  latency ìµœì†Œí™” |
| **Database** | Primary | 0 | Stateful, ë©”ëª¨ë¦¬ ì§‘ì•½ |
| **Cache (Redis)** | 1GB | 0 | ë©”ëª¨ë¦¬ ì§‘ì•½ |
| **API Gateway** | Standby | Primary | M2 ì„±ëŠ¥ í™œìš© |
| **Backend APIs** | 0 | Primary | Stateless, CPU ì§‘ì•½ |
| **Prometheus** | 2GB | Agent only | ì¤‘ì•™ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§ |
| **Grafana** | 512MB | 0 | Visualization ì„œë²„ |

---

## ğŸ“… ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ

### Phase 0: í˜„ì¬ ìƒíƒœ ìœ ì§€ (í”„ë¡œë©”í…Œìš°ìŠ¤ í•™ìŠµ ì¤‘)

```bash
í˜„ì¬ ìƒíƒœ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
M1: 4 VMs (VMware Fusion)
    - cp-k8s-1.30.3, w1, w2, w3
    - Prometheus ì„¤ì¹˜ ì™„ë£Œ âœ…

M2: ì‹¤ìŠµ ì¤‘ë‹¨ (ë©”ëª¨ë¦¬ ë¶€ì¡±)

â†’ ì¼ë‹¨ M1ë§Œìœ¼ë¡œ í”„ë¡œë©”í…Œìš°ìŠ¤ í•™ìŠµ ì™„ë£Œ!
```

### Phase 1: M2ë¥¼ ê²½ëŸ‰ í´ëŸ¬ìŠ¤í„°ë¡œ ì¬êµ¬ì„± (ì£¼ë§ 1ì¼)

```bash
# Option 1: minikube (ê°„ë‹¨)
minikube start --memory=4096 --cpus=2 --nodes=2

# Option 2: kind (ë” ê°€ë²¼ì›€, ì¶”ì²œ)
cat > kind-m2.yaml << 'EOF'
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: worker
  - role: worker
EOF

kind create cluster --config kind-m2.yaml --name m2-cluster

# í™•ì¸
kubectl get nodes
```

> [!tip] Phase 1 ì„±ê³µ ê¸°ì¤€
> - M2ì—ì„œ 2-node í´ëŸ¬ìŠ¤í„° ì •ìƒ ì‹¤í–‰
> - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 6GB ì´í•˜
> - nginx pod ë°°í¬ ë° ì ‘ê·¼ ê°€ëŠ¥

### Phase 2: ê¸°ë³¸ ì„œë¹„ìŠ¤ í†µì‹  í…ŒìŠ¤íŠ¸ (1ì¼)

```bash
# Step 1: M2ì— nginx ë°°í¬
kubectl run nginx --image=nginx --port=80

# Step 2: LoadBalancer ì„œë¹„ìŠ¤ ìƒì„± (MetalLB ì‚¬ìš©)
kubectl expose pod nginx --type=LoadBalancer --name=nginx-lb

# Step 3: External IP í™•ì¸
kubectl get svc nginx-lb
# EXTERNAL-IP: 192.168.1.20 (ì˜ˆì‹œ)

# Step 4: M1ì—ì„œ M2 ì„œë¹„ìŠ¤ í˜¸ì¶œ
# M1ì˜ í„°ë¯¸ë„ì—ì„œ:
curl http://192.168.1.20
# â†’ nginx ì‘ë‹µ í™•ì¸!
```

> [!success] Phase 2 ì„±ê³µ ê¸°ì¤€
> - M1 â†’ M2 HTTP í†µì‹  ì„±ê³µ
> - M2 â†’ M1 HTTP í†µì‹  ì„±ê³µ
> - Ping ì‘ë‹µ ì‹œê°„ < 5ms (ê°™ì€ ë„¤íŠ¸ì›Œí¬)

### Phase 3: Submarinerë¡œ í´ëŸ¬ìŠ¤í„° ì—°ê²° (2ì¼)

```bash
# Step 1: subctl ì„¤ì¹˜
curl -Ls https://get.submariner.io | bash
export PATH=$PATH:~/.local/bin

# Step 2: M1ì„ brokerë¡œ ì„¤ì •
subctl deploy-broker \
  --kubeconfig ~/.kube/config-m1 \
  --context kind-m1-cluster

# Step 3: M1 join
subctl join broker-info.subm \
  --kubeconfig ~/.kube/config-m1 \
  --clusterid m1 \
  --natt=false

# Step 4: M2 join
subctl join broker-info.subm \
  --kubeconfig ~/.kube/config-m2 \
  --clusterid m2 \
  --natt=false

# Step 5: ì—°ê²° í™•ì¸
subctl show all

# Step 6: Service Export (M2ì˜ nginx)
kubectl label service nginx-lb \
  submariner.io/exported=true

# Step 7: M1ì—ì„œ M2 service í˜¸ì¶œ (DNS ê¸°ë°˜)
kubectl run curl --image=curlimages/curl -it --rm -- \
  curl http://nginx-lb.default.svc.clusterset.local
```

> [!success] Phase 3 ì„±ê³µ ê¸°ì¤€
> - Submariner gateway ì—°ê²° ìƒíƒœ: `Connected`
> - M1 Pod â†’ M2 Service (DNS ê¸°ë°˜ í˜¸ì¶œ ì„±ê³µ)
> - M2 Pod â†’ M1 Service (DNS ê¸°ë°˜ í˜¸ì¶œ ì„±ê³µ)

### Phase 4: ë¶„ì‚° ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ (3-5ì¼)

```bash
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# M1: ë°ì´í„° ë ˆì´ì–´ êµ¬ì„±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# PostgreSQL ì„¤ì¹˜
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install postgresql bitnami/postgresql \
  --namespace data --create-namespace \
  --set auth.postgresPassword=mysecretpassword

# Redis ì„¤ì¹˜
helm install redis bitnami/redis \
  --namespace data \
  --set auth.password=redispassword

# Service Export
kubectl label service postgresql -n data \
  submariner.io/exported=true
kubectl label service redis-master -n data \
  submariner.io/exported=true

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# M2: API ì„œë²„ ë°°í¬
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

cat > api-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      containers:
      - name: api
        image: your-api-image:latest
        env:
        - name: DB_HOST
          value: "postgresql.data.svc.clusterset.local"
        - name: DB_PASSWORD
          value: "mysecretpassword"
        - name: REDIS_HOST
          value: "redis-master.data.svc.clusterset.local"
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: api-server
spec:
  type: LoadBalancer
  selector:
    app: api-server
  ports:
  - port: 80
    targetPort: 8080
EOF

kubectl apply -f api-deployment.yaml

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# M1: Prometheusê°€ M2 ë©”íŠ¸ë¦­ ìˆ˜ì§‘
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

cat > prometheus-scrape-m2.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s

    scrape_configs:
    - job_name: 'm1-cluster'
      kubernetes_sd_configs:
      - role: pod

    - job_name: 'm2-cluster'
      static_configs:
      - targets:
        - 'api-server.default.svc.clusterset.local:8080'
      metrics_path: '/metrics'
EOF

kubectl apply -f prometheus-scrape-m2.yaml
kubectl rollout restart deployment/prometheus-server -n monitoring
```

> [!success] Phase 4 ì„±ê³µ ê¸°ì¤€
> - M2 APIê°€ M1 PostgreSQL ì ‘ê·¼ ì„±ê³µ
> - M2 APIê°€ M1 Redis ì ‘ê·¼ ì„±ê³µ
> - Prometheusê°€ M1+M2 ë©”íŠ¸ë¦­ ëª¨ë‘ ìˆ˜ì§‘
> - Grafana ëŒ€ì‹œë³´ë“œì—ì„œ í†µí•© ì‹œê°í™”

### Phase 5: ë¶„ì‚° ì‹œìŠ¤í…œ ì‹¤í—˜ (ê³ ê¸‰, 1-2ì£¼)

```bash
# ì‹¤í—˜ 1: Network Partition ì‹œë®¬ë ˆì´ì…˜
# M1 í„°ë¯¸ë„ì—ì„œ:
sudo iptables -A INPUT -s 192.168.1.20 -j DROP
sudo iptables -A OUTPUT -d 192.168.1.20 -j DROP

# M2ì—ì„œ API í˜¸ì¶œ ì‹œë„
curl http://postgresql.data.svc.clusterset.local
# â†’ Timeout!

# Circuit breaker ë™ì‘ í™•ì¸
# â†’ M2 APIê°€ fallback response ë°˜í™˜

# ë³µêµ¬
sudo iptables -D INPUT -s 192.168.1.20 -j DROP
sudo iptables -D OUTPUT -d 192.168.1.20 -j DROP

# ì‹¤í—˜ 2: Leader Election (etcd)
kubectl exec -it etcd-0 -n kube-system -- etcdctl member list
# Leader í™•ì¸ í›„ ê°•ì œ ì¢…ë£Œ
kubectl delete pod etcd-0 -n kube-system --force

# ìƒˆ Leader ì„ ì¶œ í™•ì¸ (2-3ì´ˆ ë‚´)
watch -n 1 "kubectl exec -it etcd-1 -n kube-system -- etcdctl endpoint status"

# ì‹¤í—˜ 3: Distributed Tracing
# Jaeger ì„¤ì¹˜
kubectl create namespace observability
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/crds/jaegertracing.io_jaegers_crd.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/service_account.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/role.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/role_binding.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/operator.yaml

# Jaeger UI ì ‘ê·¼
kubectl port-forward -n observability svc/jaeger-query 16686:16686
# â†’ http://localhost:16686
```

---

## ğŸ’° ë¹„ìš© vs í•™ìŠµ íš¨ê³¼ ë¶„ì„

### ì‹œê°„ íˆ¬ì

| Phase | ì˜ˆìƒ ì‹œê°„ | í•™ìŠµ íš¨ê³¼ | ROI |
|-------|----------|----------|-----|
| **Phase 0 (í˜„ì¬)** | 4ì‹œê°„ | Prometheus ê¸°ë³¸ | â­â­â­ |
| **Phase 1-2 (ì—°ê²°)** | +8ì‹œê°„ | ë©€í‹° í´ëŸ¬ìŠ¤í„° ê¸°ì´ˆ | â­â­â­â­ |
| **Phase 3 (Submariner)** | +16ì‹œê°„ | ì‹¤ë¬´ê¸‰ ë„¤íŠ¸ì›Œí‚¹ | â­â­â­â­â­ |
| **Phase 4 (ë¶„ì‚° ì•±)** | +24ì‹œê°„ | í¬íŠ¸í´ë¦¬ì˜¤ ê¸‰ | â­â­â­â­â­ |
| **Phase 5 (ì‹¤í—˜)** | +40ì‹œê°„ | ë¶„ì‚°ì‹œìŠ¤í…œ ë§ˆìŠ¤í„° | â­â­â­â­â­ |

### í´ë¼ìš°ë“œ ëŒ€ì•ˆê³¼ ë¹„êµ

```
í´ë¼ìš°ë“œ ë¹„ìš© (Multi-cluster):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GKE Multi-cluster: $150/month
AWS EKS (2 clusters): $200/month
Azure AKS (2 clusters): $180/month

â†’ 6ê°œì›”ì´ë©´: $900-1200 ì ˆì•½!

í™ˆë© ë¹„ìš©:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
M1 ë§¥ë¯¸ë‹ˆ: ì´ë¯¸ ë³´ìœ  ($0)
M2 ë§¥ë¶: ì´ë¯¸ ë³´ìœ  ($0)
ì „ê¸°ì„¸: ~$5/month (M1 24ì‹œê°„ ê°€ë™)

â†’ ì‹¤ì§ˆì  ë¹„ìš©: ~$30/6ê°œì›”
```

### í•™ìŠµ ê°€ì¹˜

| ê¸°ìˆ  ìŠ¤íƒ | í™ˆë© ê²½í—˜ | í´ë¼ìš°ë“œ ëŒ€ì•ˆ | í•™ìŠµ ê¹Šì´ |
|----------|----------|-------------|-----------|
| **Kubernetes** | âœ… ì§ì ‘ ì„¤ì¹˜/ìš´ì˜ | Managed (ì¶”ìƒí™”) | í™ˆë© > í´ë¼ìš°ë“œ |
| **Networking** | âœ… L2/L3 ì§ì ‘ ì„¤ì • | VPC (ìë™í™”) | í™ˆë© >> í´ë¼ìš°ë“œ |
| **Troubleshooting** | âœ… ëª¨ë“  ë ˆì´ì–´ ì ‘ê·¼ | ì œí•œì  ë¡œê·¸ | í™ˆë© >>> í´ë¼ìš°ë“œ |
| **Cost Management** | âœ… í•˜ë“œì›¨ì–´ ì œì•½ ì²´ê° | ë¹„ìš©ë§Œ ê³ ë ¤ | í™ˆë© > í´ë¼ìš°ë“œ |
| **Real-world Scale** | âŒ ì œí•œì  | âœ… ë¬´ì œí•œ | í´ë¼ìš°ë“œ > í™ˆë© |

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ ë° ì°¸ê³  ìë£Œ

### ë‚´ë¶€ ë§í¬
- [[íŠ¸ëŸ¬ë¸”ìŠˆíŒ…-2025-12-03]] - ARM64 ê°€ìƒí™” ê²½í—˜
- [[kubernetes_cheatsheet]]
- [[kubernetes_networking]]
- [[distributed_systems_notes]]

### ì™¸ë¶€ ì°¸ê³  ìë£Œ

**Multi-cluster Kubernetes**:
- [Submariner ê³µì‹ ë¬¸ì„œ](https://submariner.io/)
- [Cilium Cluster Mesh](https://docs.cilium.io/en/stable/gettingstarted/clustermesh/)
- [Istio Multi-cluster](https://istio.io/latest/docs/setup/install/multicluster/)

**ë¶„ì‚°ì‹œìŠ¤í…œ ì´ë¡ **:
- [Designing Data-Intensive Applications](https://dataintensive.net/) (ì±…)
- [CAP Theorem ë…¼ë¬¸](https://www.comp.nus.edu.sg/~cs5223/papers/cap.pdf)
- [Raft Consensus Algorithm](https://raft.github.io/)

**Kubernetes Networking**:
- [Kubernetes Network Model](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
- [CNI Specification](https://github.com/containernetworking/cni)

---

## ğŸ¯ í•µì‹¬ ìš”ì•½

> **TL;DR: ì´ í”„ë¡œì íŠ¸ì˜ ê°€ì¹˜**
>
> **ê¸°ìˆ ì  í•™ìŠµ**:
> - Multi-cluster Kubernetes ì•„í‚¤í…ì²˜ (Submariner, Istio)
> - ë¶„ì‚°ì‹œìŠ¤í…œ ì´ë¡  â†’ ì‹¤ìŠµ (CAP, Raft, Distributed Tracing)
> - í•˜ë“œì›¨ì–´ ì œì•½ ê¸°ë°˜ ì›Œí¬ë¡œë“œ ìµœì í™”
> - Service mesh, Load balancing, Failover ê²½í—˜
>
> **ì‹¤ë¬´ í™œìš©**:
> - ì´ë ¥ì„œ: "í™ˆë©ì—ì„œ ë©€í‹° í´ëŸ¬ìŠ¤í„° Kubernetes í™˜ê²½ êµ¬ì¶•"
> - ë©´ì ‘: "ì‹¤ì œ ë¶„ì‚°ì‹œìŠ¤í…œ ë¬¸ì œ í•´ê²° ê²½í—˜ (Network partition, Failover)"
> - í¬íŠ¸í´ë¦¬ì˜¤: "M1/M2 í•˜ë“œì›¨ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„"
>
> **ë¹„ìš© ì ˆê°**:
> - í´ë¼ìš°ë“œ ë¹„ìš© ëŒ€ë¹„ 6ê°œì›”ì— $900-1200 ì ˆì•½
> - ë¬´ì œí•œ ì‹¤í—˜ ê°€ëŠ¥ (ë¹„ìš© ê±±ì • ì—†ìŒ)
>
> **ì¶”ì²œ ì‹œì‘ ì‹œì **:
> - í”„ë¡œë©”í…Œìš°ìŠ¤ í•™ìŠµ ì™„ë£Œ í›„ (2-3ì£¼ í›„)
> - Phase 1-2ë¶€í„° ì‹œì‘ (ì£¼ë§ 1-2ì¼)
> - Phase 3-4ë¡œ ì ì§„ì  í™•ì¥ (4-6ì£¼)

---

## ğŸ“Š ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: M2 í´ëŸ¬ìŠ¤í„° êµ¬ì„±
- [ ] minikube ë˜ëŠ” kind ì„¤ì¹˜
- [ ] 2-node í´ëŸ¬ìŠ¤í„° ìƒì„± (ë©”ëª¨ë¦¬ 4GB)
- [ ] kubectl ì ‘ê·¼ í™•ì¸
- [ ] nginx pod ë°°í¬ ë° ì ‘ê·¼ í…ŒìŠ¤íŠ¸

### Phase 2: ê¸°ë³¸ í†µì‹ 
- [ ] M2 LoadBalancer ì„œë¹„ìŠ¤ ìƒì„±
- [ ] M1 â†’ M2 HTTP í†µì‹  í™•ì¸
- [ ] M2 â†’ M1 HTTP í†µì‹  í™•ì¸
- [ ] Ping latency ì¸¡ì • (< 5ms)

### Phase 3: Submariner ì—°ê²°
- [ ] subctl ì„¤ì¹˜
- [ ] M1 broker êµ¬ì„±
- [ ] M1, M2 í´ëŸ¬ìŠ¤í„° join
- [ ] Service export/import í…ŒìŠ¤íŠ¸
- [ ] DNS ê¸°ë°˜ service discovery í™•ì¸

### Phase 4: ë¶„ì‚° ì•± ë°°í¬
- [ ] M1ì— PostgreSQL ì„¤ì¹˜
- [ ] M1ì— Redis ì„¤ì¹˜
- [ ] M2ì— API ì„œë²„ ë°°í¬
- [ ] Cross-cluster í†µì‹  í™•ì¸
- [ ] Prometheus ë©€í‹° í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] Grafana í†µí•© ëŒ€ì‹œë³´ë“œ

### Phase 5: ë¶„ì‚° ì‹œìŠ¤í…œ ì‹¤í—˜
- [ ] Network partition ì‹œë®¬ë ˆì´ì…˜
- [ ] etcd Leader election í…ŒìŠ¤íŠ¸
- [ ] Jaeger distributed tracing êµ¬ì„±
- [ ] Circuit breaker ë™ì‘ í™•ì¸
- [ ] Failover ìë™í™” í…ŒìŠ¤íŠ¸

---

**ì‘ì„±ì**: irix
**ì‘ì„±ì¼**: 2025-12-04
**ìƒíƒœ**: ğŸ“‹ ê³„íš ë‹¨ê³„
**ì˜ˆìƒ ì™„ë£Œ**: 2025ë…„ 1ì›” ë§ (í”„ë¡œë©”í…Œìš°ìŠ¤ í•™ìŠµ ì™„ë£Œ í›„)

---

#kubernetes #multi-cluster #distributed-systems #homelab #architecture #submariner #istio #prometheus #m1 #m2
